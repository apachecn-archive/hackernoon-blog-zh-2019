<html>
<head>
<title>MPI workloads performance on MapR Data Platform Part 2 — Matrix Multiplication</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MapR 数据平台上的 MPI 工作负载性能第 2 部分—矩阵乘法</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/mpi-workloads-performance-on-mapr-data-platform-part-2-matrix-multiplication-e1020ea0e18a#2019-02-12">https://medium.com/hackernoon/mpi-workloads-performance-on-mapr-data-platform-part-2-matrix-multiplication-e1020ea0e18a#2019-02-12</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div class="fe ff ir"><img src="../Images/42f3301b9fab3cc306c6450d2270cf32.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/1*lNPotf8xsD4fbiQIZCAuDA.gif"/></div></figure><p id="cc62" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在本系列的<a class="ae jw" rel="noopener" href="/@anicolaspp/mpi-workloads-performance-on-mapr-data-platform-part-1-c801f3d5fe08"> <strong class="ja hv"> <em class="jx">第一部分</em></strong></a><strong class="ja hv"><em class="jx"/></strong>中，我们展示了如何在 MapR 数据平台上使用 MPI 在相当大的范围内成功地找到素数。此外，我们比较了我们在 MPI 中的<code class="eh jy jz ka kb b">Sieve of Eratosthenes</code>实现和在 Spark 中的相同算法，以发现它们在查看一些有趣的实现细节时的行为。</p><p id="f581" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在本系列的第二部分，我们将在 MPI 和 Apache Spark 中实现<code class="eh jy jz ka kb b">Matrix Multiplication</code>。同样，我们将看看每个实现在 MapR 上运行时的行为。然而，我们的 MPI 实现将基于<code class="eh jy jz ka kb b">Cannon Algorithm</code>,而在 Spark 中，我们将使用 MLlib 块矩阵函数进行矩阵相乘。</p><h2 id="1f67" class="kc kd hu bd ke kf kg kh ki kj kk kl km jj kn ko kp jn kq kr ks jr kt ku kv kw dt translated">卡农算法</h2><p id="b604" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated">并行计算中有隐含的含义。随着参与计算的处理器数量的增加，内存需求也会增加。这不是微不足道的，而是经过证明的。</p><p id="0d60" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">卡农算法的可扩展性极强。这意味着通过增加处理器数量，我们可以保持较低的内存需求。下面是对使用<code class="eh jy jz ka kb b">Matrix to Matrix</code>乘法的<code class="eh jy jz ka kb b">Matrix Multiplication</code>与<code class="eh jy jz ka kb b">Cannon Algorithm</code>使用的<code class="eh jy jz ka kb b">Block Decomposition Matrix Multiplication</code>的可扩展性分析。</p><figure class="ld le lf lg fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="fe ff lc"><img src="../Images/3bef0148bad157e6bb2b6922dd99f02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSgSA73zKZN6P3HTE315uA.png"/></div></div></figure><p id="1656" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如我们所见，在第一部分(<code class="eh jy jz ka kb b">Matrix-Matrix</code>)，我们没有获得任何可伸缩性，因为内存依赖于<code class="eh jy jz ka kb b">P</code>——处理器的数量。在第二部分(<code class="eh jy jz ka kb b">Cannon-Algorithm</code>)中，内存需求与处理器的数量无关，更具体地说，它是一个常数，允许我们更好地扩展。</p><h2 id="c249" class="kc kd hu bd ke kf kg kh ki kj kk kl km jj kn ko kp jn kq kr ks jr kt ku kv kw dt translated">用 MPI 实现 Cannon 算法</h2><p id="7c8d" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated">现在，我们将展示实现的主要部分，它是核心。然后，我们将链接整个源代码，供感兴趣的人查看。</p><p id="51ea" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">首先，让我们看看我们将要使用的函数，这样我们可以更好地理解应用程序流。</p><figure class="ld le lf lg fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="3967" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">注意，从/向文件读取和写入矩阵是并行和分布式的。每个处理器负责所讨论的矩阵中它自己的块(片)。</p><p id="d798" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">我们的应用程序如下所示。</p><figure class="ld le lf lg fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="bc6b" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如我们所见，这不是一段简单的代码，但它呈现了<code class="eh jy jz ka kb b">Cannon Algorithms</code>上的核心元素。完整的源代码可以在这个<a class="ae jw" href="https://github.com/anicolaspp/Parallel-Computing-MPI-Matrix-Multiplication" rel="noopener ugc nofollow" target="_blank"> <strong class="ja hv"> <em class="jx"> Github 资源库</em> </strong> </a> <strong class="ja hv"> <em class="jx">中找到。</em>T15】</strong></p><p id="6e22" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">在 Spark 中，同样的事情可以通过简单的构造来实现。下面的代码片段展示了这个过程。</p><figure class="ld le lf lg fq iv"><div class="bz el l di"><div class="ll lm l"/></div></figure><p id="53a0" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如我们所看到的，它非常简单，但是让我们看看它与我们的 MPI 版本相比有多快。</p><h2 id="66a5" class="kc kd hu bd ke kf kg kh ki kj kk kl km jj kn ko kp jn kq kr ks jr kt ku kv kw dt translated">使用 MPI</h2><p id="5a65" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated"><em class="jx"> 1000x1000 矩阵乘法</em></p><pre class="ld le lf lg fq ln kb lo lp aw lq dt"><span id="03c8" class="kc kd hu kb b fv lr ls l lt lu">mpirun -np 16 multiply 1000x1000.in 1000x1000.in 1000x1000.out</span><span id="ff2f" class="kc kd hu kb b fv lv ls l lt lu">elapsed time: 2.082910 // around 2 seconds</span></pre><p id="bed8" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="jx"> 10000x10000 矩阵乘法</em></p><pre class="ld le lf lg fq ln kb lo lp aw lq dt"><span id="6222" class="kc kd hu kb b fv lr ls l lt lu">mpirun -np 16 multiply 10000x10000.in 10000x10000.in 10000x10000.out</span><span id="ccaa" class="kc kd hu kb b fv lv ls l lt lu">elapsed time: 307200 // around 5 mins</span></pre><h2 id="63d6" class="kc kd hu bd ke kf kg kh ki kj kk kl km jj kn ko kp jn kq kr ks jr kt ku kv kw dt translated">火花中</h2><p id="eb6d" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated"><em class="jx"> 1000x1000 矩阵乘法</em></p><pre class="ld le lf lg fq ln kb lo lp aw lq dt"><span id="c9d4" class="kc kd hu kb b fv lr ls l lt lu">multiply("1000x1000.txt")(sc)</span><span id="b825" class="kc kd hu kb b fv lv ls l lt lu">res19: Long = 4118 // around 4 seconds</span></pre><p id="43f2" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated"><em class="jx"> 10000x10000 矩阵乘法</em></p><pre class="ld le lf lg fq ln kb lo lp aw lq dt"><span id="6c35" class="kc kd hu kb b fv lr ls l lt lu">multiply("10000x10000.txt")(sc)</span><span id="d111" class="kc kd hu kb b fv lv ls l lt lu">res0: Long = 973943 // around 16 mins</span></pre><p id="dc6e" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">正如我们所看到的，Spark 所花费的时间增长得非常快，但是 MPI 能够以很小的时间代价运行这个过程。同时，当乘以 10000x10000 矩阵时，我用完了 Spark 中的堆空间，为了运行这个操作，每个执行器的堆空间必须增加到 12g。</p><p id="4c79" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">同样，这里有一个我们无法避免的权衡。与 MPI 的高性能相比，Spark 的简单性和可靠性更高，但很难编码和维护。</p><p id="91b8" class="pw-post-body-paragraph iy iz hu ja b jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv hn dt translated">归根结底，关键是为工作选择正确的工具，MapR 数据平台能够在 Spark 或 MPI 上运行，不会出现任何问题。</p><h2 id="cf41" class="kc kd hu bd ke kf kg kh ki kj kk kl km jj kn ko kp jn kq kr ks jr kt ku kv kw dt translated">MPI 从文件中读取的注意事项</h2><p id="4809" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated">为了乘矩阵，我们的 MPI 应用程序必须从文件中读取相应的输入。这些文件存储在 MapR-FS 中，MapR-FS 支持不同的接口与之交互，如 HDFS、POSIX 等。我们用于 MPI 的<code class="eh jy jz ka kb b">C</code>代码使用常规的<code class="eh jy jz ka kb b">POSIX</code>调用来读取矩阵，因此每个进程只读取矩阵的一部分，我们称之为<code class="eh jy jz ka kb b">blocks</code>。使用 Spark 时，Spark 尽可能使用数据局部性，这有助于加速处理过程，因为每个处理器都从它运行的节点本地读取数据。在 MPI 中，这依赖于 MapR <code class="eh jy jz ka kb b">POSIX</code>客户端。</p><h1 id="064f" class="lw kd hu bd ke lx ly lz ki ma mb mc km md me mf kp mg mh mi ks mj mk ml kv mm dt translated">结论</h1><p id="b438" class="pw-post-body-paragraph iy iz hu ja b jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv hn dt translated">尽管 MPI 在与 MapR 分布式文件系统交互时存在一些限制，但与 Spark 为<code class="eh jy jz ka kb b">Matrix Multiplication</code>提供的相比，整体处理时间还是相当可观的。另一方面，Spark 提供了在保持整个过程的可靠性和容错性的同时轻松完成这些操作的结构，这些都是 MPI 所缺乏的特性。</p><blockquote class="mn mo mp"><p id="202b" class="iy iz jx ja b jb jc jd je jf jg jh ji mq jk jl jm mr jo jp jq ms js jt ju jv hn dt translated">简单使用和高性能之间的权衡已经存在多年，不能忽视，意识到这一点有助于我们在每种情况下做出决定。</p></blockquote></div></div>    
</body>
</html>