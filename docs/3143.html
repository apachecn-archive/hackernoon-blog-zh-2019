<html>
<head>
<title>Combination of Abstractive &amp; Extractive methods for Text Summarization (Tutorial 7)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于文本摘要的抽象和抽取方法的组合(教程 7)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/combination-of-abstractive-extractive-methods-for-text-summarization-tutorial-7-8a4fb85d67e2#2019-05-18">https://medium.com/hackernoon/combination-of-abstractive-extractive-methods-for-text-summarization-tutorial-7-8a4fb85d67e2#2019-05-18</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/0bdeafb061cd322592ca0cadedb3790c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7tJId7G25i1WquOO_Ecew.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Combining both Abstractive &amp; Extractive methods for text summarization</figcaption></figure><p id="9307" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">本教程是一系列教程中的第七篇，将帮助您使用 tensorflow 构建一个抽象的文本摘要器。</p><p id="9c08" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">今天，我们发现了一些新颖的方法，将复制单词的抽象和提取方法结合起来，用于文本摘要，(<a class="ae ke" href="http://bit.ly/eazysum" rel="noopener ugc nofollow" target="_blank">代码</a>可以在这里找到，在 google colab 的 jupyter 笔记本格式中)，我们将结合生成新单词的概念，从给定的句子中复制单词，我们将<strong class="ji hv">了解这很重要的原因</strong>，我们将详细介绍它实际上是如何完成的！！</p><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff kf"><img src="../Images/3feaf3ab1fbd34a0a8a28d43c1f5f667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="ffbc" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">实际上，您可以通过<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>使用该模型轻松生成您自己的摘要(我已经将该模型添加到 eazymind，因此可以通过简单的 api 调用来调用它，并通过<a class="ae ke" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python 包</a>，因此该模型可以轻松集成到您的应用程序中，而无需设置 tensorflow 环境)，您可以免费注册<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank">，并享受免费使用该 api 的乐趣。</a></p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><blockquote class="kr ks kt"><p id="2552" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">今天我们将讨论这两篇论文中讨论的概念(<a class="ae ke" href="https://www.aclweb.org/anthology/K16-1028" rel="noopener ugc nofollow" target="_blank">使用 Seq </a>的抽象文本摘要)&amp; ( <a class="ae ke" href="https://arxiv.org/pdf/1704.04368.pdf" rel="noopener ugc nofollow" target="_blank">抓住要点:使用指针生成器网络的摘要</a>、<a class="ae ke" href="https://github.com/abisee/pointer-generator" rel="noopener ugc nofollow" target="_blank">他们的报告</a>、<a class="ae ke" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">他们真正<strong class="ji hv">令人惊叹的</strong>博客文章</a>)，他们的工作真的很有帮助，取得了真正伟大的成果，我真的要感谢他们惊人的努力</p></blockquote><p id="1097" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">今天我们会</p><ol class=""><li id="ef8b" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated">讨论我们如何整合文本摘要的抽象和提取方法。</li><li id="c683" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated">快速浏览<a class="ae ke" href="http://bit.ly/eazysum" rel="noopener ugc nofollow" target="_blank">代码</a> &amp; <a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods/tree/master/Implementation%20B%20(Pointer%20Generator%20seq2seq%20network)/PreProcessData" rel="noopener ugc nofollow" target="_blank">数据预处理</a>(我已经将这个模型转换成一个 jupyter 笔记本，可以在 google colab 上无缝运行，数据在 google drive 上找到，所以不需要下载代码和数据，你只需要一个 google colab 会话来运行代码，并将数据从我的 google drive 复制到你的 google drive 上(<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)，并将 Google drive 连接到你的 google colab 笔记本上)。</li><li id="b9ff" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated">这个模型已经被转换为一个<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> api </a>(和一个<a class="ae ke" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python 包</a>)，你可以简单地在你的项目中试用它，而不需要麻烦地实际设置你的 tensorflow 环境，你可以在<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>上免费注册，并且现在免费使用这个 api。</li></ol></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h1 id="fa9a" class="lm ln hu bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj dt translated">0-关于系列</h1><p id="2676" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">这是一系列教程，将帮助您在多种方法中使用 tensorflow 构建一个抽象的文本摘要器，我们称之为抽象，因为我们教神经网络生成单词而不仅仅是复制单词，今天我们将这些概念与提取概念相结合，以获得两个世界的好处。</p><p id="5b9b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">0.<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a>(如何使用 google colab 和 google drive)</p><ol class=""><li id="4346" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated"><a class="ae ke" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">概述文本摘要任务和用于该任务的不同技术</a></li><li id="0ee3" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><a class="ae ke" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">使用的数据以及如何表示我们的任务</a>(本教程的先决条件)</li><li id="7bcc" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><a class="ae ke" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">什么是 seq2seq 文本摘要，为什么</a></li><li id="8e99" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><a class="ae ke" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">多层双向 LSTM/GRU </a></li><li id="9884" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><a class="ae ke" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">光束搜索&amp;注意文字摘要</a></li><li id="e0f1" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><a class="ae ke" href="https://hackernoon.com/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55" rel="noopener ugc nofollow" target="_blank">建立 seq2seq 模型，注意&amp;波束搜索</a></li></ol><p id="54e0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在这些教程中，我们构建了一个角石模型，它将在今天的基础上得到增强，因为所有最新的方法都建立在这个角石基线模型上</p><p id="91bb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">让我们开始吧！！</p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h1 id="9fa5" class="lm ln hu bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj dt translated">1-为什么要复制？</h1><p id="ba22" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">上一个<a class="ae ke" href="https://hackernoon.com/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55" rel="noopener ugc nofollow" target="_blank">教程</a>，我们已经建立了一个 seq2seq 模型，具有注意力和波束搜索能力，能够进行抽象文本摘要，结果确实不错，但它会遇到一些问题，<strong class="ji hv">词汇表外的单词</strong> (OOV)，</p><p id="8b87" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji hv">1–1 个词汇以外的单词</strong></p><p id="74fa" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">哪些是看不见的单词，实际上这个问题来自于这样一个事实，即我们用有限的词汇来训练我们的模型(因为词汇不可能包含所有的英语单词)，所以在测试中，我们的模型会面对他以前没有见过的新单词，通常我们会将这些单词建模为<unk>，但实际上这不会生成好的摘要！！</unk></p><p id="6d8c" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji hv">1–2 错误的事实信息</strong></p><p id="021d" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">另一个问题是没有准确地产生事实信息</p><blockquote class="kr ks kt"><p id="aa68" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">给出一句话:在昨晚的比赛中，<em class="hu">德国队以 3:2</em>击败了阿根廷队 <strong class="ji hv"> <em class="hu"/></strong></p><p id="19d2" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">模型会生成:<em class="hu">德国击败阿根廷</em><strong class="ji hv"><em class="hu">2–1</em></strong></p></blockquote><p id="55af" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这是因为标记<strong class="ji hv">3–2</strong>实际上是唯一的，(不是未知的，而是唯一的)，模型更难重新生成，因此如果模型能够从原始句子中复制标记<strong class="ji hv">3–2</strong>而不是自己生成它，就会容易得多。</p><p id="c322" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji hv">1–3 用相似的错误名称替换名称</strong></p><p id="3538" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">另一个问题是人和国家的确切名称，因为我们的模型实际上会使用单词嵌入的概念将相同的国家聚集在一起，所以我们会看到模型实际上看到两个单词(<em class="ku">德里</em> &amp; <em class="ku">孟买</em>)相同，并且会看到像(<em class="ku">安娜</em> &amp; <em class="ku">艾米丽</em>)一样的名称，因为它们会有相似的单词嵌入。</p><blockquote class="kr ks kt"><p id="0374" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">因此，我们将实现一个能够从原始句子中复制独特单词的模型，因为我们的模型很难自己重新生成这些单词，这种技术被称为<strong class="ji hv">指针生成器</strong></p></blockquote></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h1 id="f6d8" class="lm ln hu bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj dt translated">2-什么是<strong class="ak">指针发生器</strong>？</h1><p id="be1f" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">这实际上是一个神经网络，它被训练来学习何时<strong class="ji hv">生成新词</strong>，何时<strong class="ji hv">从原句</strong>复制词。</p><p id="53c0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">它被称为<strong class="ji hv">指针生成器网络</strong>，因为我们使用指针来指出将从原句中复制的单词。</p><h2 id="40e8" class="mp ln hu bd lo mq mr ms ls mt mu mv lw jr mw mx ma jv my mz me jz na nb mi nc dt translated">2–1 我们的基本结构</h2><blockquote class="kr ks kt"><p id="d0fd" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">这个图是从(<a class="ae ke" href="https://arxiv.org/pdf/1704.04368.pdf" rel="noopener ugc nofollow" target="_blank">言归正传:用指针生成器网络进行总结</a>、<a class="ae ke" href="https://github.com/abisee/pointer-generator" rel="noopener ugc nofollow" target="_blank">他们的回购</a>、<a class="ae ke" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">他们真正的<strong class="ji hv">惊人的</strong>博文</a>)中借来的</p></blockquote><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nd"><img src="../Images/89c1fa1755df6317a5befd1eb3cbaa48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mWNq64X-gTbQHbax.png"/></div></div></figure><p id="becb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">基本结构构建为一个<a class="ae ke" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank"> seq2seq </a>模型(<a class="ae ke" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">多层双向 LSTM </a>编码器&amp;一个带<a class="ae ke" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">波束搜索&amp;注意</a>的解码器)并生成输出语句，我们使用两者的输出</p><ol class=""><li id="ee9e" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated"><strong class="ji hv">解码器</strong></li><li id="ad10" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><strong class="ji hv">注意力</strong>(上下文向量)(即:注意力实际上告诉我们哪些单词对我们的输入是重要的)</li></ol><p id="7e25" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">从这两个输出中，我们将生成所有词汇的概率分布，这被称为<strong class="ji hv">词汇分布</strong>，这个分布帮助我们生成最终输出</p><blockquote class="kr ks kt"><p id="5fdd" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">请记住，这里有两个重要的发行版:</p><p id="e232" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">1-一个<strong class="ji hv">局部</strong>分布(<strong class="ji hv">注意</strong>)，它告诉输入句子中哪些单词是重要的</p><p id="a6e6" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">2-这个<strong class="ji hv">局部</strong>分布(<strong class="ji hv">注意力</strong>)用于计算<strong class="ji hv">全局</strong>分布(<strong class="ji hv">词汇分布</strong>)，它根据 vocab 的<strong class="ji hv">所有</strong>单词来告诉输出的相关概率</p></blockquote><h2 id="de7c" class="mp ln hu bd lo mq mr ms ls mt mu mv lw jr mw mx ma jv my mz me jz na nb mi nc dt translated">2–2 现在让我们添加指针生成器网络</h2><p id="9094" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">指针生成器网络在这里将是一个神经网络，被训练来选择从哪里生成输出，或者从</p><ol class=""><li id="ff47" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated"><strong class="ji hv">全局</strong>分布(<strong class="ji hv">词汇分布</strong>)，即:生成新颖的新词</li><li id="a964" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated">或者从<strong class="ji hv">局部</strong>分布(<strong class="ji hv">注意</strong>)，即:从原句中复制单词</li></ol><blockquote class="kr ks kt"><p id="fdd1" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated">这个图表和公式都是借用了(<a class="ae ke" href="https://arxiv.org/pdf/1704.04368.pdf" rel="noopener ugc nofollow" target="_blank">言归正传:用指针生成网络进行总结</a>，<a class="ae ke" href="https://github.com/abisee/pointer-generator" rel="noopener ugc nofollow" target="_blank">他们的回购</a>，<a class="ae ke" href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html" rel="noopener ugc nofollow" target="_blank">他们的真实<strong class="ji hv">惊人</strong>博文</a>)</p></blockquote><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nd"><img src="../Images/2206b402b5c9d906334bf27aaeedb83e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vJgFcKRJpdN1sO2Z.png"/></div></div></figure><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div class="fe ff ne"><img src="../Images/c7e7a4371d99f5b8d810e5a4b7d959f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*d5-NvQoNbTwH24Z_DR_Hag.png"/></div></figure><p id="957a" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">因此我们将有一个参数<strong class="ji hv"> Pgen </strong>，它将包含从<strong class="ji hv"> Vocab 分布</strong> (P vocab)或从<strong class="ji hv">注意力分布</strong>(单词的注意力总和)生成单词的概率，(即:要么生成一个新单词，要么从句子中复制该单词)</p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h1 id="c612" class="lm ln hu bd lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj dt translated">3-如何构建指针生成器网络</h1><p id="ee3a" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">实现该网络有两种主要方法，两种方法都依赖于相同的概念，只是在实现上略有不同，</p><p id="b8af" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">主要投入将是</p><ol class=""><li id="3281" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated">解码器输入</li><li id="37f1" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated">注意力输入</li></ol><p id="acab" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="https://www.aclweb.org/anthology/K16-1028" rel="noopener ugc nofollow" target="_blank">使用序列间 RNNs 和超越论文的抽象文本摘要</a></p><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/26abf8d4bd1cb8923664fda4e63f5368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*xy189Sco3ZQQxr20Oh-0hQ.png"/></div></figure><p id="3475" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这里是 Pgen，这里我们通过训练一个 sigmoid 层得到它，</p><p id="c93c" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">输入将是</p><ol class=""><li id="93bf" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd ld le lf lg dt translated"><strong class="ji hv"> hi : </strong>解码器隐藏状态(解码器输出)→ <strong class="ji hv">解码器</strong>参数</li><li id="5eff" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><strong class="ji hv">E[oi1]</strong>:解码器步骤的前一时间步→ <strong class="ji hv">解码器</strong>参数</li><li id="dc23" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd ld le lf lg dt translated"><strong class="ji hv"> ci </strong>:注意力加权上下文向量→ <strong class="ji hv">注意力</strong>输入</li></ol><p id="fa46" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">Ws h、Ws e、Ws c、b s 和 v s 是可学习的参数。</p><p id="f6dd" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ke" href="https://arxiv.org/pdf/1704.04368.pdf" rel="noopener ugc nofollow" target="_blank">抓住要点:用指针生成器网络进行总结</a></p><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ng"><img src="../Images/90f0e21f88ff22763e7e2498f25705ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKmmGdBesN4RPnYcLJaURQ.png"/></div></div></figure><p id="c6f6" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">1.<strong class="ji hv"> st : </strong>解码器状态→ <strong class="ji hv">解码器</strong>参数</p><p id="c1a7" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">2.<strong class="ji hv"> xt </strong>:解码器输入→ <strong class="ji hv">解码器</strong>参数</p><p id="1f39" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">3.<strong class="ji hv">ht∫</strong>:上下文向量→ <strong class="ji hv">注意</strong>输入</p><p id="1c40" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">其中，矢量 wh∫、ws、wx 和标量 bptr 是可学习的参数</p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h2 id="a6ab" class="mp ln hu bd lo mq mr ms ls mt mu mv lw jr mw mx ma jv my mz me jz na nb mi nc dt translated">4-张量流实现</h2><p id="1d4c" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated"><a class="ae ke" href="https://github.com/abisee" rel="noopener ugc nofollow" target="_blank"> abisee </a>已经使用 tensorflow 实现了论文<a class="ae ke" href="https://arxiv.org/pdf/1704.04368.pdf" rel="noopener ugc nofollow" target="_blank">Get To The Point:summary with Pointer-Generator Networks</a>，他的代码基于来自 Google Brain 的<a class="ae ke" href="https://github.com/tensorflow/models/tree/master/textsum" rel="noopener ugc nofollow" target="_blank"> TextSum 代码</a>。</p><p id="0332" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我修改了他的代码(<a class="ae ke" href="http://bit.ly/eazysum" rel="noopener ugc nofollow" target="_blank">我的修改</a>)</p><ul class=""><li id="3796" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd nh le lf lg dt translated">在 jupyter 笔记本上运行，在 google colab 上无缝运行</li><li id="4982" class="ky kz hu ji b jj lh jn li jr lj jv lk jz ll kd nh le lf lg dt translated">并且已经将<a class="ae ke" href="https://drive.google.com/open?id=15c2wPpL4MGCooDx8Y1dw9M0o05P-EJto" rel="noopener ugc nofollow" target="_blank">数据</a>上传到 google drive，很容易整合到 google colab 中(更多关于这个的<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank"/></li></ul><p id="17a4" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">所以不需要下载代码，也不需要下载数据，你只需要一个 google colab 会话来运行代码，将数据从我的 google drive 复制到你的 google drive 上(关于这个还有<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">，将 Google drive 连接到你的 google colab 笔记本上)</a></p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h2 id="fedd" class="mp ln hu bd lo mq mr ms ls mt mu mv lw jr mw mx ma jv my mz me jz na nb mi nc dt translated">5-数据表示</h2><p id="a2ed" class="pw-post-body-paragraph jg jh hu ji b jj mk jl jm jn ml jp jq jr mm jt ju jv mn jx jy jz mo kb kc kd hn dt translated">这个模型是建立在 CNN/每日邮报的数据集上的，这个数据集是为同一个故事建立多个摘要的</p><p id="9ecc" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">通过将数据运行到一个<a class="ae ke" href="https://github.com/abisee/cnn-dailymail" rel="noopener ugc nofollow" target="_blank">脚本</a>中，将数据转换成分块的二进制文件，然后提供给模型，从而将数据提供给模型</p><p id="ba9e" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我已经修改了这个脚本(<a class="ae ke" href="http://v" rel="noopener ugc nofollow" target="_blank">我的修改</a>)，使其更加容易(以防您需要重新处理自己的数据)</p><p id="3a76" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">原始脚本期望以. story 格式提供数据，这是一个在同一个文件中包含文本和摘要的数据文件，所以我只是编辑得更简单，现在您可以以 csv 格式将您的数据提供给<a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods/tree/master/Implementation%20B%20(Pointer%20Generator%20seq2seq%20network)/PreProcessData" rel="noopener ugc nofollow" target="_blank">我的脚本</a></p><p id="7c9b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我还用更简单的 nltk tokenizer 取代了下载特定 java script (Stanford CoreNLP)进行标记化的需要(希望这能有所帮助)</p></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><h2 id="7526" class="mp ln hu bd lo mq mr ms ls mt mu mv lw jr mw mx ma jv my mz me jz na nb mi nc dt translated">6- <a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> EazyMind </a> API</h2><figure class="kg kh ki kj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff kf"><img src="../Images/3feaf3ab1fbd34a0a8a28d43c1f5f667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div></div></figure><p id="8d74" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">如果你需要尝试这个模型(在尝试<a class="ae ke" href="http://bit.ly/eazysum" rel="noopener ugc nofollow" target="_blank">代码</a>之前)，你可以通过<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>很容易地做到这一点，这是一个<strong class="ji hv">免费的人工智能即服务平台</strong>，为抽象文本摘要提供这个指针生成器模型</p><p id="16e2" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">你也可以免费<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank">resister</a>通过 curl 调用这个模型作为 api</p><pre class="kg kh ki kj fq ni nj nk nl aw nm dt"><span id="113b" class="mp ln hu nj b fv nn no l np nq">curl -X POST \<br/>  <a class="ae ke" href="http://eazymind.herokuapp.com/arabic_sum/eazysum" rel="noopener ugc nofollow" target="_blank">http://eazymind.herokuapp.com/arabic_sum/eazysum</a> \<br/>  -H 'cache-control: no-cache' \<br/>  -H 'content-type: application/x-www-form-urlencoded' \<br/>  -d 'key=xxxxxxxxx&amp;sentence=Facebook%20CEO%20Mark%20Zuckerberg%2C%20left%2C%20makes%20the%20keynote%20speech%20at%20F8%2C%20the%20Facebook%26%2339%3Bs%20developer%20conference%2C%20Tuesday%2C%20April%2030%2C%202019%2C%20in%20San%20Jose%2C%20Calif.%20(AP%20Photo%2FTony%20Avelar%20)%0AFacebook%20says%20that%2C%20unlike%20its%20past%2C%20its%20future%20is%20privacy%0AA%20trader%20works%20ahead%20of%20the%20closing%20bell%20on%20the%20floor%20of%20the%20New%20York%20Stock%20Exchange%20(NYSE)%20on%20April%2012%2C%202019%20in%20New%20York%20City.%20(Photo%20by%20Johannes%20EISELE%20%2F%20AFP)%20%20%20%20%20%20%20%20(Photo%20credit%20should%20read%20JOHANNES%20EISELE%2FAFP%2FGetty%20Images)%0AResilience%20is%20still%20the%20word%20for%20stocks'</span></pre><p id="bcc7" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">或者通过<a class="ae ke" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python 包</a></p><pre class="kg kh ki kj fq ni nj nk nl aw nm dt"><span id="2a07" class="mp ln hu nj b fv nn no l np nq">pip install eazymind</span></pre><p id="9292" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">那就干脆称之为</p><pre class="kg kh ki kj fq ni nj nk nl aw nm dt"><span id="8edc" class="mp ln hu nj b fv nn no l np nq">from nlp.eazysum import Summarizer<br/>key = "xxxxxxxxxxxxxxxxxxxxx"</span><span id="9441" class="mp ln hu nj b fv nr no l np nq">sentence = """(CNN)The White House has instructed former    White House Counsel Don McGahn not to comply with a subpoena    for documents from House Judiciary Chairman Jerry Nadler,     teeing up the latest in a series of escalating oversight     showdowns between the Trump administration and congressional Democrats.      McGahn's decision not to comply     with the subpoena could push Nadler     to hold McGahn in contempt of Congress,     just as he's moving to do with Attorney General William Barr after the Justice     Department defied a subpoena for the unredacted Mueller report and underlying evidence."""  </span><span id="95c1" class="mp ln hu nj b fv nr no l np nq">summarizer = Summarizer(key)<br/>print(summarizer.run(sentence))</span></pre></div><div class="ab cl kk kl hc km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hn ho hp hq hr"><p id="5b37" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">如果上帝愿意，下一次我们会去的</p><ul class=""><li id="a4f7" class="ky kz hu ji b jj jk jn jo jr la jv lb jz lc kd nh le lf lg dt translated">结合<strong class="ji hv">强化学习和 seq2seq </strong>用于摘要文本摘要的新方法</li></ul><p id="56a4" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">(<a class="ae ke" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">更多关于文本摘要 seq2seq 的不同实现</a>)</p><p id="9a3f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">本教程的所有代码都可以在这里找到。</p><blockquote class="kr ks kt"><p id="a6b5" class="jg jh ku ji b jj jk jl jm jn jo jp jq kv js jt ju kw jw jx jy kx ka kb kc kd hn dt translated"><em class="hu">我真心希望你喜欢阅读本教程，我希望我已经把这些概念讲清楚了，这一系列教程的所有代码都可以在这里找到</em><a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"><em class="hu"/></a><em class="hu">，你可以简单地使用 google colab 来运行它，请查看教程和代码并告诉我你对它的看法，别忘了尝试一下</em><a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"><em class="hu">【eazymind</em></a><em class="hu">来免费生成文本摘要，希望再次见到你</em></p></blockquote></div></div>    
</body>
</html>