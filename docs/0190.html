<html>
<head>
<title>ResNet: Block Level Design with Deep Learning Studio |PART 1|</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ResNet:使用深度学习工作室进行块级设计|第1部分|</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/resnet-block-level-design-with-deep-learning-studio-part-1-727c6f4927ac?source=collection_archive---------14-----------------------#2019-01-08">https://medium.com/hackernoon/resnet-block-level-design-with-deep-learning-studio-part-1-727c6f4927ac?source=collection_archive---------14-----------------------#2019-01-08</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><h1 id="0459" class="ir is hu bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dt translated">1 —深度神经网络的问题</h1><p id="2280" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">非常深的网络的主要好处是它可以表示非常复杂的功能。它还可以学习许多不同抽象层次的特性，从边缘(较低层)到非常复杂的特性(较深层)。然而，使用更深的关系网并不总是有帮助的。训练它们的一个巨大障碍是梯度消失:非常深的网络通常有一个梯度信号，它很快变为零，从而使梯度下降慢得令人无法忍受。更具体地说，在梯度下降过程中，当您从最后一层反向投影回第一层时，您在每一步上都乘以权重矩阵，因此梯度可以按指数规律快速下降到零(或者，在极少数情况下，按指数规律快速增长并“爆炸”以获得非常大的值)。</p><div class="kn ko fm fo kp kq"><a href="https://hackernoon.com/exploding-and-vanishing-gradient-problem-math-behind-the-truth-6bd008df6e25" rel="noopener  ugc nofollow" target="_blank"><div class="kr ab ej"><div class="ks ab kt cl cj ku"><h2 class="bd hv fv z el kv eo ep kw er et ht dt translated">爆炸和消失的梯度问题:真相背后的数学</h2><div class="kx l"><h3 class="bd b fv z el kv eo ep kw er et ek translated">你好星尘！今天我们将看到爆炸和消失梯度问题背后的数学原因，但首先让我们…</h3></div><div class="ky l"><p class="bd b gc z el kv eo ep kw er et ek translated">hackernoon.com</p></div></div><div class="kz l"><div class="la l lb lc ld kz le lf kq"/></div></div></a></div><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff lg"><img src="../Images/edf425c556db6bc053c3e1552d559a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fzfbjRP-Ki2-rftR6O5HWA.png"/></div></div></figure><p id="f6c5" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">因此，在训练过程中，随着训练的进行，您可能会看到早期层的梯度幅度(或范数)非常快地下降到零。</p><h1 id="a63d" class="ir is hu bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dt translated">2 —构建剩余网络</h1><p id="2ea4" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">在ResNets中,“快捷方式”或“跳过连接”允许渐变直接反向传播到更早的层:</p><p id="5678" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">ResNet中使用两种主要类型的块，主要取决于输入/输出维度是相同还是不同。你将在DLS实施这两项计划。</p><p id="f659" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">我们将实现类似于他们的研究论文中提到的resnet块。这不准确。</p><h1 id="d129" class="ir is hu bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dt translated">2.1 —身份模块</h1><p id="fca4" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">标识块是ResNets中使用的标准块，对应于输入激活(比如a[l])与输出激活(比如a[l+2])具有相同维数的情况。为了充实ResNet的identity块中发生的不同步骤，下面是显示各个步骤的另一个图表:</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff lw"><img src="../Images/ff3c072b790c29292188e2237bdc2f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIMU3ekaWGvEdZpQlTUSyg.png"/></div></div><figcaption class="lx ly fg fe ff lz ma bd b be z ek"><strong class="bd mb">Identity block.</strong> Skip connection “skips over” 2 layers.</figcaption></figure><p id="663f" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">上面的路径是“快捷路径”较低的路径是“主路径”在该图中，我们还明确了各层中的CONV2D和ReLU步骤。为了加快训练速度，我们还增加了一个BatchNorm步骤。批次归一化必须沿通道轴进行(模式=1)。</p><p id="b112" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">要了解更多关于DLS的信息:</p><div class="kn ko fm fo kp kq"><a href="https://towardsdatascience.com/iris-genus-classification-deepcognition-azure-ml-studio-4b930f54435a" rel="noopener follow" target="_blank"><div class="kr ab ej"><div class="ks ab kt cl cj ku"><h2 class="bd hv fv z el kv eo ep kw er et ht dt translated">鸢尾属分类|DeepCognition| Azure ML studio</h2><div class="kx l"><h3 class="bd b fv z el kv eo ep kw er et ek translated">界:植物界分支:被子植物目:天冬目:鸢尾科亚科:环烯醚萜族:环烯醚萜属:鸢尾</h3></div><div class="ky l"><p class="bd b gc z el kv eo ep kw er et ek translated">towardsdatascience.com</p></div></div><div class="kz l"><div class="mc l lb lc ld kz le lf kq"/></div></div></a></div><p id="e2bf" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">为了实现下面的，你可以从DLS选择任何数据集。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff md"><img src="../Images/b24780c0a05815ca32c321ce8dde842f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SguxqyAbsfFL9c0iWBz_xg.png"/></div></div><figcaption class="lx ly fg fe ff lz ma bd b be z ek"><strong class="bd mb">Identity Block ResNet</strong></figcaption></figure><p id="c98f" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">以下是各个步骤。</p><p id="36c7" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">主路径的第一部分:</p><ul class=""><li id="7c56" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">第一个CONV2D具有(F1，F1)个滤波器，其形状为(1，1)，跨距为(1，1)。它的填充是“有效的”。</li><li id="c8b8" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">第一个批处理规范是标准化通道轴。</li><li id="f2f4" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">然后应用ReLU激活功能。这没有名字，也没有超参数。</li></ul><p id="4b22" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">主路径的第二部分:</p><ul class=""><li id="d215" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">第三个CONV2D具有(32，32)个形状为(1，1)且跨距为(1，1)的滤波器。它的填充是“有效的”。</li><li id="1fb3" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">第三个批处理规范是标准化通道轴。</li></ul><p id="0ec6" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">最后一步:</p><ul class=""><li id="ce38" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">快捷键和输入被加在一起。</li><li id="2710" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">然后应用ReLU激活功能。这没有名字，也没有超参数。</li></ul><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff ms"><img src="../Images/eb265b744c5c2a4a712b9cb9f6105200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNseIgNXMtNg05K6ZwaO8g.png"/></div></div></figure><h1 id="285e" class="ir is hu bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dt translated">2.2 —卷积块</h1><p id="d40a" class="pw-post-body-paragraph jp jq hu jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hn dt translated">您已经实现了ResNet标识块。接下来，ResNet“卷积块”是另一种类型的块。当输入和输出尺寸不匹配时，可以使用这种类型的块。与identity块的不同之处在于快捷路径中有一个CONV2D层:</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff mt"><img src="../Images/3cb82e79f0a98fca5f002c0bda157017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5wkA4O1IpY-ekXqFh0tUQ.png"/></div></div><figcaption class="lx ly fg fe ff lz ma bd b be z ek"><strong class="bd mb">Figure 4</strong> : <strong class="bd mb">Convolutional block</strong></figcaption></figure><p id="10f7" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">快捷路径中的CONV2D层用于将输入x的大小调整为不同的尺寸，以便在将快捷路径值添加回主路径所需的最终添加中尺寸匹配。例如，要将激活维度的高度和宽度减少2倍，可以使用步长为2的1x1卷积。快捷路径上的CONV2D层不使用任何非线性激活函数。它的主要作用是应用一个(学习的)线性函数来减少输入的维数，以便维数为后面的加法步骤匹配。</p><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff mu"><img src="../Images/9fe59ce46c4ce6d50f09f6ea3a6a91ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYpQrWEvtKq2xP-roAchgA.png"/></div></div></figure><p id="93df" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">卷积块的细节如下。</p><p id="9cb0" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated"><strong class="jr hv">(您可以根据您的数据集选择f、F1、F2、F3的值)</strong></p><p id="74d1" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">主路径的第一部分:</p><ul class=""><li id="ef30" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">第一个CONV2D具有形状为(1，1)的(F1，F1)个滤波器，跨距为(s，s)。</li><li id="97ad" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">第一个BatchNorm正在归一化通道轴(mode=1)。</li><li id="2910" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">然后应用ReLU激活功能。这没有名字，也没有超参数。</li></ul><p id="205e" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">主路径的第二部分:</p><ul class=""><li id="78ad" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">第二个CONV2D具有(f，f)的(F2，F2)滤波器和(1，1)的跨距。</li><li id="2b3e" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">第二个批处理规范是标准化通道轴。</li><li id="4ac4" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">然后应用ReLU激活功能。这没有名字，也没有超参数。</li></ul><p id="24e6" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">主路径的第三部分:</p><ul class=""><li id="5bd0" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">第三个CONV2D具有(1，1)的(F3，F3)滤波器和(1，1)的跨距。</li><li id="8bbf" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">第三个BatchNorm是归一化通道轴(mode=1)。注意，这个组件中没有ReLU激活函数。</li></ul><p id="f150" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">快捷路径:</p><ul class=""><li id="9cbd" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">CONV2D具有形状为(1，1)的(F3，F3)滤波器和跨度为(s，s)的滤波器。它的填充是“有效的”。</li><li id="12e8" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">BatchNorm正在规范化通道轴(模式=1)。</li></ul><p id="6ddd" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">最后一步:</p><ul class=""><li id="000e" class="me mf hu jr b js lr jw ls ka mg ke mh ki mi km mj mk ml mm dt translated">快捷方式和主路径值相加。</li><li id="84c9" class="me mf hu jr b js mn jw mo ka mp ke mq ki mr km mj mk ml mm dt translated">然后应用ReLU激活功能。这没有名字，也没有超参数。</li></ul><figure class="lh li lj lk fq ll fe ff paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="fe ff ms"><img src="../Images/eb265b744c5c2a4a712b9cb9f6105200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNseIgNXMtNg05K6ZwaO8g.png"/></div></div></figure><p id="4acd" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">如果你喜欢这篇文章，请👏和分享😄。更多关于深度学习的文章请关注我的<a class="ae mv" rel="noopener" href="/@maniksoni653"><strong class="jr hv"/></a><a class="ae mv" href="https://www.linkedin.com/in/maniksoni/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hv">LinkedIn</strong></a>。</p><p id="34cf" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">请订阅我的YouTube频道:</p><div class="kn ko fm fo kp kq"><a href="https://www.youtube.com/channel/UCNrS8D0rHKh8k2SU8oDY4Hg?view_as=subscriber?&amp;ab_channel=AIwithMANIK" rel="noopener  ugc nofollow" target="_blank"><div class="kr ab ej"><div class="ks ab kt cl cj ku"><h2 class="bd hv fv z el kv eo ep kw er et ht dt translated">AI与MANIK</h2><div class="kx l"><h3 class="bd b fv z el kv eo ep kw er et ek translated">AI又回来了！</h3></div><div class="ky l"><p class="bd b gc z el kv eo ep kw er et ek translated">www.youtube.com</p></div></div><div class="kz l"><div class="mw l lb lc ld kz le lf kq"/></div></div></a></div><p id="07b4" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">感谢阅读😃</p><p id="f715" class="pw-post-body-paragraph jp jq hu jr b js lr ju jv jw ls jy jz ka lt kc kd ke lu kg kh ki lv kk kl km hn dt translated">祝你幸福。</p></div></div>    
</body>
</html>