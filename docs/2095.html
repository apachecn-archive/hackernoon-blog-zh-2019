<html>
<head>
<title>Multilayer Bidirectional LSTM/GRU for text summarization made easy (tutorial 4)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多层双向LSTM/GRU使文本摘要变得简单(教程4)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f?source=collection_archive---------2-----------------------#2019-03-31">https://medium.com/hackernoon/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f?source=collection_archive---------2-----------------------#2019-03-31</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/6770b3a83907dad0175be3bbcc7c4ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iq_RCJXjfzbecZl-bSeiEg.jpeg"/></div></div></figure><p id="362a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程是一系列教程中的第四个，它将帮助你使用tensorflow构建一个抽象的文本摘要器，今天我们将讨论对核心RNN seq2seq模型的一些有用的修改，我们在上一个教程中已经介绍过<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank"/></p><p id="cf0c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这些修改是</p><ol class=""><li id="1669" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">RNN修改(GRU和LSTM)</li><li id="c5df" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">双向网络</li><li id="0ce3" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">多层网络</li></ol></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h2 id="0ee5" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">关于系列</h2><p id="2676" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">这是一系列教程，将帮助您使用tensorflow使用多种方法构建一个抽象的文本摘要器，<strong class="je hv"> <em class="lw">您不需要下载数据，也不需要在您的设备</em> </strong>上本地运行代码，因为<strong class="je hv">数据</strong>可以在<strong class="je hv"> google drive </strong>上找到，(您可以简单地将其复制到您的google drive，在此了解更多<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank"/>，本系列的<strong class="je hv">代码</strong>是用Jupyter笔记本编写的，可以在上面运行</p><p id="5b9b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">0.<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a>(如何使用google colab和google drive)</p><ol class=""><li id="4346" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">概述文本摘要任务和用于该任务的不同技术</a></li><li id="0ee3" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">使用的数据以及如何表示我们的任务</a></li><li id="7bcc" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">什么是seq2seq文本摘要，为什么</a></li></ol><p id="1ec0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们开始吧</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="ab fr cl mb"><img src="../Images/16577fbdd2beff9f3d1dcb3f38417358.png" data-original-src="https://miro.medium.com/v2/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="8b81" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我在一个网站上添加了一个文本摘要模型<a class="ae ka" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>，这样你就可以自己尝试生成你自己的摘要(看看你能构建什么)，它可以通过简单的api调用来调用，并且通过<a class="ae ka" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python包</a>，这样文本摘要就可以很容易地集成到你的应用程序中，而不需要设置tensorflow环境的麻烦，你可以免费注册，并享受免费使用这个api的乐趣。</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h1 id="fd4c" class="mg kx hu bd ky mh mi mj lc mk ml mm lg mn mo mp lj mq mr ms lm mt mu mv lp mw dt translated">快速回顾</h1><p id="adf6" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">我们的任务是文本摘要，我们称之为抽象，因为我们教神经网络生成单词，而不仅仅是复制单词。</p><p id="990e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">将被使用的数据将是新闻和它们的标题，可以在我的google drive上找到，所以你只需将它复制到你的google drive上，而不需要下载它</p><p id="2ee6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将使用单词嵌入来表示数据，这只是简单地将每个单词转换成一个特定的向量，我们将为我们的单词创建一个字典(<a class="ae ka" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)</p><p id="865a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对于这项任务，有<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">种不同的方法</a>，它们建立在基石概念的基础上，并且它们继续开发和建立，它们从一种叫做RNN的网络开始，这种网络被安排在一种叫做seq2seq的编码器/解码器架构中(<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)，这些不同方法的代码可以在这里<a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">找到</a></p><blockquote class="mx my mz"><p id="6b92" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated"><em class="hu">本教程一直以</em> <strong class="je hv"> <em class="hu">吴君如</em></strong><em class="hu"/><a class="ae ka" href="https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt" rel="noopener ugc nofollow" target="_blank"><em class="hu">的惊人之作为基础，他的课程对RNN </em> </a> <em class="hu">已经真正有用了，推荐你去看一下</em></p></blockquote></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><p id="95a2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt nd translated">今天，我们将对编码器/解码器模型的核心组件进行一些修改，这些修改发生在RNN模块本身，以提高其在整个模型中的效率。</p><h1 id="2c81" class="mg kx hu bd ky mh nm mj lc mk nn mm lg mn no mp lj mq np ms lm mt nq mv lp mw dt translated">1.RNN修改(LSTM和GRU)</h1><p id="62be" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">RNN装置有两个主要问题</p><ol class=""><li id="eb15" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated"><strong class="je hv">爆炸梯度:</strong>发生在深层网络中(即:有许多层的网络<em class="lw"/>就像我们的例子)，当我们应用反向传播时，梯度会变得太大。实际上，使用<strong class="je hv">梯度削波</strong>的概念，可以很容易地解决这个误差，这只是简单地设置一个特定的阈值，当梯度超过阈值时，我们会将其削波到某个值。</li><li id="4551" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><strong class="je hv">消失梯度:</strong>这证明是一个更难解决的问题，这也发生在<em class="lw">由于大量的层</em>，但这是由于正常的RNN单元不能记住在序列早期出现的旧值</li></ol><blockquote class="mx my mz"><p id="0fa3" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated">这在处理nlp问题时非常重要，因为一些单词依赖于出现在句子早期的单词，如</p></blockquote><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/c9338585a4541f70ad65c665688bb348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*a_KfZnu3xgZD0lWDyV2F6w.jpeg"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">Here the word cat/cats which appeared early in the sentence would directly affect choosing either was/were later in the sentence.</figcaption></figure><p id="27b0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了解决这个问题，我们需要一个新的RNN架构，这里我们将讨论两种主要方法:</p><ol class=""><li id="e658" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">GRU(门控循环单元)</li><li id="774f" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">LSTM(长短期记忆)</li></ol><h2 id="c2ed" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">1.A) GRU(门控循环单元)</h2><p id="5d09" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">GRU和LSTM都解决了普通RNN单元所遭受的消失梯度的问题，他们通过在他们的网络中实现一个存储单元来做到这一点，这使他们能够在序列中存储早期的数据，以便在序列中稍后使用。</p><p id="eab5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这里我们将讨论GRU(门控循环单元)，我们从RNN的激活方程开始(<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ns"><img src="../Images/d424d5a7a1f8837a77278f0076f2588a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDjYWB3LZgmpFZGJ6vw7YA.jpeg"/></div></div></figure><p id="f648" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后我们会对它进行一些简单的修改</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nt"><img src="../Images/ebc14fe91c6783fbc4d1939cf57dae9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/1*jMz_8MccHwSYW3wXibFKow.gif"/></div></figure><p id="f490" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">直到我们最终</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nu"><img src="../Images/f86ec63fb169bad744cb6e5c481e2d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*153_kybJiYBolwFqMIaMEw.jpeg"/></div></div></figure><p id="0551" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> c </strong>这里表示为存储单元，这里它将是GRU单元的输出。</p><p id="82c2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> N </strong>子字母表示它是新提出的c值(我们稍后将使用它来生成GRU的实际c输出)。</p><p id="fcf5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，这里新的建议输出c(候选)，将取决于旧的输出c(旧的候选)，以及当时的当前输入</p><p id="bfdb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了记住C(候选)的值，我们使用另一个名为F(门更新)的参数，这将控制我们是否更新C的值</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nv"><img src="../Images/06f39c88dd9b055e8bb2521e1b6d7da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8voQEGDvGLccobsZaLN1Q.jpeg"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">here we would use a sigmoid function , we would take into consideration <strong class="bd nw">the old c</strong> , and the<strong class="bd nw"> current input X</strong></figcaption></figure><p id="2a3d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，为了更新C的值，我们将使用</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nx"><img src="../Images/428754c58bc5fc43f7f379d412a659fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B33Iv2fxrYpsgqKLEcxlqA.jpeg"/></div></div></figure><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff ny"><img src="../Images/061a627e6c94e99ef35bdfbafd4287bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*jKgYE-eg08YUD8IAU6QESQ.jpeg"/></div></figure><p id="3fd7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">让我们假设C是一个向量，它的第一个元素将<strong class="je hv">记住句子中的重要特征，</strong>这里我们假设这个特征是单词是猫还是猫</p><p id="b955" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，首先c向量是空的，直到我们看到单词<strong class="je hv"> cat </strong>，然后F将被设置为1以记住它是一个单数单词，并且它<strong class="je hv">将保持</strong>的值，直到它在句子的后面被使用(以生成‘was’而不是‘were’)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nz"><img src="../Images/0fc51df8dadbde22689e93c4a46635b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FG1kFD8zpKg2ER3vYs1ELQ.gif"/></div></div></figure><p id="5c08" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">构建完整的GRU单元还需要做另一个修改，它发生在创建新的候选C所需的函数上。</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/d3d99cade2a8dfda5b29ed47993613ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*o_s_6ne_QW2ITfXsE_HiDA.jpeg"/></div></figure><p id="a3eb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们将有一个可学习的(<strong class="je hv"> Fr </strong>)参数来学习<strong class="je hv"> C new </strong>和<strong class="je hv"> C old </strong>之间的相关性</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ob"><img src="../Images/56db6e79789750a171529f0492ce29c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sq3qu-vj3Zuo_LCJ8v3QcA.jpeg"/></div></div></figure><p id="9121" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">总而言之，我们有四个主要的方程来支配GRU</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oc"><img src="../Images/a58bfb081003792276dcfba99c8a01a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ok-Ah0zmps5N0eAG6wDXAw.jpeg"/></div></div></figure></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h2 id="cb56" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">1.LSTM(长短期记忆)</h2><p id="0c5a" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">LSTM是对RNN的另一个修改，它也是使用相同的内存概念构建的，以记住长序列的数据，它是在GRU之前提出的，所以GRU实际上是对LSTM的简化</p><p id="6202" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在LSTM这里，</p><ol class=""><li id="f811" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">我们使用激活值，不仅仅是C(候选值)，</li><li id="200d" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">我们也有来自单元的2个输出，一个新的激活，和一个新的候选值</li></ol><p id="ebc9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以为了计算新的候选人</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff od"><img src="../Images/60e06dcbe6e268f6419d5e7be2904f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K9GCUC2FafZKpEdtgFSZDw.jpeg"/></div></div></figure><p id="c7a9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在LSTM，我们通过三个不同的门来控制存储单元</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oe"><img src="../Images/c9049c00c42f79d3276b0c074277aef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFoQxAP7bYbigUq13cjGfQ.jpeg"/></div></div></figure><p id="35b2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">正如我们之前所说，我们有2个来自LSTM的输出，新的候选和新的激活，在它们中我们将使用先前的门</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff of"><img src="../Images/49c5052942ae4a23887a8442e9f63423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7OFMDguv1Ox2egrMAVJOBw.jpeg"/></div></div></figure><p id="b820" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">将所有这些结合在一起</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff og"><img src="../Images/9a1f77082cc9e794e4f8f8b0e7b6606c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k1wSDVWIX0uDGPCAyxy_gA.jpeg"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">we could also output y prediction from LSTM (by passing them to softmax )</figcaption></figure><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oh"><img src="../Images/42b778342038cd550ef1b3355b58385d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvD20LTkOYvSN6BJPpKdkA.jpeg"/></div></div></figure><p id="8da6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当我们将多个LSTMs连接在一起时，我们可以看到，如果网络正确地学习了门参数，我们可以将候选值(红色值)从序列的早期传递到序列的最末端，因此我们可以高精度地对长相关性进行建模</p><h1 id="b21f" class="mg kx hu bd ky mh nm mj lc mk nn mm lg mn no mp lj mq np ms lm mt nq mv lp mw dt translated">2.双向网络</h1><p id="5a16" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">这是对普通RNN网络的一种改进，使它能够适应nlp问题中的一个重要需求，</p><blockquote class="mx my mz"><p id="4f83" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated">就像在nlp中一样，有时为了理解一个单词，我们不仅需要理解前面的单词，还需要理解后面的单词，就像这个例子一样</p></blockquote><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oi"><img src="../Images/66ae6543e0240f6ee2a79b78dcd92519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/1*EBQaJPDi0CnfUL6YqHqhuw.gif"/></div></figure><p id="d481" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了区分单词<strong class="je hv"> teddy </strong>的两种不同含义(一次是人名的一部分，另一次是单词bear的一部分),我们需要寻找下一个单词，这就是我们需要应用双向网络的原因</p><p id="c41b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">双向网络是一种通用架构，可以利用任何RNN模型(正常的RNN、GRU、LSTM)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oi"><img src="../Images/2372f9325fd39e97a8a6a5e5ca6d144e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/1*U96JgeQ4V4VSMsW3634rOQ.gif"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">forward propagation for the 2 direction of cells</figcaption></figure><p id="c4d5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们应用两次前向传播，一次用于前向单元，一次用于后向单元</p><p id="5831" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">两种激活(向前、向后)都将被考虑来计算时间t处的输出y^</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ca"><img src="../Images/7b54733f880656a18624ab065cc1c597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxBvFed1KUfwof6rQ-5K4Q.jpeg"/></div></div></figure><h1 id="bb3e" class="mg kx hu bd ky mh nm mj lc mk nn mm lg mn no mp lj mq np ms lm mt nq mv lp mw dt translated">3.多层网络</h1><p id="d1be" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">为了获得更好的结果，我们可以将多个RNN(LSTM、GRU或正常的RNN)堆叠在一起，但我们必须考虑到它们需要时间。</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oj"><img src="../Images/c8674eb5e6fffdd86248cf2ced88bf29.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*6j2y63q1kybMFV78ZocenQ.jpeg"/></div></figure><p id="4a17" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先，这是一个普通的深度网络，我们可以看到它包含多个层(本例中为50层)，而当我们在RNN上应用相同的概念时，我们倾向于选择更少的层数，因为这就足够了，而且计算量很大</p><p id="d3f2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在让我们看看如何将深度网络的概念应用于RNN</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ok"><img src="../Images/99e130f03e3558407ad803e31232dac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/1*WiTldvdh0Cx93reqSwqyBA.gif"/></div></div></figure><p id="4d83" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">正如我们所看到的，由于我们正在研究RNN或其变体，我们必须考虑时间因素，因此每个垂直的单元格列代表一个层，而每个时间进程我们重复这个列</p><p id="407e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以我们的符号是[layer] <time/></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff ol"><img src="../Images/5c0850b1cd82d0c5261b0829023b26d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*zXQkH2P6QjU_kUhaMwmx-w.jpeg"/></div></figure><p id="a8bd" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了获得任何激活层的值，我们使用两者</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff om"><img src="../Images/dbc8035d1f3f54741680a87eea7d4a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7M_D4zbvjkz9TxxVbOEPA.jpeg"/></div></div></figure><ol class=""><li id="974b" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">来自同一层(层2)的时间上(时间2)的先前激活💚<strong class="je hv">绿色</strong></li><li id="f3b9" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">前一层(层1)中同一时间(时间3)的前一个单元格🔵<strong class="je hv">蓝色</strong></li></ol></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><p id="5cc4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">下一次，如果上帝愿意，我们将讨论如何使用</p><ol class=""><li id="a4f7" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">波束搜索</li><li id="993b" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">注意力模型</li></ol><blockquote class="mx my mz"><p id="a6b5" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated"><em class="hu">我真心希望你喜欢阅读本教程，我希望我已经把这些概念讲清楚了，这一系列教程的所有代码都可以在这里找到</em><a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"><em class="hu"/></a><em class="hu">，你可以简单地使用google colab来运行它，请查看教程并告诉我你对它的看法，希望再次见到你</em></p></blockquote><h1 id="d7dd" class="mg kx hu bd ky mh nm mj lc mk nn mm lg mn no mp lj mq np ms lm mt nq mv lp mw dt translated">后续教程</h1><ul class=""><li id="1a9b" class="kb kc hu je b jf lr jj ls jn on jr oo jv op jz oq kh ki kj dt translated"><a class="ae ka" href="http://bit.ly/2G4XCo3" rel="noopener ugc nofollow" target="_blank">波束搜索&amp;注意让文本摘要变得简单(教程5) </a></li><li id="4b66" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz oq kh ki kj dt translated"><a class="ae ka" href="http://bit.ly/2ZeEmvO" rel="noopener ugc nofollow" target="_blank">在Tensorflow的94行中构建一个抽象的文本摘要器！！(教程6) </a></li><li id="7e8a" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz oq kh ki kj dt translated"><a class="ae ka" href="http://bit.ly/2EhcRIZ" rel="noopener ugc nofollow" target="_blank">用于文本摘要的抽象&amp;提取方法的组合(教程7) </a></li></ul><figure class="lx ly lz ma fq iv"><div class="bz el l di"><div class="or os l"/></div></figure></div></div>    
</body>
</html>