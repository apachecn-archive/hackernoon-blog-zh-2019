<html>
<head>
<title>Building a Feedforward Neural Network from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始构建前馈神经网络</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b?source=collection_archive---------0-----------------------#2019-04-09">https://medium.com/hackernoon/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b?source=collection_archive---------0-----------------------#2019-04-09</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/114ad5441bcb786441d9c2a2b7e65bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ntm7DimXomXJlu8S"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Photo by <a class="ae jg" href="https://unsplash.com/@cdr6934?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Chris Ried</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="25b7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在这篇文章中，我们将看到如何用python从头开始实现前馈神经网络。这是我上一篇关于<a class="ae jg" href="https://hackernoon.com/deep-learning-feedforward-neural-networks-explained-c34ae3f084f1" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>的文章的后续。</p><blockquote class="kf kg kh"><p id="8e33" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">引用说明:本文的内容和结构基于四分之一实验室——<a class="ae jg" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">pad hai</a>的深度学习讲座。</p></blockquote><h1 id="cd87" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">前馈神经网络</h1><p id="0a51" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">前馈神经网络也被称为<strong class="jj hv">多层神经元网络</strong> (MLN)。这些模型网络被称为前馈，因为信息仅在神经网络中向前传播，通过输入节点，然后通过隐藏层(单层或多层)，最后通过输出节点。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff lp"><img src="../Images/5336367415d1234149da2740bf8d8919.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*LsmIIg6u4BhGHIykJNa2hA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Generic Network with Connections</figcaption></figure><p id="a44e" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">传统模型，如麦卡洛克皮茨，感知器和Sigmoid神经元模型的能力仅限于线性函数。为了处理输入和输出之间复杂的非线性决策边界，我们使用了多层神经元网络。</p><p id="545d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了理解前馈神经网络学习算法和网络中存在的计算，请参考我以前关于前馈神经网络的文章。</p><div class="lu lv fm fo lw lx"><a href="https://hackernoon.com/deep-learning-feedforward-neural-networks-explained-c34ae3f084f1" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab ej"><div class="lz ab ma cl cj mb"><h2 class="bd hv fv z el mc eo ep md er et ht dt translated">深度学习:解释前馈神经网络</h2><div class="me l"><h3 class="bd b fv z el mc eo ep md er et ek translated">你的第一个深度神经网络</h3></div><div class="mf l"><p class="bd b gc z el mc eo ep md er et ek translated">hackernoon.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ja lx"/></div></div></a></div></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><h1 id="cff1" class="km kn hu bd ko kp mt kr ks kt mu kv kw kx mv kz la lb mw ld le lf mx lh li lj dt translated">编码部分</h1><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff my"><img src="../Images/47ac1d191123e974c61b66008167e8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DOzWNpktUANnOIFw"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Photo by <a class="ae jg" href="https://unsplash.com/@hiteshchoudhary?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hitesh Choudhary</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2e35" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在编码部分，我们将涉及以下主题。</p><ol class=""><li id="fe7f" class="mz na hu jj b jk jl jo jp js nb jw nc ka nd ke ne nf ng nh dt translated"><strong class="jj hv">生成不可线性分离的数据</strong></li><li id="6c91" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">用乙状结肠神经元训练并观察表现</strong></li><li id="2c55" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">从头开始编写我们的第一个前馈网络</strong></li><li id="51ae" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">根据数据训练FF网络，并与Sigmoid神经元进行比较</strong></li><li id="8e2f" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">为一个FF网络写一个通用类</strong></li><li id="44ca" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">训练二元分类的类属</strong></li><li id="3112" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><strong class="jj hv">使用交叉熵损失函数为多类数据训练FF网络</strong></li></ol><p id="6c16" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你想跳过理论部分，直接进入代码，</p><div class="lu lv fm fo lw lx"><a href="https://github.com/Niranjankumar-c/Feedforward_NeuralNetworrks" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab ej"><div class="lz ab ma cl cj mb"><h2 class="bd hv fv z el mc eo ep md er et ht dt translated">niranjankumar-c/前馈_神经网络</h2><div class="me l"><h3 class="bd b fv z el mc eo ep md er et ek translated">从头开始构建我们的神经网络。通过…为Niranjankumar-c/前馈神经网络的发展做出贡献</h3></div><div class="mf l"><p class="bd b gc z el mc eo ep md er et ek translated">github.com</p></div></div><div class="mg l"><div class="nn l mi mj mk mg ml ja lx"/></div></div></a></div><p id="b26c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="ki"> PS:如果你有兴趣把代码转换成</em> <strong class="jj hv"> <em class="ki"> R，</em> </strong> <em class="ki">一旦完成就给我发消息我会在这里和GitHub页面上展示你的作品。</em></p><p id="9c94" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在开始构建我们的网络之前，首先我们需要导入所需的库。我们导入<code class="eh no np nq nr b">numpy</code>来评估两个向量之间的矩阵乘法和点积，<code class="eh no np nq nr b">matplotlib</code>来可视化数据，从<code class="eh no np nq nr b">sklearn</code>包中导入函数来生成数据和评估网络性能。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><h1 id="ca80" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">生成虚拟数据</h1><p id="a0fa" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">请记住，我们使用前馈神经网络是因为我们想要处理非线性可分数据。在本节中，我们将了解如何随机生成非线性可分数据。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="1e83" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了随机生成数据，我们将使用<code class="eh no np nq nr b">make_blobs</code>生成高斯分布的点。我在2D空间生成了1000个数据点，用四个blob<code class="eh no np nq nr b">centers=4</code>作为多类分类预测问题。每个数据点有两个输入和0、1、2或3个类别标签。<strong class="jj hv">第9、10行</strong>中的代码有助于使用散点图可视化数据。我们可以看到，它们有4个中心，数据是线性可分的(几乎)。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/d943c5d7a833978f5ad76915c768c213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*h45yTNjxqi52s3qq-G79zg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Multi-Class Data</figcaption></figure><p id="da80" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在上面的图中，我能够用一个简单的散点图用颜色表示3个维度——2个输入和类别标签。注意<code class="eh no np nq nr b">make_blobs()</code>函数会生成线性可分的数据，但是我们需要有非线性可分的数据进行二分类。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="1ec9" class="nz kn hu nr b fv oa ob l oc od">labels_orig = labels<br/>labels = np.mod(labels_orig, 2)</span></pre><p id="af47" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">将这4个类转换为二进制分类的一种方法是将这4个类的余数除以2，这样我就可以得到新的标签0和1。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/8ca26ff2420db685aa38bb51b0f80477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*8RwYRRDBHyDO_h1I6BUXWQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Binary Class Data</figcaption></figure><p id="bad9" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">从该图中，我们可以看到斑点的中心被合并，使得我们现在有一个二元分类问题，其中决策边界不是线性的。一旦我们准备好数据，我已经使用了<code class="eh no np nq nr b">train_test_split</code>函数以90:10的比例分割<code class="eh no np nq nr b">training</code>和<code class="eh no np nq nr b">validation</code>的数据</p><h1 id="bdea" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">用乙状结肠神经元训练</h1><p id="6c21" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在我们开始训练关于乙状结肠神经元的数据之前，我们将在名为<em class="ki">乙状结肠神经元的类中构建我们的模型。</em></p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="1fb1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在<code class="eh no np nq nr b">SigmoidNeuron</code>类中，我们有9个函数，我将一个接一个地向你介绍这些函数，并解释它们在做什么。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="b254" class="nz kn hu nr b fv oa ob l oc od">def __init__(self):    <br/>    self.w = None    <br/>    self.b = None</span></pre><p id="22f1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">__init__</code>函数(构造函数)有助于将s形神经元<strong class="jj hv"> w </strong>权重和<strong class="jj hv"> b </strong>偏差的参数初始化为无。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="b91c" class="nz kn hu nr b fv oa ob l oc od">#forward pass    <br/>def perceptron(self, x):  <br/>    return np.dot(x, self.w.T) + self.b</span><span id="7756" class="nz kn hu nr b fv oe ob l oc od">def sigmoid(self, x):    <br/>    return 1.0/(1.0 + np.exp(-x))</span></pre><p id="2bd5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们将定义两个函数<code class="eh no np nq nr b">perceptron</code>和<code class="eh no np nq nr b">sigmoid</code>，这两个函数表征了正向传递。在乙状结肠神经元的情况下，正向传递包括两个步骤</p><ol class=""><li id="d7a2" class="mz na hu jj b jk jl jo jp js nb jw nc ka nd ke ne nf ng nh dt translated"><code class="eh no np nq nr b">perceptron</code> —计算输入<strong class="jj hv">x</strong>T32】权重<strong class="jj hv"> w </strong>之间的点积，并加上偏差<strong class="jj hv"> b </strong></li><li id="3cb3" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke ne nf ng nh dt translated"><code class="eh no np nq nr b">sigmoid</code> —获取感知器的输出，并在其上应用sigmoid(逻辑)函数。</li></ol><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="d681" class="nz kn hu nr b fv oa ob l oc od">#updating the gradients using mean squared error loss  <br/>def grad_w_mse(self, x, y):<br/>    .....</span><span id="5754" class="nz kn hu nr b fv oe ob l oc od">def grad_b_mse(self, x, y):<br/>    .....</span><span id="6c81" class="nz kn hu nr b fv oe ob l oc od">#updating the gradients using cross entropy loss  <br/>def grad_w_ce(self, x, y):<br/>    .....</span><span id="97ae" class="nz kn hu nr b fv oe ob l oc od">def grad_b_ce(self, x, y):    <br/>    .....</span></pre><p id="5be5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来的四个函数表征了梯度计算。我已经编写了两个独立的函数，使用均方误差损失和交叉熵损失来更新权重<strong class="jj hv"> w </strong>和偏差<strong class="jj hv"> b </strong>。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="f203" class="nz kn hu nr b fv oa ob l oc od">def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn="mse", display_loss=False):<br/>    .....<br/>    return</span></pre><p id="0ac1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们定义接受几个参数的“fit”方法，</p><p id="83d4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">X</code> —输入</p><p id="7b47" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">Y</code> —标签</p><p id="a0c7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">epochs</code> —我们将允许我们的算法迭代数据的次数，默认值设置为1</p><p id="d4c7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">learning_rate</code> —通过我们的训练数据，我们的体重在每一步中的变化幅度，默认值设置为1</p><p id="28b6" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">intialise</code> —是否随机初始化模型参数。如果设置为<code class="eh no np nq nr b">True</code>，权重将被初始化，如果你想重新训练模型，你可以设置为<code class="eh no np nq nr b">False</code>。</p><p id="f078" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">loss_fn</code> —选择算法的损失函数来更新参数。它可以是“mse”或“ce”</p><p id="8a88" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">display_loss</code> —布尔变量，指示是否显示每个历元的损耗减少</p><p id="7402" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在<code class="eh no np nq nr b">fit</code>方法中，我们检查通过参数X和Y传递的数据，并使用均方损失或交叉熵损失计算参数的更新值。一旦我们更新了值，我们就去更新权重和偏差项(<em class="ki">第49–62行</em>)。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="a069" class="nz kn hu nr b fv oa ob l oc od">def predict(self, X):</span></pre><p id="4759" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在我们定义我们的预测函数将输入<code class="eh no np nq nr b">X</code>作为一个参数，它期望是一个<code class="eh no np nq nr b">numpy</code>数组。在预测函数中，我们将使用训练好的模型计算每个输入的前向传递，并发回一个numpy <code class="eh no np nq nr b">array</code>，其中包含每个输入数据的预测值。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="ac82" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在我们将在我们创建的乙状结肠神经元上训练我们的数据。首先，我们实例化Sigmoid Neuron类，然后对1000个时期和学习率设置为1的训练数据调用<code class="eh no np nq nr b">fit</code>方法(这些值是任意的，不是该数据的最佳值，您可以围绕这些值进行操作，找到最佳时期数和学习率)。默认情况下，损失函数设置为<em class="ki">均方误差损失</em>，但您也可以将其更改为<em class="ki">交叉熵损失</em>。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff of"><img src="../Images/84f2c127910de062511bc4c00fb92462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*OP9TRHyb5N2Kc-eO1a-baw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Sigmoid Neuron Loss Variation</figcaption></figure><p id="03f7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">正如你所看到的，乙状结肠神经元的损失正在减少，但有很多振荡可能是因为大的学习率。您可以降低学习率并检查损失变化。一旦我们训练了模型，我们就可以对测试数据进行预测，并通过将0.5作为阈值来二进制化这些预测。我们可以计算模型的训练和验证准确性，以评估模型的性能，并通过改变时期数或学习率来检查任何改进范围。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="0832" class="nz kn hu nr b fv oa ob l oc od">#visualizing the results<br/>plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap=my_cmap, s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2))<br/>plt.show()</span></pre><p id="ed63" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">了解模型对训练集中每个点的预测正确与否的数据点。我们将使用<code class="eh no np nq nr b">matplotlib.pyplot</code>中的散点图功能。该函数将两个输入作为第一和第二特征，对于颜色，我使用了<code class="eh no np nq nr b">Y_pred_binarised_train</code>并定义了一个自定义的“cmap”用于可视化。正如你所看到的，下图中每个点的大小是不同的。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff og"><img src="../Images/6329f9aef9242619b5fcd2e33c39ff29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*VNg5DhA2yfBetwjGHdoprA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">4D Scatter Plot</figcaption></figure><p id="afe2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">图中每个点的大小由一个公式给出，</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="6e70" class="nz kn hu nr b fv oa ob l oc od">s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2)</span></pre><p id="2f14" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">该公式采用预测值和实际值之间的绝对差值。</p><ul class=""><li id="8c55" class="mz na hu jj b jk jl jo jp js nb jw nc ka nd ke oh nf ng nh dt translated">如果实际值等于预测值，则大小= 3</li><li id="4462" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke oh nf ng nh dt translated">如果实际值不等于预测值，则大小= 18</li></ul><p id="2977" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">图中的所有小点表示模型正确预测了这些观察值，大点表示这些观察值分类不正确。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff og"><img src="../Images/6329f9aef9242619b5fcd2e33c39ff29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*VNg5DhA2yfBetwjGHdoprA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">4D Scatter Plot</figcaption></figure><p id="26be" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在该图中，我们能够表示4个维度-两个输入特征，颜色表示不同的标签，点的大小表示预测是否正确。该图中重要的一点是，乙状结肠神经元不能处理非线性可分离数据。</p><p id="e6cf" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你想学习sigmoid神经元学习算法的详细数学检查我以前的职位。</p><div class="lu lv fm fo lw lx"><a href="https://towardsdatascience.com/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07" rel="noopener follow" target="_blank"><div class="ly ab ej"><div class="lz ab ma cl cj mb"><h2 class="bd hv fv z el mc eo ep md er et ht dt translated">用数学解释的Sigmoid神经元学习算法</h2><div class="me l"><h3 class="bd b fv z el mc eo ep md er et ek translated">在本帖中，我们将详细讨论sigmoid神经元学习算法背后的数学直觉。</h3></div><div class="mf l"><p class="bd b gc z el mc eo ep md er et ek translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="oi l mi mj mk mg ml ja lx"/></div></div></a></div><h1 id="76eb" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">先写前馈神经网络</h1><p id="5c86" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在本节中，我们将采用一个非常简单的前馈神经网络，并用python从头开始构建它。网络总共有三个神经元——两个在第一个隐藏层，一个在输出层。对于这些神经元中的每一个，预激活用‘a’表示，后激活用‘h’表示。在网络中，我们总共有9个参数——6个权重参数和3个偏差项。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff oj"><img src="../Images/09b7b308b6c192fd72cc227c87f7ed5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*2JUgj6ST5dxJ5bV3aXgP8w.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Simple Feedforward Network</figcaption></figure><p id="d451" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">类似于Sigmoid神经元的实现，我们将在一个名为<em class="ki">firstfnetwork的类中编写我们的神经网络。</em></p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="f731" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在<code class="eh no np nq nr b">FirstFFNetwork</code>类中，我们有6个函数，我们将一个接一个地检查这些函数。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="c768" class="nz kn hu nr b fv oa ob l oc od">def __init__(self):    <br/>.....</span></pre><p id="33cc" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">__init__</code>函数初始化网络的所有参数，包括权重和偏差。与神经网络中只有两个参数的sigmoid神经元不同，我们有9个参数需要初始化。所有6个权重被随机初始化，3个偏差被设置为零。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="56c0" class="nz kn hu nr b fv oa ob l oc od">def sigmoid(self, x):    <br/>    return 1.0/(1.0 + np.exp(-x))</span></pre><p id="771a" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们为网络中的每个神经元定义用于后激活的sigmoid函数。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="de6f" class="nz kn hu nr b fv oa ob l oc od">def forward_pass(self, x):    <br/>#forward pass - preactivation and activation    <br/>    self.x1, self.x2 = x    <br/>    self.a1 = self.w1*self.x1 + self.w2*self.x2 + self.b1    <br/>    self.h1 = self.sigmoid(self.a1)    <br/>    self.a2 = self.w3*self.x1 + self.w4*self.x2 + self.b2    <br/>    self.h2 = self.sigmoid(self.a2)   <br/>    self.a3 = self.w5*self.h1 + self.w6*self.h2 + self.b3    <br/>    self.h3 = self.sigmoid(self.a3)    <br/>    return self.h3</span></pre><p id="f697" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在我们有了正向传递函数，它接受一个输入<strong class="jj hv"> x </strong>并计算输出。首先，我已经初始化了两个局部变量，并等同于输入<strong class="jj hv"> x </strong>，它有两个特性。</p><p id="be75" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">对于这三个神经元中的每一个，会发生两件事，</p><blockquote class="kf kg kh"><p id="627d" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">预激活用“a”表示:它是输入加上偏差的加权和。</p><p id="957a" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">激活用“h”表示:激活函数是Sigmoid函数。</p></blockquote><p id="4944" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">第一神经元的预激活由下式给出，</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="144a" class="nz kn hu nr b fv oa ob l oc od">a₁ = w₁ * x₁ + w₂ * x₂ + b₁</span></pre><p id="e29d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了获得第一个神经元的激活后值，我们简单地将逻辑函数应用于激活前a₁.的输出</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="7a16" class="nz kn hu nr b fv oa ob l oc od">h₁ = sigmoid(a₁)</span></pre><p id="1efb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">对第二个神经元重复同样的过程，得到a₂和h₂.</p><p id="083e" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">第一个隐藏层中存在的两个神经元的输出将作为第三个神经元的输入。第三神经元的预激活由下式给出，</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="1180" class="nz kn hu nr b fv oa ob l oc od">a₃ = w₅ * h₁ + w₆ * h₂ + b₃</span></pre><p id="b1b2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">并且在a₃上应用sigmoid将给出最终的预测输出。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="bd4b" class="nz kn hu nr b fv oa ob l oc od">def grad(self, x, y):    <br/>    #back propagation<br/>    ......</span></pre><p id="0eef" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们有一个<code class="eh no np nq nr b">grad</code>函数，它将输入<strong class="jj hv"> x </strong>和<strong class="jj hv"> y </strong>作为参数，并计算向前传递。基于向前传递，它计算这些权重相对于损失函数的偏导数，在这种情况下损失函数是均方误差损失。</p><blockquote class="kf kg kh"><p id="5483" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">注意:在这篇文章中，我不会解释我们如何得到这些参数的偏导数。现在就把这个函数当作一个黑盒，在我的下一篇文章中，我将解释我们如何在反向传播中计算这些偏导数。</p></blockquote><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="fc42" class="nz kn hu nr b fv oa ob l oc od">def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):<br/>    ......</span></pre><p id="fc13" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">然后，我们有类似于乙状结肠神经元的<code class="eh no np nq nr b">fit</code>功能。在这个函数中，我们遍历每个数据点，通过调用<code class="eh no np nq nr b">grad</code>函数计算偏导数，并将这些值存储在每个参数的新变量中(<em class="ki">第63–75行</em>)。然后，我们继续更新所有参数的值(<em class="ki">第77–87行</em>)。我们还有<code class="eh no np nq nr b">display_loss</code>条件，如果设置为<code class="eh no np nq nr b">True</code>，它将显示所有时期的网络损耗变化图。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="12b9" class="nz kn hu nr b fv oa ob l oc od">def predict(self, X):    <br/>#predicting the results on unseen data<br/>    .....</span></pre><p id="fbb4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">最后，我们有predict函数，它将一大组值作为输入，并通过对每个输入调用<code class="eh no np nq nr b">forward_pass</code>函数来计算每个输入的预测值。</p><h1 id="6328" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated"><strong class="ak">根据数据训练FF网络</strong></h1><p id="01f9" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们现在将在我们创建的前馈网络上训练我们的数据。首先，我们实例化<code class="eh no np nq nr b">FirstFFNetwork</code>类，然后在2000个历元和学习率设置为0.01的训练数据上调用<code class="eh no np nq nr b">fit</code>方法。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff ok"><img src="../Images/d2d2dcaa30517990e218f8058c92d0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*T1gbO0D8Xvlx8zlAb_1g_A.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">FF Network Loss</figcaption></figure><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="31cd" class="nz kn hu nr b fv oa ob l oc od">#visualize the predictions<br/>plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap=my_cmap, s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2))<br/>plt.show()</span></pre><p id="551c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了更好地了解神经网络的性能，我们将使用在sigmoid neuron中使用的相同4D可视化图，并将其与sigmoid neuron模型进行比较。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ol"><img src="../Images/029beaa56347b69471d71f8780c5a344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VrsiDaSoqPbByTYoWN15LA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Single Sigmoid Neuron (Left) &amp; Neural Network (Right)</figcaption></figure><p id="db17" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">正如你所看到的，大多数点都被神经网络正确分类了。关键是，仅仅通过组合三个sigmoid神经元，我们就能够解决非线性可分离数据的问题。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><h1 id="df01" class="km kn hu bd ko kp mt kr ks kt mu kv kw kx mv kz la lb mw ld le lf mx lh li lj dt translated">前馈神经网络的一般类</h1><p id="2737" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在本节中，我们将编写一个通用类，它可以通过将隐藏层的数量和每个隐藏层中的神经元数量作为输入参数来生成神经网络。泛型类也将输入的数量作为参数，之前我们只有两个输入，但现在我们也可以有n维输入。</p><blockquote class="kf kg kh"><p id="4acf" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">注意:在这种情况下，我只考虑二进制分类的网络。</p></blockquote><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff om"><img src="../Images/9f92d4e9d167751c5af7e84015c39d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hk9ZPlh34woAXmXYWma8rg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Generic Feedforward Network</figcaption></figure><p id="c987" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在我们开始为通用神经网络编写代码之前，让我们理解表示与特定神经元相关联的权重和偏差的指数的格式。</p><blockquote class="kf kg kh"><p id="f6fb" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">w(层数)(该层中的神经元数)(输入数)<br/> b(层数)(与该输入相关的偏置数)<br/> a(层数)(输入数)</p></blockquote><p id="f0c5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">W₁₁₁ —与连接到第一个输入的第一个隐藏层中存在的第一个神经元相关联的权重。</p><p id="1b91" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">W₁₁₂ —与连接到第二个输入的第一个隐藏层中的第一个神经元相关联的权重。</p><p id="a004" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">b₁₁ —与第一个隐藏层中的第一个神经元相关联的偏差。</p><p id="e952" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">b₁₂ —与第一个隐藏层中的第二个神经元相关联的偏差。</p><h1 id="a1b4" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated"><strong class="ak">代码</strong>:</h1><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="5ea1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">一个功能一个功能的解释，</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="f844" class="nz kn hu nr b fv oa ob l oc od">def __init__(self, n_inputs, hidden_sizes=[2]):    <br/>#intialize the inputs    <br/>    self.nx = n_inputs    <br/>    self.ny = 1 #one final neuron for binary classification.   <br/>    self.nh = len(hidden_sizes)    <br/>    self.sizes = [self.nx] + hidden_sizes + [self.ny]<br/>    .....</span></pre><p id="ba3d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">__init__</code>函数有几个参数，</p><p id="80ec" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">n_inputs</code> —进入网络的输入数量。</p><p id="6832" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">hidden_sizes</code> —需要一个整数列表，表示隐藏层中存在的神经元数量。</p><ul class=""><li id="8e69" class="mz na hu jj b jk jl jo jp js nb jw nc ka nd ke oh nf ng nh dt translated">[2] —一个具有2个神经元的隐藏层</li><li id="04fc" class="mz na hu jj b jk ni jo nj js nk jw nl ka nm ke oh nf ng nh dt translated">[2，3] —两个隐藏层，第一层有2个神经元，第二层有3个神经元。</li></ul><p id="7e5f" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在这个函数中，我们初始化两个字典<strong class="jj hv"> W </strong>和<strong class="jj hv"> B </strong>来存储网络中每个隐藏层的随机初始化的权重和偏差。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="dff9" class="nz kn hu nr b fv oa ob l oc od">def forward_pass(self, x):    <br/>    self.A = {}    <br/>    self.H = {}    <br/>    self.H[0] = x.reshape(1, -1)<br/>    ....</span></pre><p id="aa45" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在<code class="eh no np nq nr b">forward_pass</code>函数中，我们已经初始化了两个字典<strong class="jj hv"> A </strong>和<strong class="jj hv"> H </strong>，我没有将输入表示为<strong class="jj hv"> X </strong>而是将其表示为H₀，这样我们就可以将其保存在后激活字典<strong class="jj hv"> H </strong>中。然后，我们将遍历所有层，计算预激活&amp;后激活值，并将它们存储在各自的字典中。最终层的预激活输出与我们网络的预测值相同。该函数将在外部返回该值。这样我们就可以用这个值来计算神经元的损失。</p><p id="897c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">请记住，在之前的类<em class="ki"> FirstFFNetwork </em>中，我们已经分别为每个神经元的预激活和后激活计算进行了硬编码，但在我们的一般类中情况并非如此。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="8bc2" class="nz kn hu nr b fv oa ob l oc od">def grad_sigmoid(self, x):    <br/>    return x*(1-x)       </span><span id="1d37" class="nz kn hu nr b fv oe ob l oc od">def grad(self, x, y):    <br/>    self.forward_pass(x)<br/>    .....</span></pre><p id="ae6d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们定义两个函数来帮助计算参数对损失函数的偏导数。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="b15d" class="nz kn hu nr b fv oa ob l oc od">def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):        <br/>    # initialise w, b    <br/>    if initialise:      <br/>        for i in range(self.nh+1):        <br/>            self.W[i+1] = np.random.randn(self.sizes[i],    self.sizes[i+1])        <br/>            self.B[i+1] = np.zeros((1, self.sizes[i+1]))</span></pre><p id="59e0" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">然后，我们定义我们的<code class="eh no np nq nr b">fit</code>函数，它本质上是相同的，但是在这里我们循环通过每个输入，并以一般化的方式更新权重和偏差，而不是更新单个参数。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="6ef4" class="nz kn hu nr b fv oa ob l oc od">def predict(self, X):    <br/>#predicting the results on unseen data<br/>    .....</span></pre><p id="43fb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">最后，我们有一个predict函数，它将一大组值作为输入，并通过对每个输入调用<code class="eh no np nq nr b">forward_pass</code>函数来计算每个输入的预测值。</p><h1 id="e07b" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">前馈神经网络的训练类属</h1><p id="cdc7" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们现在将在我们创建的通用前馈网络上训练我们的数据。首先，我们实例化<code class="eh no np nq nr b">FFSNetwork</code>类，然后在2000个历元和学习率设置为0.01的训练数据上调用<code class="eh no np nq nr b">fit</code>方法。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="8216" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">下面给出了训练数据的神经网络的损失变化，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff on"><img src="../Images/4f324ab5d687063d2a28b0591195b83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*4rt29B-LHWKcDnQEqw_6tw.png"/></div></figure><p id="52d1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">从图中，我们看到损失函数比之前的网络下降得稍慢，因为在这种情况下，我们有两个隐藏层，分别有2个和3个神经元。因为它是一个具有更多参数的大型网络，所以学习算法需要更多的时间来学习所有参数并通过网络传播损耗。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="8b12" class="nz kn hu nr b fv oa ob l oc od">#visualize the predictions<br/>plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_binarised_train, cmap=my_cmap, s=15*(np.abs(Y_pred_binarised_train-Y_train)+.2))<br/>plt.show()</span></pre><p id="00f2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们将再次使用相同的4D图来可视化我们的一般网络的预测。请记住，小点表示这些观察结果被正确分类，大点表示这些观察结果被错误分类。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff oo"><img src="../Images/cda40f542f859d230aa541e3352c0081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*3Vm2LpekxApycgUtLOMmlQ.png"/></div></figure><p id="f8b7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="ki">您可以调整历元数和学习率，看看是否可以将误差降低到当前值以下。此外，您可以创建一个每层都有许多神经元的更深层次的网络，并观察该网络的表现。</em></p><h1 id="9723" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">用于多类分类的通用FF类</h1><p id="22ca" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在这一节中，我们将扩展上一节中编写的通用函数，以支持多类分类。在开始构建泛型类之前，我们需要做一些数据预处理。</p><p id="7cfb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">请记住，最初，我们用4个类生成数据，然后我们将多类数据转换为二进制类数据。在本节中，我们将使用原始数据来训练我们多类神经网络。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="f0fe" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这里我们有4个不同的类，所以我们对每个标签进行编码，以便机器能够理解并在其上进行计算。为了对标签进行编码，我们将在训练和验证标签上使用<code class="eh no np nq nr b">sklearn.OneHotEncoder</code>。</p><p id="80d0" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们将在一个名为<code class="eh no np nq nr b">FFSN_MultiClass</code>的类中编写用于多类分类的通用前馈网络。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="bde9" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我将解释在我们之前的<code class="eh no np nq nr b">FFSNetwork</code>类中所做的改变，以使其适用于多类分类。</p><p id="4937" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">首先，我们有我们的<code class="eh no np nq nr b">forward_pass</code>函数，</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="b60d" class="nz kn hu nr b fv oa ob l oc od">def forward_pass(self, x):<br/>    self.A = {}<br/>    self.H = {}<br/>    self.H[0] = x.reshape(1, -1)<br/>    for i in range(self.nh):<br/>      self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1]<br/>      self.H[i+1] = self.sigmoid(self.A[i+1])<br/>    self.A[self.nh+1] = np.matmul(self.H[self.nh], self.W[self.nh+1]) + self.B[self.nh+1]<br/>    self.H[self.nh+1] = self.softmax(self.A[self.nh+1])<br/>    return self.H[self.nh+1]</span></pre><p id="e971" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">因为我们有来自网络的多类输出，所以我们在输出层使用softmax激活而不是sigmoid激活。在<em class="ki">第29–30行</em>，我们使用softmax层来计算输出层的正向传递。</p><pre class="lq lr ls lt fq nv nr nw nx aw ny dt"><span id="41ef" class="nz kn hu nr b fv oa ob l oc od">def cross_entropy(self,label,pred):<br/>    yl=np.multiply(pred,label)<br/>    yl=yl[yl!=0]<br/>    yl=-np.log(yl)<br/>    yl=np.mean(yl)<br/>    return yl</span></pre><p id="d8e8" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们有损失函数。在这种情况下，我们使用的是交叉熵损失函数，而不是均方误差。通过使用交叉熵损失，我们可以找到预测概率分布和实际概率分布之间的差异，以计算网络的损失。</p><h1 id="bde7" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">为多类分类训练通用类</h1><p id="fa4d" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们现在将在我们创建的通用多类前馈网络上训练我们的数据。首先，我们实例化<code class="eh no np nq nr b">FFSN_MultiClass</code>类，然后在2000个历元和学习率设置为0.005的训练数据上调用<code class="eh no np nq nr b">fit</code>方法。请记住，我们的数据有两个输入和4个编码标签。</p><figure class="lq lr ls lt fq iv"><div class="bz el l di"><div class="ns nt l"/></div></figure><p id="8ede" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">下面给出了训练数据的神经网络的损失变化，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff op"><img src="../Images/fddf0ab63d2e1d851ab8b5480cdabbf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*FY7Qn1Bb2EVZ7J0yjeehHg.png"/></div></figure><p id="63ab" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们将再次使用相同的4D图来可视化我们的一般网络的预测。为了绘制图表，我们需要从网络中获得一个最终的预测标签，为了获得该预测值，我应用了<code class="eh no np nq nr b">argmax</code>函数来获得概率最高的标签。使用该标签，我们可以绘制4D图，并将其与实际输入数据散点图进行比较。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oq"><img src="../Images/38952c4920b95363e764cecbbc65da79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5kCpHsH_HIuJxW7etY1zQ.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Original Labels (Left) &amp; Predicted Labels(Right)</figcaption></figure><p id="2239" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在，我们已经成功地从零开始构建了用于多类分类的通用神经网络。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff or"><img src="../Images/8db217c33b17565df78e54d45d56d076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CMXtk0QRBTdUUa2S"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Photo by <a class="ae jg" href="https://unsplash.com/@napr0tiv?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Vasily Koloda</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="bf2f" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">下一步是什么？</h1><blockquote class="os"><p id="6caf" class="ot ou hu bd ov ow ox oy oz pa pb ke ek translated">通过编码学习</p></blockquote><p id="9f33" class="pw-post-body-paragraph jh ji hu jj b jk pc jm jn jo pd jq jr js pe ju jv jw pf jy jz ka pg kc kd ke hn dt translated">在本文中，我们使用了<code class="eh no np nq nr b">make_blobs</code>函数来生成玩具数据，我们已经看到<code class="eh no np nq nr b">make_blobs</code>生成线性可分数据。如果你想生成一些复杂的非线性可分数据来训练你的前馈神经网络，可以使用<code class="eh no np nq nr b">sklearn</code>包中的<code class="eh no np nq nr b">make_moons</code>函数。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff ph"><img src="../Images/75914ba824147331e3f5afc769232e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qwxyN05Bb0D2LC_WSoSKjg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Make Moons Function Data</figcaption></figure><p id="ca2d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><code class="eh no np nq nr b">make_moons</code>函数产生两个交错的半圆数据，本质上给你一个非线性可分的数据。此外，您可以在数据中添加一些高斯噪声，使神经网络更复杂地达到非线性可分离的决策边界。</p><p id="1ad8" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">使用我们的通用神经网络类，您可以创建一个更深的网络，每层中有更多数量的神经元(每层中也有不同数量的神经元),并调整学习速率&amp;多个时期，以检查在哪些参数下神经网络能够达到可能的最佳决策边界。</p><p id="56c7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">本文中讨论的全部代码都在这个GitHub存储库中。随意叉或者下载。</p><div class="lu lv fm fo lw lx"><a href="https://github.com/Niranjankumar-c/Feedforward_NeuralNetworrks" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab ej"><div class="lz ab ma cl cj mb"><h2 class="bd hv fv z el mc eo ep md er et ht dt translated">niranjankumar-c/前馈_神经网络</h2><div class="me l"><h3 class="bd b fv z el mc eo ep md er et ek translated">从头开始构建我们的神经网络。通过…为Niranjankumar-c/前馈神经网络的发展做出贡献</h3></div><div class="mf l"><p class="bd b gc z el mc eo ep md er et ek translated">github.com</p></div></div><div class="mg l"><div class="nn l mi mj mk mg ml ja lx"/></div></div></a></div><p id="3900" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="ki"> PS:如果你有兴趣把代码转换成</em> <strong class="jj hv"> <em class="ki"> R，</em> </strong> <em class="ki">一旦完成就给我发消息。我会在这里和GitHub页面上展示你的作品。</em></p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><p id="c2a2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你有兴趣了解更多关于人工神经网络的知识，请查看来自<a class="ae jg" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的Abhishek和Pukhraj的<a class="ae jg" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。还有，这门课会用最新版本的Tensorflow 2.0 (Keras后端)来教。他们还有一个非常好的关于机器学习(基础+高级)的包，用Python和R语言编写。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><h1 id="7bb9" class="km kn hu bd ko kp mt kr ks kt mu kv kw kx mv kz la lb mw ld le lf mx lh li lj dt translated"><strong class="ak">结论</strong></h1><p id="87f7" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在这篇文章中，我们从头构建了一个简单的神经元网络，并看到它表现良好，而我们的sigmoid神经元不能处理非线性可分数据。然后，我们已经看到了如何编写一个通用类，它可以采用'<strong class="jj hv"> n </strong>个输入和'<strong class="jj hv"> L </strong>个隐藏层(每层有许多神经元)来使用均方误差作为损失函数进行二进制分类。之后，我们使用softmax和交叉熵作为损失函数来扩展我们的泛型类，以处理多类分类，并看到它的性能相当好。</p><p id="c591" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="ki">推荐阅读</em></p><div class="lu lv fm fo lw lx"><a href="https://hackernoon.com/deep-learning-feedforward-neural-networks-explained-c34ae3f084f1" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab ej"><div class="lz ab ma cl cj mb"><h2 class="bd hv fv z el mc eo ep md er et ht dt translated">深度学习:解释前馈神经网络</h2><div class="me l"><h3 class="bd b fv z el mc eo ep md er et ek translated">你的第一个深度神经网络</h3></div><div class="mf l"><p class="bd b gc z el mc eo ep md er et ek translated">hackernoon.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ja lx"/></div></div></a></div><p id="11db" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在我的下一篇文章中，我会用一些数学知识详细解释反向传播。所以确保你在媒体上跟踪我，以便在它下降时得到通知。</p><p id="28e0" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">直到那时和平:)</p><p id="55cb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">NK。</p></div><div class="ab cl mm mn hc mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hn ho hp hq hr"><p id="cd55" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Niranjan Kumar 是汇丰银行分析部门的实习生。他对深度学习和人工智能充满热情。他是<a class="ae jg" rel="noopener" href="/tag/artificial-intelligence/top-writers">人工智能</a>中<a class="pi pj gr" href="https://medium.com/u/504c7870fdb6?source=post_page-----d3526457156b--------------------------------" rel="noopener" target="_blank">媒介</a>的顶尖作家之一。在<a class="ae jg" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，或者在<a class="ae jg" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我，了解关于深度学习和人工智能的最新文章。</p><p id="91d4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj hv">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>