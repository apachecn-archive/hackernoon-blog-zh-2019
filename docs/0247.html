<html>
<head>
<title>Implementing a Sequence-to-Sequence Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实现序列到序列模型</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/implementing-a-sequence-to-sequence-model-45a6133958ca?source=collection_archive---------6-----------------------#2019-01-10">https://medium.com/hackernoon/implementing-a-sequence-to-sequence-model-45a6133958ca?source=collection_archive---------6-----------------------#2019-01-10</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><p id="541d" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="jp">在这篇文章中，了解如何实现序列到序列模型，作者是Skejul的创始人兼首席执行官Matthew Lamons，这是一个帮助人们管理其活动的人工智能平台，作者是人工智能科学家、深度学习实践者和独立研究员Rahul Kumar。</em></p><p id="1c04" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">在本文中，您将为一个简单的序列间问答任务实现一个seq2seq模型(一个编码器-解码器RNN)。该模型可以被训练成将输入序列(问题)映射到输出序列(答案)，输出序列不一定彼此长度相同。</p><p id="ef09" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">这种类型的seq2seq模型在各种其他任务中表现出令人印象深刻的性能，例如语音识别、机器翻译、问题回答、<strong class="it hv">神经机器翻译</strong> ( <strong class="it hv"> NMT </strong>)和图像字幕生成。</p><p id="6161" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">下图有助于您可视化seq2seq模型:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff jq"><img src="../Images/c4f3ec166c86889df4a13186c89eef9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*ZpB4PxOvLGZ0uEEqMDg_5w.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">The illustration of the sequence to sequence (seq2seq) model</figcaption></figure><p id="e79e" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">每个矩形框是RNN单元，蓝色的是编码器，红色的是解码器。在编码器-解码器结构中，一个RNN(蓝色)<strong class="it hv">编码</strong>输入序列。编码器发出上下文<strong class="it hv"> C </strong>，通常作为其最终隐藏状态的简单函数，第二RNN(红色)<strong class="it hv">解码器</strong>计算目标值并生成输出序列。一个重要的步骤是让编码器和解码器进行通信。在最简单的方法中，使用编码器的最后一个隐藏状态来初始化解码器。其他方法让解码器在解码过程中的不同时间步骤关注编码输入的不同部分。</p><p id="7b34" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">因此，从数据准备、模型构建、训练、调优和评估seq2seq模型开始，看看它的表现如何。模型文件可以在<a class="ae kg" href="https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter05/3.%20rnn_lstm_seq2seq.py" rel="noopener ugc nofollow" target="_blank">https://github . com/packt publishing/Python-Deep-Learning-Projects/blob/master/chapter 05/3找到。%20rnn_lstm_seq2seq.py </a>。</p><h1 id="7090" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">数据准备</h1><p id="32f1" class="pw-post-body-paragraph ir is hu it b iu lf iw ix iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo hn dt translated">在这里，你建立你的问题回答系统。对于该项目，您需要一个包含问题和答案对的数据集，如下面的屏幕截图所示。这两列都包含单词序列，这是您需要输入seq2seq模型的内容。另外，请注意，您的句子可以是动态长度的:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lk"><img src="../Images/563da6237bfb957c6917e5d38451b73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uStk2D55UKMdv-yuC6WfLw.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">The dataset prepared with a set of questions and answers</figcaption></figure><p id="1dfe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在，加载它们并使用<code class="eh ll lm ln lo b">build_dataset()</code> <strong class="it hv"> <em class="jp">进行数据处理。</em> </strong>最后，你会有一个以单词为关键字的字典，其中关联的值是单词在各自语料库中的计数。此外，您还有四个额外值:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="7e52" class="lt ki hu lo b fv lu lv l lw lx">import numpy as np</span><span id="00cb" class="lt ki hu lo b fv ly lv l lw lx">import tensorflow as tf</span><span id="22c3" class="lt ki hu lo b fv ly lv l lw lx">import collections</span><span id="fc01" class="lt ki hu lo b fv ly lv l lw lx">from utils import *</span><span id="4392" class="lt ki hu lo b fv ly lv l lw lx">file_path = './conversation_data/'</span><span id="28c9" class="lt ki hu lo b fv ly lv l lw lx">with open(file_path+'from.txt', 'r') as fopen:</span><span id="632e" class="lt ki hu lo b fv ly lv l lw lx">text_from = fopen.read().lower().split('\n')</span><span id="fe86" class="lt ki hu lo b fv ly lv l lw lx">with open(file_path+'to.txt', 'r') as fopen:</span><span id="5be3" class="lt ki hu lo b fv ly lv l lw lx">text_to = fopen.read().lower().split('\n')</span><span id="11c9" class="lt ki hu lo b fv ly lv l lw lx">print('len from: %d, len to: %d'%(len(text_from), len(text_to)))</span><span id="76a2" class="lt ki hu lo b fv ly lv l lw lx">concat_from = ' '.join(text_from).split()</span><span id="1aeb" class="lt ki hu lo b fv ly lv l lw lx">vocabulary_size_from = len(list(set(concat_from)))</span><span id="ae13" class="lt ki hu lo b fv ly lv l lw lx">data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)</span><span id="7f39" class="lt ki hu lo b fv ly lv l lw lx">concat_to = ' '.join(text_to).split()</span><span id="8ce4" class="lt ki hu lo b fv ly lv l lw lx">vocabulary_size_to = len(list(set(concat_to)))</span><span id="af56" class="lt ki hu lo b fv ly lv l lw lx">data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)</span><span id="4bce" class="lt ki hu lo b fv ly lv l lw lx">GO = dictionary_from['GO']</span><span id="bf34" class="lt ki hu lo b fv ly lv l lw lx">PAD = dictionary_from['PAD']</span><span id="8864" class="lt ki hu lo b fv ly lv l lw lx">EOS = dictionary_from['EOS']</span><span id="2035" class="lt ki hu lo b fv ly lv l lw lx">UNK = dictionary_from['UNK']</span></pre><h1 id="dbf8" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">定义seq2seq模型</h1><p id="f7ee" class="pw-post-body-paragraph ir is hu it b iu lf iw ix iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo hn dt translated">在本节中，您将概述TensorFlow seq2seq模型定义。您将使用一个嵌入层将输入从整数表示转换为向量表示。这个seq2seq模型有四个主要组件:嵌入层、编码器、解码器和成本/优化器。</p><p id="cfbe" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">您可以在下图中以图形形式看到该模型:</p><figure class="jr js jt ju fq jv fe ff paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="fe ff lz"><img src="../Images/1f3c3fef196f644656fa13604d18cc3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utve3F_bH0J6qrVRWV8Q3A.png"/></div></div><figcaption class="kc kd fg fe ff ke kf bd b be z ek">The TensorBoard visualization of the seq2seq model</figcaption></figure><p id="d338" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">该图显示了编码器和解码器与优化器等其他相关组件之间的连接。以下是TensorFlow seq2seq模型定义的正式概述:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="c347" class="lt ki hu lo b fv lu lv l lw lx">class Chatbot:</span><span id="3ba1" class="lt ki hu lo b fv ly lv l lw lx">def __init__(self, size_layer, num_layers, embedded_size,</span><span id="a5f1" class="lt ki hu lo b fv ly lv l lw lx">from_dict_size, to_dict_size, learning_rate, batch_size):</span><span id="3490" class="lt ki hu lo b fv ly lv l lw lx">def cells(reuse=False):</span><span id="348b" class="lt ki hu lo b fv ly lv l lw lx">return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)</span><span id="ca8d" class="lt ki hu lo b fv ly lv l lw lx">self.X = tf.placeholder(tf.int32, [None, None])</span><span id="3b69" class="lt ki hu lo b fv ly lv l lw lx">self.Y = tf.placeholder(tf.int32, [None, None])</span><span id="2c2d" class="lt ki hu lo b fv ly lv l lw lx">self.X_seq_len = tf.placeholder(tf.int32, [None])</span><span id="bebd" class="lt ki hu lo b fv ly lv l lw lx">self.Y_seq_len = tf.placeholder(tf.int32, [None])</span><span id="82bd" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("encoder_embeddings"):</span><span id="bc2e" class="lt ki hu lo b fv ly lv l lw lx">encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))</span><span id="0a12" class="lt ki hu lo b fv ly lv l lw lx">encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)</span><span id="0c1d" class="lt ki hu lo b fv ly lv l lw lx">main = tf.strided_slice(self.X, [0, 0], [batch_size, -1], [1, 1])</span><span id="85ac" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("decoder_embeddings"):</span><span id="0e6b" class="lt ki hu lo b fv ly lv l lw lx">decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)</span><span id="5fea" class="lt ki hu lo b fv ly lv l lw lx">decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))</span><span id="aa18" class="lt ki hu lo b fv ly lv l lw lx">decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)</span><span id="d0c2" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("encoder"):</span><span id="25fb" class="lt ki hu lo b fv ly lv l lw lx">rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])</span><span id="ebd9" class="lt ki hu lo b fv ly lv l lw lx">_, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded,</span><span id="7721" class="lt ki hu lo b fv ly lv l lw lx">dtype = tf.float32)</span><span id="a07e" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("decoder"):</span><span id="51d2" class="lt ki hu lo b fv ly lv l lw lx">rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])</span><span id="a78c" class="lt ki hu lo b fv ly lv l lw lx">outputs, _ = tf.nn.dynamic_rnn(rnn_cells_dec, decoder_embedded,</span><span id="289e" class="lt ki hu lo b fv ly lv l lw lx">initial_state = last_state,</span><span id="c699" class="lt ki hu lo b fv ly lv l lw lx">dtype = tf.float32)</span><span id="5a15" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("logits"):</span><span id="7769" class="lt ki hu lo b fv ly lv l lw lx">self.logits = tf.layers.dense(outputs,to_dict_size)</span><span id="2299" class="lt ki hu lo b fv ly lv l lw lx">print(self.logits)</span><span id="8d37" class="lt ki hu lo b fv ly lv l lw lx">masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)</span><span id="34c3" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("cost"):</span><span id="5e78" class="lt ki hu lo b fv ly lv l lw lx">self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.logits,</span><span id="0ba7" class="lt ki hu lo b fv ly lv l lw lx">targets = self.Y,</span><span id="ea3d" class="lt ki hu lo b fv ly lv l lw lx">weights = masks)</span><span id="7217" class="lt ki hu lo b fv ly lv l lw lx">with tf.variable_scope("optimizer"):</span><span id="6d46" class="lt ki hu lo b fv ly lv l lw lx">self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)</span></pre><h1 id="18e7" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">超参数</h1><p id="0f67" class="pw-post-body-paragraph ir is hu it b iu lf iw ix iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo hn dt translated">既然您已经准备好了模型定义，那么就定义超参数。保持大部分配置与上一个配置相同:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="474c" class="lt ki hu lo b fv lu lv l lw lx">size_layer = 128</span><span id="ceda" class="lt ki hu lo b fv ly lv l lw lx">num_layers = 2</span><span id="9391" class="lt ki hu lo b fv ly lv l lw lx">embedded_size = 128</span><span id="f1e6" class="lt ki hu lo b fv ly lv l lw lx">learning_rate = 0.001</span><span id="ec80" class="lt ki hu lo b fv ly lv l lw lx">batch_size = 32</span><span id="0e0d" class="lt ki hu lo b fv ly lv l lw lx">epoch = 50</span></pre><h1 id="8f12" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">为seq2seq模型定型</h1><p id="f430" class="pw-post-body-paragraph ir is hu it b iu lf iw ix iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo hn dt translated">现在，训练模型。您将需要一些辅助函数来填充句子并计算模型的准确性:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="8d69" class="lt ki hu lo b fv lu lv l lw lx">def pad_sentence_batch(sentence_batch, pad_int):</span><span id="1652" class="lt ki hu lo b fv ly lv l lw lx">padded_seqs = []</span><span id="ebb3" class="lt ki hu lo b fv ly lv l lw lx">seq_lens = []</span><span id="efdd" class="lt ki hu lo b fv ly lv l lw lx">max_sentence_len = 50</span><span id="5f4e" class="lt ki hu lo b fv ly lv l lw lx">for sentence in sentence_batch:</span><span id="f1a5" class="lt ki hu lo b fv ly lv l lw lx">padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))</span><span id="1a20" class="lt ki hu lo b fv ly lv l lw lx">seq_lens.append(50)</span><span id="f44c" class="lt ki hu lo b fv ly lv l lw lx">return padded_seqs, seq_lens</span><span id="a6ad" class="lt ki hu lo b fv ly lv l lw lx">def check_accuracy(logits, Y):</span><span id="99c0" class="lt ki hu lo b fv ly lv l lw lx">acc = 0</span><span id="2ef0" class="lt ki hu lo b fv ly lv l lw lx">for i in range(logits.shape[0]):</span><span id="9268" class="lt ki hu lo b fv ly lv l lw lx">internal_acc = 0</span><span id="9185" class="lt ki hu lo b fv ly lv l lw lx">for k in range(len(Y[i])):</span><span id="bc49" class="lt ki hu lo b fv ly lv l lw lx">if Y[i][k] == logits[i][k]:</span><span id="340c" class="lt ki hu lo b fv ly lv l lw lx">internal_acc += 1</span><span id="9521" class="lt ki hu lo b fv ly lv l lw lx">acc += (internal_acc / len(Y[i]))</span><span id="de04" class="lt ki hu lo b fv ly lv l lw lx">return acc / logits.shape[0]</span></pre><p id="461a" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">现在，初始化您的模型，并按照定义的历元数迭代会话:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="4f52" class="lt ki hu lo b fv lu lv l lw lx">tf.reset_default_graph()</span><span id="cef0" class="lt ki hu lo b fv ly lv l lw lx">sess = tf.InteractiveSession()</span><span id="397e" class="lt ki hu lo b fv ly lv l lw lx">model = Chatbot(size_layer, num_layers, embedded_size, vocabulary_size_from + 4,</span><span id="fe84" class="lt ki hu lo b fv ly lv l lw lx">vocabulary_size_to + 4, learning_rate, batch_size)</span><span id="35ff" class="lt ki hu lo b fv ly lv l lw lx">sess.run(tf.global_variables_initializer())</span><span id="5bc4" class="lt ki hu lo b fv ly lv l lw lx">for i in range(epoch):</span><span id="f6c3" class="lt ki hu lo b fv ly lv l lw lx">total_loss, total_accuracy = 0, 0</span><span id="1295" class="lt ki hu lo b fv ly lv l lw lx">for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):</span><span id="7092" class="lt ki hu lo b fv ly lv l lw lx">batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)</span><span id="b896" class="lt ki hu lo b fv ly lv l lw lx">batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)</span><span id="9ac2" class="lt ki hu lo b fv ly lv l lw lx">predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer],</span><span id="46ff" class="lt ki hu lo b fv ly lv l lw lx">feed_dict={model.X:batch_x,</span><span id="a750" class="lt ki hu lo b fv ly lv l lw lx">model.Y:batch_y,</span><span id="a11d" class="lt ki hu lo b fv ly lv l lw lx">model.X_seq_len:seq_x,</span><span id="bceb" class="lt ki hu lo b fv ly lv l lw lx">model.Y_seq_len:seq_y})</span><span id="57af" class="lt ki hu lo b fv ly lv l lw lx">total_loss += loss</span><span id="88a9" class="lt ki hu lo b fv ly lv l lw lx">total_accuracy += check_accuracy(predicted,batch_y)</span><span id="67e3" class="lt ki hu lo b fv ly lv l lw lx">total_loss /= (len(text_from) // batch_size)</span><span id="a592" class="lt ki hu lo b fv ly lv l lw lx">total_accuracy /= (len(text_from) // batch_size)</span><span id="1a0b" class="lt ki hu lo b fv ly lv l lw lx">print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))</span><span id="99b8" class="lt ki hu lo b fv ly lv l lw lx"><strong class="lo hv">OUTPUT:</strong></span><span id="9e8d" class="lt ki hu lo b fv ly lv l lw lx">epoch: 47, avg loss: 0.682934, avg accuracy: 0.000000</span><span id="6e0a" class="lt ki hu lo b fv ly lv l lw lx">epoch: 48, avg loss: 0.680367, avg accuracy: 0.000000</span><span id="1b3b" class="lt ki hu lo b fv ly lv l lw lx">epoch: 49, avg loss: 0.677882, avg accuracy: 0.000000</span><span id="ed00" class="lt ki hu lo b fv ly lv l lw lx">epoch: 50, avg loss: 0.678484, avg accuracy: 0.000000</span><span id="44ff" class="lt ki hu lo b fv ly lv l lw lx">.</span><span id="b829" class="lt ki hu lo b fv ly lv l lw lx">.</span><span id="6398" class="lt ki hu lo b fv ly lv l lw lx">.</span><span id="723b" class="lt ki hu lo b fv ly lv l lw lx">epoch: 1133, avg loss: 0.000464, avg accuracy: 1.000000</span><span id="abb9" class="lt ki hu lo b fv ly lv l lw lx">epoch: 1134, avg loss: 0.000462, avg accuracy: 1.000000</span><span id="3d0f" class="lt ki hu lo b fv ly lv l lw lx">epoch: 1135, avg loss: 0.000460, avg accuracy: 1.000000</span><span id="63bd" class="lt ki hu lo b fv ly lv l lw lx">epoch: 1136, avg loss: 0.000457, avg accuracy: 1.000000</span></pre><h1 id="b1ad" class="kh ki hu bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le dt translated">seq2seq模型的评估</h1><p id="0dad" class="pw-post-body-paragraph ir is hu it b iu lf iw ix iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo hn dt translated">因此，在GPU上运行几个小时的训练过程后，您可以看到精度已经达到了值<code class="eh ll lm ln lo b">1.0</code>，并且损耗已经显著降低到<code class="eh ll lm ln lo b">0.00045</code>。现在，看看当你问一些一般性的问题时，模型的表现如何。</p><p id="6822" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">要进行预测，创建一个<code class="eh ll lm ln lo b">predict()</code>函数，该函数将任意大小的原始文本作为输入，并返回您所提问题的答案。你做了一个快速修复来处理词汇(<strong class="it hv"/>)中的<strong class="it hv">，把它们替换成<code class="eh ll lm ln lo b">PAD</code>:</strong></p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="2859" class="lt ki hu lo b fv lu lv l lw lx">def predict(sentence):</span><span id="a0cb" class="lt ki hu lo b fv ly lv l lw lx">X_in = []</span><span id="2033" class="lt ki hu lo b fv ly lv l lw lx">for word in sentence.split():</span><span id="a6d1" class="lt ki hu lo b fv ly lv l lw lx">try:</span><span id="0f28" class="lt ki hu lo b fv ly lv l lw lx">X_in.append(dictionary_from[word])</span><span id="e735" class="lt ki hu lo b fv ly lv l lw lx">except:</span><span id="a9fd" class="lt ki hu lo b fv ly lv l lw lx">X_in.append(PAD)</span><span id="06a3" class="lt ki hu lo b fv ly lv l lw lx">pass</span><span id="3914" class="lt ki hu lo b fv ly lv l lw lx">test, seq_x = pad_sentence_batch([X_in], PAD)</span><span id="571d" class="lt ki hu lo b fv ly lv l lw lx">input_batch = np.zeros([batch_size,seq_x[0]])</span><span id="a86b" class="lt ki hu lo b fv ly lv l lw lx">input_batch[0] =test[0]</span><span id="a640" class="lt ki hu lo b fv ly lv l lw lx">log = sess.run(tf.argmax(model.logits,2),</span><span id="cd99" class="lt ki hu lo b fv ly lv l lw lx">feed_dict={</span><span id="f488" class="lt ki hu lo b fv ly lv l lw lx">model.X:input_batch,</span><span id="a8f1" class="lt ki hu lo b fv ly lv l lw lx">model.X_seq_len:seq_x,</span><span id="30cb" class="lt ki hu lo b fv ly lv l lw lx">model.Y_seq_len:seq_x</span><span id="12e9" class="lt ki hu lo b fv ly lv l lw lx">}</span><span id="d3be" class="lt ki hu lo b fv ly lv l lw lx">)</span><span id="5f78" class="lt ki hu lo b fv ly lv l lw lx">result=' '.join(rev_dictionary_to[i] for i in log[0])</span><span id="36e1" class="lt ki hu lo b fv ly lv l lw lx">return result</span></pre><p id="b5a7" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当为前50个时期训练模型时，您将得到以下结果:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="cc07" class="lt ki hu lo b fv lu lv l lw lx">&gt;&gt; predict('where do you live')</span><span id="2eae" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; i PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</span><span id="94b3" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; print predict('how are you ?')</span><span id="54ac" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; i am PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</span></pre><p id="fc30" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">当模型被训练1136个时期时:</p><pre class="jr js jt ju fq lp lo lq lr aw ls dt"><span id="a252" class="lt ki hu lo b fv lu lv l lw lx">&gt;&gt; predict('where do you live')</span><span id="1db0" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; miami florida PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</span><span id="2572" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; print predict('how are you ?')</span><span id="1599" class="lt ki hu lo b fv ly lv l lw lx">&gt;&gt; i am fine thank you PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</span></pre><p id="3b79" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated">嗯！令人印象深刻，对吧？现在你的模型不仅能够理解上下文，还能逐字生成答案。</p><p id="880b" class="pw-post-body-paragraph ir is hu it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hn dt translated"><em class="jp">如果你觉得这篇文章有趣，可以探索</em> <a class="ae kg" href="https://www.amazon.com/Python-Deep-Learning-Projects-demystifying/dp/1788997093" rel="noopener ugc nofollow" target="_blank"> <em class="jp"> Python深度学习项目</em> </a> <em class="jp">掌握使用Python和Keras的深度学习和神经网络架构。</em> <a class="ae kg" href="https://www.packtpub.com/big-data-and-business-intelligence/python-deep-learning-projects" rel="noopener ugc nofollow" target="_blank"> <em class="jp"> Python深度学习项目</em> </a> <em class="jp">传授在计算语言学和计算机视觉领域实现复杂深度学习项目所需的所有知识。</em></p></div></div>    
</body>
</html>