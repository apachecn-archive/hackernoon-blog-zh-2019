<html>
<head>
<title>Under the Hood of AdaBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 AdaBoost 的引擎盖下</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/under-the-hood-of-adaboost-8eb499d78eab#2019-01-07">https://medium.com/hackernoon/under-the-hood-of-adaboost-8eb499d78eab#2019-01-07</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="a5b1" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated"><em class="jj">AdaBoost 算法简介</em></h2></div><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="fe ff jk"><img src="../Images/ddc60c71b9f10d824b546f78fb07b313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CwehBVhrSBY12K9t"/></div></div><figcaption class="jw jx fg fe ff jy jz bd b be z ek"><em class="jj">An Ensemble Orchestra: </em>Photo by <a class="ae ka" href="https://unsplash.com/@kaelbloom?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kael Bloom</a> on <a class="ae ka" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dd1a" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">在这篇文章中，我们将介绍<em class="kx">非常简短的</em>boosting 算法，以及深入研究流行的 boosting 算法 AdaBoost。这篇文章的目的是对 boosting 和 AdaBoost 的一些关键概念做一个简单的介绍。这不是 AdaBoost 与梯度增强等的明确优缺点，而是对理解该算法的关键理论点的总结。</p><h2 id="fc9a" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated">【AdaBoost 的真实应用</h2><p id="23f8" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">AdaBoost 可用于解决各种现实世界的问题，例如预测<a class="ae ka" href="https://www.cs.rit.edu/~rlaz/PatternRecognition/slides/churn_adaboost.pdf" rel="noopener ugc nofollow" target="_blank">客户流失</a>以及对客户正在<a class="ae ka" href="http://www.cs.princeton.edu/~schapire/talks/nips-tutorial.pdf" rel="noopener ugc nofollow" target="_blank">谈论/致电</a>的主题类型进行分类。该算法主要用于解决分类问题，因为它在 R 和 Python 等语言中相对容易实现。</p><h2 id="d0bc" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated"><strong class="ak">什么是 Boosting 算法？</strong></h2><p id="5ea3" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">推进算法属于更广泛的集成建模家族。概括地说，在数据科学中有两种关键的建模方法；构建单一模型和构建模型集合。Boosting 属于后一种方法，并且参考 AdaBoost，模型构造如下:对于每次迭代，顺序引入新的弱学习器，并且旨在补偿先前模型的“缺点”以创建强分类器。这项工作的总体目标是连续拟合新的模型，以提供我们的反应变量更准确的估计。</p><p id="f0e6" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">Boosting 的工作原理是假设每个弱假设或模型都比随机猜测具有更高的准确性:这种假设被称为“弱学习条件”。</p><h2 id="2fdc" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated"><strong class="ak">什么是 AdaBoost？</strong></h2><p id="babc" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">AdaBoost 算法是由 Freund 和 Schapire 于 1996 年开发的，目前仍在各种行业中大量使用。AdaBoost 通过连续引入新模型来补偿先前模型的“缺点”,从而达到分类器的最终目标。Scikit Learn 将 AdaBoost 的核心原则总结为，它“在反复修改的数据版本上适合一系列弱学习者。”这个定义将允许我们理解和扩展 AdaBoost 的过程。</p><h2 id="18d6" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated">入门指南</h2><p id="1b51" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">首先，训练一个弱分类器，并且所有的示例数据样本被赋予相等的权重。一旦训练了初始分类器，就会发生两件事。为分类器计算权重，更准确的分类器被给予更高的权重，较不准确的分类器被给予较低的权重。权重的计算基于分类器的错误率，即训练集中的错误分类数除以总训练集大小。每个模型的输出重量被称为“alpha”。</p><h2 id="9e32" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated"><strong class="ak">计算阿尔法值</strong></h2><p id="87d3" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">每个分类器都有一个基于分类器错误率计算的权重。</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="fe ff ly"><img src="../Images/70154efdc180f0f81cdd3f94b0f6ef05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*1mTYlAA6_GH1GPx51mOwFA.png"/></div></div></figure><p id="6bde" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">对于每次迭代，计算分类器的 alpha，错误率越低= alpha 越高。这可以想象如下:</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="fe ff lz"><img src="../Images/6cd6a7846b188081924fdcb6b5baa6a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BnnkVWLw0Q6tOXLhlXXMWQ.png"/></div></div><figcaption class="jw jx fg fe ff jy jz bd b be z ek"><em class="jj">Image from Chris McCormick’s excellent AdaBoost tutorial</em></figcaption></figure><p id="1a39" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">直观上，训练样本的权重和 alpha 之间也有关系。如果我们有一个具有高 alpha 的分类器，它错误地分类了一个训练样本，那么这个样本将比一个同样错误地分类了一个训练样本的较弱的分类器被给予更多的权重。这被称为“直觉”，因为我们可以认为具有较高 alpha 的分类器是更可靠的见证；当它错误分类时，我们希望进一步调查。</p><h1 id="7b46" class="ma kz hu bd la mb mc md le me mf mg li ja mh jb ll jd mi je lo jg mj jh lr mk dt translated"><strong class="ak">了解训练样本的权重:</strong></h1><p id="94aa" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">其次，AdaBoost 算法通过为每个数据样本分配权重，将其注意力引向来自我们的第一个弱分类器的错误分类的数据样本，其值由分类器对样本的正确分类还是错误分类来定义。</p><p id="517d" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">我们可以将每个示例的权重分解如下:</p><p id="e2f4" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated"><strong class="kd hv">第一步:我们的第一个模型，其中</strong> wi=1/N</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/bacc54469ea31192b7e25b399ebe0bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*WmbnNplIUohLhnLFig7r2g.png"/></div></figure><p id="2c91" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">在这种情况下，我们可以看到每个训练示例都具有相同的权重，并且模型对某些示例进行了正确和错误的分类。在每次迭代之后，样本权重被修改，并且那些具有较高权重的样本(被错误分类的样本)更有可能被包括在训练集中。当一个样本被正确分类时，在下一步的模型构建中，它被赋予较小的权重。</p><p id="b275" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated"><em class="kx">新款车型的一个例子，重量已经改变</em>:</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/79a5b820dd9d6f942527c266a7eca6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*XxRQo0ITJq987I61_oxNbA.png"/></div></figure><p id="bdb4" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">此权重更新的公式如下所示:</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="fe ff mn"><img src="../Images/7a9f40fb5a0389b46ed67646c25ff9dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5B9Xv77S4KVPgXB9BimsVQ.png"/></div></div></figure><h2 id="4bbd" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated">构建最终分类器</h2><p id="e0ba" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">一旦所有的迭代完成，所有的弱学习器与它们的权重结合以形成强分类器，如下面的等式所示:</p><figure class="jl jm jn jo fq jp fe ff paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="fe ff mo"><img src="../Images/de8d52d4ee29de62cda1ff8bf6141ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fm8dugL5KDGJS6ZJkaMbBQ.png"/></div></div></figure><p id="6f00" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">因此，最终分类器由“<em class="kx"> T </em>”个弱分类器组成，<em class="kx"> ht(x) </em>是弱分类器的输出，其中<em class="kx">在</em>处是应用于分类器的权重。因此，最终输出是所有分类器的组合。</p><p id="0215" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">这是对 AdaBoost 理论的一个简单介绍，应该被看作是对 boosting 算法的介绍性探索。对于进一步的阅读，我推荐以下资源:</p><h2 id="20da" class="ky kz hu bd la lb lc ld le lf lg lh li kk lj lk ll ko lm ln lo ks lp lq lr ls dt translated">推荐进一步阅读</h2><p id="0d36" class="pw-post-body-paragraph kb kc hu kd b ke lt iv kg kh lu iy kj kk lv km kn ko lw kq kr ks lx ku kv kw hn dt translated">纳特金，a .诺尔，A. (2018)。梯度推进机，教程。神经生物学前沿。[在线]可在:<a class="ae ka" href="https://core.ac.uk/download/pdf/82873637.pdf" rel="noopener ugc nofollow" target="_blank">https://core.ac.uk/download/pdf/82873637.pdf</a>找到</p><p id="19f6" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">李(2018)。渐变增强的温和介绍。【在线】可在:<a class="ae ka" href="http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf" rel="noopener ugc nofollow" target="_blank">http://www . CCS . neu . edu/home/VIP/teach/ml course/4 _ boosting/slides/gradient _ boosting . pdf</a></p><p id="a45d" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">沙皮雷河(2018 年)。解释 AdaBoost。【在线】可在:<a class="ae ka" href="https://math.arizona.edu/~hzhang/math574m/Read/explaining-adaboost.pdf" rel="noopener ugc nofollow" target="_blank">https://math . Arizona . edu/~ hzhang/math 574m/Read/explaining-AdaBoost . pdf</a></p><p id="9545" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">YouTube。(2018).扩展机器学习算法 AdaBoost 分类器| packtpub.com。[在线]可在:<a class="ae ka" href="https://www.youtube.com/watch?v=BoGNyWW9-mE" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=BoGNyWW9-mE</a>找到</p><p id="170c" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">Scikit-learn.org(2018)。1.11.整体方法-sci kit-学习 0.20.2 文件。[在线]可从以下网址获得:<a class="ae ka" href="https://scikit-learn.org/stable/modules/ensemble.html#AdaBoost" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/ensemble . html # AdaBoost</a></p><p id="fc65" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">McCormick，C. (2013 年)。AdaBoost 教程克里斯麦考密克。[在线]可从<a class="ae ka" href="http://mccormickml.com/2013/12/13/adaboost-tutorial/" rel="noopener ugc nofollow" target="_blank">http://mccormickml.com/2013/12/13/adaboost-tutorial/</a>获得</p><p id="e288" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">沙皮雷河(2018 年)。解释 AdaBoost。【在线】可在:<a class="ae ka" href="https://math.arizona.edu/~hzhang/math574m/Read/explaining-adaboost.pdf" rel="noopener ugc nofollow" target="_blank">https://math . Arizona . edu/~ hzhang/math 574m/Read/explaining-AdaBoost . pdf</a></p><p id="ae0d" class="pw-post-body-paragraph kb kc hu kd b ke kf iv kg kh ki iy kj kk kl km kn ko kp kq kr ks kt ku kv kw hn dt translated">莫赫利，M. (2018)。机器学习基础[在线]可在:<a class="ae ka" href="https://cs.nyu.edu/~mohri/mls/ml_boosting.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.nyu.edu/~mohri/mls/ml_boosting.pdf</a>获得</p></div></div>    
</body>
</html>