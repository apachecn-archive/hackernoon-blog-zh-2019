<html>
<head>
<title>Preventing AI Systems from Amplifying Bias with Adversarial Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">防止人工智能系统通过对抗性学习放大偏见</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/preventing-ai-system-from-amplifying-bias-with-adversarial-learning-bd5e224f5a31#2019-05-23">https://medium.com/hackernoon/preventing-ai-system-from-amplifying-bias-with-adversarial-learning-bd5e224f5a31#2019-05-23</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="4ed4" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">对抗学习中偏见的介绍、原因及解决方法</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/0771f6192054ef5a05f1832be09ca88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PetwS1-bqFAvlhOG.png"/></div></div></figure><p id="0d9c" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">说到放大偏差，我们首先想到的问题是，AI 系统是如何放大偏差的？</strong></p><p id="c7da" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">判别或生成模型是偏差放大的原因。因为判别模型更像是“黑箱”,只学习回答特定的训练数据集。</p><p id="3f36" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">下一个问题是，什么是判别模型，它是如何导致偏见的？</strong></p><p id="2bab" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">判别模型也称为条件模型，它试图仅根据观察到的数据来训练模型，同时学习如何从给定的统计数据中进行分类。判别模型，如神经网络、逻辑回归、SVM、条件随机场。</p><p id="2ede" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">那么，人工智能算法是如何放大偏差的？</strong></p><p id="54d7" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">现在，让我们以单词嵌入偏差为例，它是放大偏差的来源。</strong></p><p id="ca61" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">机器学习系统的力量不仅预示着巨大的技术进步，也带来了社会危害的风险。流行的单词嵌入算法表现出刻板偏见，例如性别偏见。这些算法在机器学习系统中的广泛使用，从自动翻译服务到简历扫描仪，可以在重要的背景下放大刻板印象。尽管已经开发了测量这些偏差和改变单词嵌入以减轻它们的有偏差表示的方法，但是缺乏对单词嵌入偏差如何依赖于训练数据的理解。给定在语料库上训练的单词嵌入，特定偏差度量方法识别对语料库的扰动将如何影响所得嵌入的偏差。这可以用来追溯单词嵌入偏差的来源，追溯到原始训练文档。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff kr"><img src="../Images/6b7d9065f0436da4f214de619a8997eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*NxfVoyaCDC0dlLHK"/></div><figcaption class="ks kt fg fe ff ku kv bd b be z ek">Fig shows how certain cultural context of information learned by Athe I system leads to bias</figcaption></figure><p id="5524" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">它们如何影响？</strong></p><p id="6fa2" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如果执行分类任务，他们可能会接受最大化分类准确性的培训。这意味着模型将利用任何有助于提高数据集准确性的信息，尤其是数据中存在的任何偏差。</p><p id="a653" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">例如，让我们看看一个实时自动化招聘平台的案例研究，它是如何影响候选人而不进入工作的。</p><p id="a86b" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">“亚马逊抛弃了人工智能招聘工具，该工具偏爱从事技术工作的男性。”</strong></p><p id="c59d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">怎么发生的？</strong></p><p id="806f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">该公司的实验性招聘工具使用人工智能给求职者打分，从一颗星到五颗星不等，就像购物者在亚马逊上给产品打分一样。“他们真的希望它是一个引擎，我会给你 100 份简历，它会分出前五名，我们会雇用他们。”</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff kw"><img src="../Images/bb0588cac6d4b97fa18fa971fd7d3940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trNOqGXv9OAvEk1I84ozxQ.png"/></div></div><figcaption class="ks kt fg fe ff ku kv bd b be z ek">Amazon recruitment System bias against women</figcaption></figure><p id="c9fe" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">但到 2015 年，该公司意识到其新系统并没有以性别中立的方式对软件开发工作和其他技术职位的候选人进行评级。这是因为亚马逊招聘平台中的“<strong class="jx hv">人工智能模型”是通过观察 10 年间提交给该公司的简历模式来训练申请人的。大多数来自男性，这导致了男性在整个招聘平台上的主导地位。</strong></p><p id="f4cd" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">如何处理人工智能(NLP)系统中性别中立方式的偏见？</strong></p><p id="1d09" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">单词嵌入模型已经成为广泛的自然语言处理(NLP)应用中的基本组件。然而，在人类生成的语料库上训练的嵌入已经被证明继承了反映社会结构的强烈的性别刻板印象。</p><p id="91ff" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">嵌入是将离散变量(例如单词、地区、URL)投影到多维实值空间的强大机制。已经开发了几种强有力的方法来学习嵌入。一个例子是跳格算法。在该算法中，周围的上下文用于预测单词的存在。不幸的是，许多真实世界的文本数据都有一个微妙的偏差，机器学习算法会隐式地将它包含在从该数据创建的嵌入中。这种偏差可以通过使用学习到的嵌入来执行单词类比任务来说明</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff kx"><img src="../Images/1867a42ee6876ce59b742494526a76ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zo1-EuxzdHUABYup.png"/></div></div></figure><p id="01a3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在，让我们讨论一下减轻偏见的最有效的对抗性学习。</p><h1 id="da57" class="ky kz hu bd la lb lc ld le lf lg lh li ja lj jb lk jd ll je lm jg ln jh lo lp dt translated">对抗性学习如何有助于减轻偏见？</h1><p id="013c" class="pw-post-body-paragraph jv jw hu jx b jy lq iv ka kb lr iy kd ke ls kg kh ki lt kk kl km lu ko kp kq hn dt translated">对抗法消除了嵌入中的一些偏见，它基于这样一种思想，即那些嵌入旨在用于预测基于输入𝑋的一些结果𝑌，但是在公平的世界中，该结果应该与一些受保护的变量𝑍.完全无关如果是这样的话，那么了解𝑌并不会帮助你更好地预测𝑍。这个原理可以直接转化为如下图所示的两个串联网络。第一次尝试使用𝑋作为输入来预测𝑌。第二次尝试使用𝑌的预测值来预测𝑍.如下图所示，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lv"><img src="../Images/8c3371846b4120e84dbad6e99f29deb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpohoM7Q-m6uNYvMru044g.png"/></div></div><figcaption class="ks kt fg fe ff ku kv bd b be z ek">The architecture of Adversarial network</figcaption></figure><p id="4959" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">然而，简单地训练基于∇𝑊𝐿1 的 w 中的权重和基于∇𝑈𝐿2 的𝑈中的权重实际上不会实现无偏的模型。为了做到这一点，你需要在𝑊's 更新函数中加入这样一个概念，即𝑈在预测𝑍.时应该不会比机会更好你可以实现这一点的方式类似于生成敌对网络(GANs) ( <a class="ae lw" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="noopener ugc nofollow" target="_blank"> Goodfellow 等人 2014 </a>)训练其生成器的方式。</p><p id="7d8c" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">除了∇𝑊𝐿1，你还把对∇𝑊𝐿2 的否定纳入了𝑊's 的更新函数。然而，有可能∇𝑊𝐿1 正在改变𝑊，通过使用你试图保护的有偏见的信息来提高准确性。为了避免这一点，你还加入了一个术语，通过将∇𝑊𝐿1 的成分投射到∇𝑊𝐿2.，来移除它一旦将这两个术语合并，𝑊的更新函数就变成了:</p><p id="ea86" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt">∇𝑊𝐿1−𝑝𝑟𝑜𝑗(∇𝑊𝐿2)∇𝑊𝐿1−∇𝑊𝐿2</p><p id="667d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">对如何将敌对网络合并到机器学习模型中的描述是非常通用的，因为该技术通常适用于任何类型的系统，这些系统可以根据预测𝑌的输入𝑋来描述，但是可能包含关于受保护变量𝑍.的信息只要你能构造相关的更新函数，你就能应用这种技术。然而，这并不能告诉你太多关于𝑋、𝑌和𝑍.的本质在单词类比任务中，𝑋 =𝐵+𝐶−𝐴和𝑌=𝐷.不过，要弄清楚𝑍应该是什么样子，要稍微复杂一点。为此，请参考 Bulokbasi 等人的一篇论文。艾尔。他们开发了一种无监督的方法来从单词嵌入中去除性别语义。</p><p id="96a3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">现在，我们将深入探讨使用对抗性学习来减轻偏见的实现部分</strong></p><p id="004d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">第一步是选择与试图消除的偏见类型相关的词对。就性别而言，选择像“男人”、“女人”、“男孩”、“女孩”这样的词对，它们在语义上的唯一区别就是性别。这些词对可以计算它们的嵌入之间的差异，以在嵌入的语义空间中产生大致平行于性别语义的向量。对这些向量执行主成分分析(PCA ),然后给出由所提供的性别化单词对定义的性别语义的主要成分。因此，让我们定义用于执行嵌入的主要组件的函数，</p><pre class="jk jl jm jn fq lx ly lz ma aw mb dt"><span id="c16c" class="mc kz hu ly b fv md me l mf mg">def find_gender_direction(embed,<br/>                          indices):<br/>  """Finds and returns a 'gender direction'."""<br/>  pairs = [<br/>      ("woman", "man"),<br/>      ("her", "his"),<br/>      ("she", "he"),<br/>      ("aunt", "uncle"),<br/>      ("niece", "nephew"),<br/>      ("daughters", "sons"),<br/>      ("mother", "father"),<br/>      ("daughter", "son"),<br/>      ("granddaughter", "grandson"),<br/>      ("girl", "boy"),<br/>      ("stepdaughter", "stepson"),<br/>      ("mom", "dad"),<br/>  ]<br/>  m = []<br/>  for wf, wm in pairs:<br/>    m.append(embed[indices[wf]] - embed[indices[wm]])<br/>  m = np.array(m)</span><span id="8976" class="mc kz hu ly b fv mh me l mf mg"># the next three lines are just a PCA.<br/>  m = np.cov(np.array(m).T)<br/>  evals, evecs = np.linalg.eig(m)<br/>  return _np_normalize(np.real(evecs[:, np.argmax(evals)]))</span><span id="6048" class="mc kz hu ly b fv mh me l mf mg"># Using the embeddings, find the gender vector.<br/>gender_direction = find_gender_direction(embed, indices)<br/>print "gender direction: %s" % str(gender_direction.flatten())</span></pre><p id="2102" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">一旦完成了嵌入差异的第一个主成分，开始把单词的嵌入投射到它上面。这个预测可以作为对手试图根据𝑌.的预测值预测的受保护变量𝑍现在让我们来看看对性别维度有最大负面影响的单词。</p><pre class="jk jl jm jn fq lx ly lz ma aw mb dt"><span id="866f" class="mc kz hu ly b fv md me l mf mg">words = set()<br/>for a in analogies:<br/>  words.update(a)</span><span id="4666" class="mc kz hu ly b fv mh me l mf mg">df = pd.DataFrame(data={"word": list(words)})<br/>df["gender_score"] = df["word"].map(<br/>    lambda w: client.word_vec(w).dot(gender_direction))<br/>df.sort_values(by="gender_score", inplace=True)<br/>print df.head(10)</span></pre><p id="28e4" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在让我们来看看那些在性别维度上有最大<em class="mi">正面</em>投射的单词。</p><pre class="jk jl jm jn fq lx ly lz ma aw mb dt"><span id="2625" class="mc kz hu ly b fv md me l mf mg">df.sort_values(by="gender_score", inplace=True, ascending=False)<br/>print df.head(10)</span></pre><h1 id="cb48" class="ky kz hu bd la lb lc ld le lf lg lh li ja lj jb lk jd ll je lm jg ln jh lo lp dt translated">训练模型</h1><p id="2cfb" class="pw-post-body-paragraph jv jw hu jx b jy lq iv ka kb lr iy kd ke ls kg kh ki lt kk kl km lu ko kp kq hn dt translated">训练敌对网络是困难的。他们很敏感，如果接触的方式不对，他们很快就会爆发。必须非常小心地以足够慢的速度训练两个模型，以便模型中的参数不会发散。实际上，这通常需要显著降低分类器和对手的步长。将对手的参数初始化为极小也可能是有益的，以确保分类器不会过度适应特定的(次优)对手(这种过度适应会很快导致发散！).也有可能的是，如果分类器太擅长于对对手隐藏受保护的变量，那么对手将强加发散的更新以努力提高其性能。解决方法有时是实际上增加对手的学习速率以防止发散(这在大多数学习系统中几乎是闻所未闻的)。在我的<a class="ae lw" href="https://github.com/rashmimarganiatgithub/preventing_bias_adversarial" rel="noopener ugc nofollow" target="_blank"> <strong class="jx hv"> GitHub </strong> </a> <strong class="jx hv">，</strong>中可以找到相同的单词嵌入的去偏置模型，请查看它以重现实验。下面是训练模型的代码。</p><pre class="jk jl jm jn fq lx ly lz ma aw mb dt"><span id="ffe8" class="mc kz hu ly b fv md me l mf mg">class AdversarialEmbeddingModel(object):<br/>  """A model for doing adversarial training of embedding models."""</span><span id="f37b" class="mc kz hu ly b fv mh me l mf mg">def __init__(self, client,<br/>               data_p, embed_dim, projection,<br/>               projection_dims, pred):<br/>    """Creates a new AdversarialEmbeddingModel.</span><span id="98b9" class="mc kz hu ly b fv mh me l mf mg">Args:<br/>      client: The (possibly biased) embeddings.<br/>      data_p: Placeholder for the data.<br/>      embed_dim: Number of dimensions used in the embeddings.<br/>      projection: The space onto which we are "projecting".<br/>      projection_dims: Number of dimensions of the projection.<br/>      pred: Prediction layer.<br/>    """<br/>    # load the analogy vectors as well as the embeddings<br/>    self.client = client<br/>    self.data_p = data_p<br/>    self.embed_dim = embed_dim<br/>    self.projection = projection<br/>    self.projection_dims = projection_dims<br/>    self.pred = pred</span><span id="3b99" class="mc kz hu ly b fv mh me l mf mg">def nearest_neighbors(self, sess, in_arr,<br/>                        k):<br/>    """Finds the nearest neighbors to a vector.</span><span id="7f06" class="mc kz hu ly b fv mh me l mf mg">Args:<br/>      sess: Session to use.<br/>      in_arr: Vector to find nearest neighbors to.<br/>      k: Number of nearest neighbors to return<br/>    Returns:<br/>      List of up to k pairs of (word, score).<br/>    """<br/>    v = sess.run(self.pred, feed_dict={self.data_p: in_arr})<br/>    return self.client.similar_by_vector(v.flatten().astype(float), topn=k)</span><span id="d048" class="mc kz hu ly b fv mh me l mf mg">def write_to_file(self, sess, f):<br/>    """Writes a model to disk."""<br/>    np.savetxt(f, sess.run(self.projection))</span><span id="dbe9" class="mc kz hu ly b fv mh me l mf mg">def read_from_file(self, sess, f):<br/>    """Reads a model from disk."""<br/>    loaded_projection = np.loadtxt(f).reshape(<br/>        [self.embed_dim, self.projection_dims])<br/>    sess.run(self.projection.assign(loaded_projection))</span><span id="f563" class="mc kz hu ly b fv mh me l mf mg">def fit(self,<br/>          sess,<br/>          data,<br/>          data_p,<br/>          labels,<br/>          labels_p,<br/>          protect,<br/>          protect_p,<br/>          gender_direction,<br/>          pred_learning_rate,<br/>          protect_learning_rate,<br/>          protect_loss_weight,<br/>          num_steps,<br/>          batch_size,<br/>          debug_interval=1000):<br/>    """Trains a model.</span><span id="dc8b" class="mc kz hu ly b fv mh me l mf mg">Args:<br/>      sess: Session.<br/>      data: Features for the training data.<br/>      data_p: Placeholder for the features for the training data.<br/>      labels: Labels for the training data.<br/>      labels_p: Placeholder for the labels for the training data.<br/>      protect: Protected variables.<br/>      protect_p: Placeholder for the protected variables.<br/>      gender_direction: The vector from find_gender_direction().<br/>      pred_learning_rate: Learning rate for predicting labels.<br/>      protect_learning_rate: Learning rate for protecting variables.<br/>      protect_loss_weight: The constant 'alpha' found in<br/>          debias_word_embeddings.ipynb.<br/>      num_steps: Number of training steps.<br/>      batch_size: Number of training examples in each step.<br/>      debug_interval: Frequency at which to log performance metrics during<br/>          training.<br/>    """<br/>    feed_dict = {<br/>        data_p: data,<br/>        labels_p: labels,<br/>        protect_p: protect,<br/>    }<br/>    # define the prediction loss<br/>    pred_loss = tf.losses.mean_squared_error(labels_p, self.pred)</span><span id="a272" class="mc kz hu ly b fv mh me l mf mg"># compute the prediction of the protected variable.<br/>    # The "trainable"/"not trainable" designations are for the predictor. The<br/>    # adversary explicitly specifies its own list of weights to train.<br/>    protect_weights = tf.get_variable(<br/>        "protect_weights", [self.embed_dim, 1], trainable=False)<br/>    protect_pred = tf.matmul(self.pred, protect_weights)<br/>    protect_loss = tf.losses.mean_squared_error(protect_p, protect_pred)</span><span id="a319" class="mc kz hu ly b fv mh me l mf mg">pred_opt = tf.train.AdamOptimizer(pred_learning_rate)<br/>    protect_opt = tf.train.AdamOptimizer(protect_learning_rate)</span><span id="be3a" class="mc kz hu ly b fv mh me l mf mg">protect_grad = {v: g for (g, v) in pred_opt.compute_gradients(protect_loss)}<br/>    pred_grad = []</span><span id="1576" class="mc kz hu ly b fv mh me l mf mg"># applies the gradient expression found in the document linked<br/>    # at the top of this file.<br/>    for (g, v) in pred_opt.compute_gradients(pred_loss):<br/>      unit_protect = tf_normalize(protect_grad[v])<br/>      # the two lines below can be commented out to train without debiasing<br/>      g -= tf.reduce_sum(g * unit_protect) * unit_protect<br/>      g -= protect_loss_weight * protect_grad[v]<br/>      pred_grad.append((g, v))<br/>      pred_min = pred_opt.apply_gradients(pred_grad)</span><span id="3d4f" class="mc kz hu ly b fv mh me l mf mg"># compute the loss of the protected variable prediction.<br/>    protect_min = protect_opt.minimize(protect_loss, var_list=[protect_weights])</span><span id="02e7" class="mc kz hu ly b fv mh me l mf mg">sess.run(tf.global_variables_initializer())<br/>    sess.run(tf.local_variables_initializer())<br/>    step = 0<br/>    while step &lt; num_steps:<br/>      # pick samples at random without replacement as a minibatch<br/>      ids = np.random.choice(len(data), batch_size, False)<br/>      data_s, labels_s, protect_s = data[ids], labels[ids], protect[ids]<br/>      sgd_feed_dict = {<br/>          data_p: data_s,<br/>          labels_p: labels_s,<br/>          protect_p: protect_s,<br/>      }</span><span id="ae8b" class="mc kz hu ly b fv mh me l mf mg">if not step % debug_interval:<br/>        metrics = [pred_loss, protect_loss, self.projection]<br/>        metrics_o = sess.run(metrics, feed_dict=feed_dict)<br/>        pred_loss_o, protect_loss_o, proj_o = metrics_o<br/>        # log stats every so often: number of steps that have passed,<br/>        # prediction loss, adversary loss<br/>        print("step: %d; pred_loss_o: %f; protect_loss_o: %f" % (step,<br/>                     pred_loss_o, protect_loss_o))<br/>        for i in range(proj_o.shape[1]):<br/>          print("proj_o: %f; dot(proj_o, gender_direction): %f)" %<br/>                       (np.linalg.norm(proj_o[:, i]),<br/>                       np.dot(proj_o[:, i].flatten(), gender_direction)))<br/>      sess.run([pred_min, protect_min], feed_dict=sgd_feed_dict)<br/>      step += 1<br/>      <br/>def filter_analogies(analogies,<br/>                     index_map):<br/>  filtered_analogies = []<br/>  for analogy in analogies:<br/>    if filter(index_map.has_key, analogy) != analogy:<br/>      print "at least one word missing for analogy: %s" % analogy<br/>    else:<br/>      filtered_analogies.append(map(index_map.get, analogy))<br/>  return filtered_analogies</span><span id="1bda" class="mc kz hu ly b fv mh me l mf mg">def make_data(<br/>    analogies, embed,<br/>    gender_direction):<br/>  """Preps the training data.</span><span id="7c3d" class="mc kz hu ly b fv mh me l mf mg">Args:<br/>    analogies: a list of analogies<br/>    embed: the embedding matrix from load_vectors<br/>    gender_direction: the gender direction from find_gender_direction</span><span id="4c7c" class="mc kz hu ly b fv mh me l mf mg">Returns:<br/>    Three numpy arrays corresponding respectively to the input, output, and<br/>    protected variables.<br/>  """<br/>  data = []<br/>  labels = []<br/>  protect = []<br/>  for analogy in analogies:<br/>    # the input is just the word embeddings of the first three words<br/>    data.append(embed[analogy[:3]])<br/>    # the output is just the word embeddings of the last word<br/>    labels.append(embed[analogy[3]])<br/>    # the protected variable is the gender component of the output embedding.<br/>    # the extra pair of [] is so that the array has the right shape after<br/>    # it is converted to a numpy array.<br/>    protect.append([np.dot(embed[analogy[3]], gender_direction)])<br/>  # Convert all three to numpy arrays, and return them.<br/>  return tuple(map(np.array, (data, labels, protect))</span></pre><p id="122d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">对抗方法有助于减少单词嵌入中的偏差，并且可以很好地推广到其他领域和任务。通过试图对对手隐藏受保护变量，机器学习系统可以减少系统中隐含的关于受保护变量的有偏见的信息量。除了特定的方法之外，在这个主题上还有许多变体，可以用来实现不同程度和类型的去偏置。</p><p id="fb67" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">希望你喜欢阅读这个故事，并发现它是有帮助的。谢谢你。</p></div></div>    
</body>
</html>