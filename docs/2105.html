<html>
<head>
<title>Deep Learning: Feedforward Neural Networks Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:解释前馈神经网络</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/deep-learning-feedforward-neural-networks-explained-c34ae3f084f1?source=collection_archive---------2-----------------------#2019-04-01">https://medium.com/hackernoon/deep-learning-feedforward-neural-networks-explained-c34ae3f084f1?source=collection_archive---------2-----------------------#2019-04-01</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/78f0e6ecdcc51e31522b955f627e23a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4XfTBSAQWAmiYjQK"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Photo by <a class="ae jg" href="https://unsplash.com/@barkiple?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">John Barkiple</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e1a6" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">前馈神经网络也被称为<strong class="jj hv">多层神经元网络</strong> (MLN)。这些模型网络被称为前馈，因为信息仅在神经网络中向前传播，通过输入节点，然后通过隐藏层(单层或多层)，最后通过输出节点。在MLN，没有反馈连接，因此网络的输出被反馈回自身。这些网络由许多更简单的模型(sigmoid神经元)的组合来表示。</p><blockquote class="kf kg kh"><p id="dc43" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated"><em class="hu">引用注:本文内容和结构基于四分之一实验室的深度学习讲座——</em><a class="ae jg" href="https://padhai.onefourthlabs.in" rel="noopener ugc nofollow" target="_blank"><em class="hu">帕德海</em> </a> <em class="hu">。</em></p></blockquote><h1 id="7fde" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">动机:非线性数据</h1><p id="c233" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在我们谈论前馈神经网络之前，让我们了解一下对这种神经网络的需求是什么。像感知器这样的传统模型——它接受真实输入并给出布尔输出，只有在数据是线性可分的情况下才有效。这意味着正点(绿色)应该位于边界的一侧，负点(红色)位于边界的另一侧。正如你从下图中看到的，感知器在寻找最佳决策边界来区分正负点方面做得很差。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff lp"><img src="../Images/d72995a06bbac9cd9be0d35a1729ed88.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*snMv0UUBXNY576Bc8D-oKQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Perceptron Decision Surface for Non-Linear Data</figcaption></figure><p id="7875" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们有sigmoid神经元模型，它类似于<a class="ae jg" href="https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6" rel="noopener ugc nofollow" target="_blank">感知器</a>，但是sigmoid模型稍有修改，使得sigmoid神经元的输出比感知器的阶跃函数输出平滑得多。尽管我们已经引入了非线性sigmoid神经元函数，但是它仍然不能够有效地将红点(阴性)与绿点(阳性)分开。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff lu"><img src="../Images/162337181c53ced8fb6ce073d4fc36f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*elrNjMoQrsuit2Xaa2xRUQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Sigmoid Neuron Decision Boundary for Non-Linear Data</figcaption></figure><p id="912d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">重要的一点是，从感知器中的刚性决策边界开始，我们已经朝着创建适用于非线性可分离数据的决策边界的方向迈出了第一步。因此，乙状结肠神经元是我们前馈神经网络的构建模块。</p><p id="2e62" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">从我之前关于<a class="ae jg" href="https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>的帖子来看，我们已经证明了即使单个sigmoid神经元无法处理非线性数据。如果我们以有效的方式连接多个sigmoid神经元，我们可以将神经元的组合近似为处理非线性数据所需的输入和输出之间的任何复杂关系。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lv"><img src="../Images/ca4f1ee2289f1429e7d9dffa9b2d438f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ToDCO2n9YAmffZewMSrv9Q.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Combination of Sigmoid Neurons for Non-Linear Data</figcaption></figure></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><h1 id="6757" class="km kn hu bd ko kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj dt translated">前馈神经网络</h1><p id="ab1f" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">多层神经元网络由许多sigmoid神经元组成。mln能够处理非线性可分离数据。输入层和输出层之间的层称为隐藏层。隐藏层用于处理输入和输出之间复杂的非线性可分关系。</p><h1 id="68cb" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">简单深度神经网络</h1><p id="3576" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在本节中，让我们看看如何使用一个非常简单的神经网络来解决复杂的非线性决策边界。</p><p id="1045" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">让我们以一个有两个变量的手机喜欢/不喜欢预测器为例:屏幕尺寸和成本。它有一个复杂的决策边界，如下所示:</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mi"><img src="../Images/3435b97d0599952a396118152aeb24ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*bgzpuHRoV3v7_W9gIjr1Ww.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Decision Boundary</figcaption></figure><p id="4f74" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们知道，通过使用单个sigmoid神经元，不可能获得这种非线性的决策边界。不管我们如何改变乙状结肠神经元参数<strong class="jj hv"> w </strong>和<strong class="jj hv"> b </strong>。现在改变一下情况，用一个简单的神经元网络来解决同样的问题，看看它是如何处理的。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mj"><img src="../Images/cf74d7375cae3ecbe11ce6253bb3313f.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*dGuHU7QWfMjbjMjZhywGnw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Simple Neural Network</figcaption></figure><p id="b767" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们有我们的输入x₁-屏幕尺寸和x₂—价格进入网络随着偏见b₁和b₂.</p><p id="4fb1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在让我们逐个神经元地分解模型神经元来理解。我们在第一层中有第一个神经元(最左边),它连接到输入x₁和x₂，权重为w₁₁和w₁₂，偏置为b₁.神经元输出表示为h₁，它是具有参数w₁₁和w₁₂.的x₁和x₂的函数</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mk"><img src="../Images/6d1be8659ece820b848d62944744ef23.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*seQwO9cckGx4LyBySh7TiQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Output of the first Neuron</figcaption></figure><p id="b4e3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果我们将sigmoid函数应用于具有适当权重的输入x₁和x₂w₁₁、w₁₂和偏置b₁，我们将得到输出h₁，它将是0和1之间的某个实数值。第一神经元h₁的sigmoid输出将由下面的等式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/31c82f95948ab56af16e087afce387d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*ixpOs7PWLXNvUy31kW0gwA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Output Sigmoid for First Neuron</figcaption></figure><p id="1ad2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">接下来，我们在第一层有另一个神经元，它连接到输入x₁和x₂，权重为w₁₃和w₁₄，偏置为b₂.第二个神经元的输出表示为h₂.</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/af05cff2fb8887302d25214bb930b8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*IbsE7OW_V3AoS_3_HwBAhw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Output of the second Neuron</figcaption></figure><p id="501a" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">类似地，第二神经元h₂的sigmoid输出将由下面的等式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/967dcde1765744b99e37f662e4deef71.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*N4xvcLuStVMy6TL0nmuAnA.png"/></div></figure><p id="ecc1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt mo translated">到目前为止，我们已经看到了第一层中存在的神经元，但我们还有另一个输出神经元，它将h₁和h₂作为输入，这与前面的神经元相反。这个神经元的输出将是最终的预测输出，它是h₁和h₂.的函数预测输出由下面的等式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mx"><img src="../Images/8d69af524aa42761b2c87974ab87f486.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*dPSjQv38N-873WXXYwXovg.png"/></div></figure><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff my"><img src="../Images/e605988caf1926abff3ceeb8f047e327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2_g7msPQHGw12f65h_hFg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Complex looking Equation!! Really?. Comment if you understood it or not.</figcaption></figure><p id="2e8c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">以前，我们只能调整单个乙状结肠神经元的w₁、w₂和b参数。现在，我们可以调整9个参数(w₁₁、w₁₂、w₁₃、w₁₄、w₂₁、w₂₂、b₁、b₂、b ),这允许处理更复杂的决策边界。通过尝试这些参数的不同配置，我们将能够找到最佳表面，其中整个中间区域(红点)的输出为1，而其他地方的输出为0，这正是我们所期望的。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/9f13a1e80424acb21a20e9d1976b1cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*NOzP0S7ju3GRog-qD7ofOw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Decision Boundary from Network</figcaption></figure><p id="5360" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">需要注意的重要一点是，即使使用简单的神经网络，我们也能够模拟输入和输出之间的复杂关系。现在的问题是，我们如何预先知道这种特殊的配置是好的，为什么不在第一层之间增加几层或增加几个神经元。所有这些问题都是有效的，但现在，我们将保持事情简单，采取网络，因为它是。当我们讨论<em class="ki">超参数调整</em>时，我们将讨论这些问题以及更多细节。</p><h1 id="2754" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">通用深度神经网络</h1><p id="d71d" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在前面，我们已经看到了用于特定任务的神经网络，现在我们将讨论一般形式的神经网络。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff na"><img src="../Images/d5aca61861865071556f6b2891a3d21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*lFbOLNUlg5tnOWxULpHkPA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Generic Network without Connections</figcaption></figure><p id="ed0b" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">让我们假设我们的神经元网络有两个隐藏层(蓝色，但如果需要可以有两层以上)，每个隐藏层有3个乙状结肠神经元，可以有更多的神经元，但现在我保持事情简单。我们有三个输入进入网络(为了简单起见，我只用了三个输入，但它可以接受n个输入)，在输出层有两个神经元。正如我之前所说的，我们将按原样接受这个网络，并理解深层神经网络的复杂性。</p><p id="c648" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">首先，我将解释术语，然后我们将进入这些神经元如何相互作用。对于这些神经元中的每一个，将会发生两件事</p><ol class=""><li id="627d" class="nb nc hu jj b jk jl jo jp js nd jw ne ka nf ke ng nh ni nj dt translated">预激活用“a”表示:它是输入加上偏差的加权和。</li><li id="957a" class="nb nc hu jj b jk nk jo nl js nm jw nn ka no ke ng nh ni nj dt translated">激活用“h”表示:激活函数是Sigmoid函数。</li></ol><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff np"><img src="../Images/5336367415d1234149da2740bf8d8919.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*LsmIIg6u4BhGHIykJNa2hA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Generic Network with Connections</figcaption></figure><p id="9c7d" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">让我们一个神经元一个神经元地去理解网络神经元。考虑第一个隐藏层中的第一个神经元。第一个神经元通过权重₁.连接到每个输入端</p><blockquote class="kf kg kh"><p id="5faa" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">接下来，我将使用这种形式的指数来表示与特定神经元相关的权重和偏差，</p><p id="f6fb" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">w(层数)(层中的神经元数)(输入数)</p><p id="7dc4" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">b(层数)(与该输入相关的偏置数)</p></blockquote><p id="f0c5" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">与连接到第一输入的第一隐藏层中存在的第一神经元相关联的W₁₁₁—权重。</p><p id="1b91" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">与连接到第二输入的第一隐藏层中存在的第一神经元相关联的W₁₁₂—权重。</p><p id="a004" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">b₁₁ —与第一个隐藏层中的第一个神经元相关联的偏差。</p><p id="e952" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">b₁₂ —与第一个隐藏层中的第二个神经元相关联的偏差。</p><p id="e048" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt">….</p><p id="79b1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这里，W₁是一个权重矩阵，包含与各个输入相关联的各个权重。每层的预激活是来自前一层的输入加上偏置的加权和。在每一层“I”上预激活的数学方程由下式给出:</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nq"><img src="../Images/eaa9ad3de423628c6983142da2f96152.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*RstFETHbE9PqG7jRlK7P6w.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Pre-activation Function</figcaption></figure><p id="f400" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">每层的激活等于将sigmoid函数应用于该层的预激活输出。在每一层“I”上激活的数学方程由下式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nr"><img src="../Images/48d1e86fbd76f68b093f6df0127ac4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*T2AwzEQ6cvadyVKgOirnsw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Activation Function</figcaption></figure><p id="b20e" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">最后，我们可以通过对前一层的预激活输出应用某种激活函数(根据任务可以是softmax)来获得神经网络的预测输出。预测输出的等式如下所示，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ns"><img src="../Images/6a2725e16071fa8c3262df77ce680267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIkxFlR6_wMYodI0tvR8fA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Output Activation Function</figcaption></figure><h1 id="ccc5" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">深度神经网络中的计算</h1><p id="d9c7" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们已经看到了神经网络的术语和功能方面。现在我们将看到DNN内部的计算是如何发生的。</p><p id="b930" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">假设你有100个输入，在第一和第二隐藏层有10个神经元。100个输入中的每一个将被连接到神经元，这将是第一个神经元的权重矩阵，W₁将具有总共10×100个权重。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nt"><img src="../Images/141d46c6ff3859e655fea26b792ff758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*Th1fOEQkOf-YN9Yl-vMRQg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Weight Matrix</figcaption></figure><p id="6b60" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">请记住，对于权重和偏差变量的指数，我们遵循非常具体的格式，如下所示。</p><blockquote class="kf kg kh"><p id="4842" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">w(层数)(层中的神经元数)(输入数)</p><p id="3253" class="jh ji ki jj b jk jl jm jn jo jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd ke hn dt translated">b(层数)(与该输入相关的偏置数)</p></blockquote><p id="aaf3" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在让我们看看如何计算第一层a₁₁.的第一个神经元的预激活我们知道预激活只不过是输入加上偏差的加权和。换句话说，它是权重矩阵W₁的第一行和输入矩阵x加上偏差b₁₁.之间的点积</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/bdf87ccc3a8c91a529ba7b07c663868b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*ujHtfoIqE756sHCUj0fdNA.png"/></div></figure><p id="1d6c" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">类似地，第一层中其他9个神经元的预激活由下式给出:</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nv"><img src="../Images/45ab76d9bc5f8862c768d2feb52cc062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5JX1ZF7rA10AyZFFIC0-w.png"/></div></div></figure><p id="5981" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">简而言之，第一层的整体预激活由下式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/f21a8e16e4f7ad7617dfef987b5e4ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*_l5_Q62hiD8_pk9m0lYPqQ.png"/></div></figure><p id="2013" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在哪里，</p><p id="50b2" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">W₁是包含与相应输入相关联的个体权重的矩阵，而b₁是containing(b₁₁、b₁₂、b₁₃,….的向量,b₁₀)与乙状结肠神经元相关的个体偏见。</p><p id="a6d4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">第一层的激活由下式给出，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/db4ae441bbb3bba96bf8d85d34cc89a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*l4O32Lzv_hXfBKNdhWzDWQ.png"/></div></figure><p id="76c4" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">其中“g”代表sigmoid函数。</p><p id="b0aa" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">请记住，a₁是10个预激活值的向量，这里我们对所有这10个值应用元素式sigmoid函数，并将它们存储在另一个表示为h₁.的向量中类似地，我们可以计算网络中存在的“n”个隐藏层的预激活和激活值。</p><h1 id="5cf6" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">DNN的输出层</h1><p id="ce46" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">到目前为止，我们已经讨论了隐藏层中的计算。现在我们将讨论输出层中的计算。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff np"><img src="../Images/5336367415d1234149da2740bf8d8919.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*LsmIIg6u4BhGHIykJNa2hA.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Generic Network with Connections</figcaption></figure><p id="61ca" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们可以通过取与前一层h₂加上偏置向量b₃.的激活相关联的W₃和权重的点积来计算输出层的预激活a₃</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ny"><img src="../Images/201511c8a3779b0b847ccf94c2d8e87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rX_NmjoTKw_1OmrN7OD6Q.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Pre-activation for the output layer</figcaption></figure><p id="9cb1" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">为了找出网络的预测输出，我们将一些函数(我们还不知道)应用于预激活值。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff nz"><img src="../Images/5c286b652d480f9bf894cc7ea83af793.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*BDjwQohNtvwneaDRUCGi6w.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Predicted Values</figcaption></figure><p id="ce80" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">这两个输出将形成一个概率分布，这意味着它们的总和等于1。</p><blockquote class="oa"><p id="6809" class="ob oc hu bd od oe of og oh oi oj ke ek translated">根据手头的任务选择输出激活函数，可以是softmax或线性。</p></blockquote><h1 id="9fa7" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ok kz la lb ol ld le lf om lh li lj dt translated">Softmax函数</h1><p id="238f" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们将使用Softmax函数作为输出激活函数。分类问题深度学习中最常用的激活函数。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff on"><img src="../Images/c2e27c95e4407483978cecdac58fbf40.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*x6Oorhp7TvoVTKMg2TPEDg.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Softmax Function</figcaption></figure><p id="a909" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在Softmax功能中，无论输入如何，输出始终为正。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff oo"><img src="../Images/c64801ed8f04f35c609ee5bbbeabfc17.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*W52HQyA0ifVDb4Sf9FtTAg.png"/></div></figure><p id="4319" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">现在，让我们在上面显示的具有4个输出神经元的网络上说明Softmax函数。所有这4个神经元的输出用向量“a”表示。对于这个向量，我们将应用我们的softmax激活函数来获得如下所示的预测概率分布，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff op"><img src="../Images/6f3829d4bb5b840ef57d6313679776dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*vKg6AqrweRGpsMI0Dks4Vw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Applying the Softmax Function</figcaption></figure><p id="cb41" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">通过应用softmax函数，我们将得到一个预测的概率分布，我们的真实输出也是一个概率分布，我们可以比较这两个分布来计算网络的损耗。</p></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><h1 id="98ff" class="km kn hu bd ko kp md kr ks kt me kv kw kx mf kz la lb mg ld le lf mh lh li lj dt translated">损失函数</h1><p id="aa70" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在这一节中，我们将讨论二值和多值分类的损失函数。损失函数的目的是告诉模型在学习过程中需要进行一些校正。</p><p id="db12" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">一般来说，输出层中神经元的数量等于类的数量。但是在二元分类的情况下，我们可以仅使用一个输出概率P(Y=1)的sigmoid神经元，因此我们可以获得P(Y=0) = 1-P(Y=1)。在分类的情况下，我们将使用交叉熵损失来比较预测的概率分布和真实的概率分布。</p><p id="e316" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">二元分类的交叉熵损失由下式给出:</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff oq"><img src="../Images/3d020823da21abec24b384090fe8b21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*vganOhn6tNF4CKp9oLJe0A.png"/></div></figure><p id="8ef7" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">多类分类的交叉熵损失由下式给出:</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff or"><img src="../Images/0cc391db26c8adf7a63392ab5a2a2e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*lw7nvSHBwsxjVkAw2Po2Mg.png"/></div></figure><h1 id="e785" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">学习算法:非数学版本</h1><p id="185e" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">我们将会看到使用梯度下降的学习算法的非数学版本。学习算法的目标是确定参数的最佳可能值，使得深度神经网络的总损失尽可能最小。</p><p id="a387" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">学习算法是这样的，</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div class="fe ff os"><img src="../Images/fa7a9c17f1a57bc8c751ea9b798985d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*vbEfs3pVvsO32HvrRd6bRQ.png"/></div></figure><p id="6025" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">我们初始化所有的权重<strong class="jj hv">w</strong>(w₁₁₂,…w₁₁₁)和<strong class="jj hv">b</strong>(b₂,….b₁))随机。然后，我们迭代数据中的所有观察值，对于每个观察值，从神经网络中找到相应的预测分布，并使用交叉熵函数计算损失。基于损失值，我们将更新权重，使得在新参数下模型的总损失将<strong class="jj hv">小于模型的当前损失</strong>。</p><figure class="lq lr ls lt fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ot"><img src="../Images/00d94123cad58b08345d092aaa3c89ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VkvteCNL4N9ayg0f"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Photo by <a class="ae jg" href="https://unsplash.com/@napr0tiv?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Vasily Koloda</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0dd4" class="km kn hu bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj dt translated">结论</h1><p id="b986" class="pw-post-body-paragraph jh ji hu jj b jk lk jm jn jo ll jq jr js lm ju jv jw ln jy jz ka lo kc kd ke hn dt translated">在这篇文章中，我们简要介绍了感知器和sigmoid neuron等传统模型在处理非线性数据方面的局限性，然后我们继续探讨如何使用简单的神经网络来解决复杂的决策边界问题。然后，我们从一般意义上看神经网络，并详细研究神经网络背后的计算。最后，我们看了深度神经网络的学习算法。</p></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><p id="7189" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">如果你有兴趣了解更多关于人工神经网络的知识，请查看来自<a class="ae jg" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的Abhishek和Pukhraj的<a class="ae jg" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。还有，这门课会用最新版本的Tensorflow 2.0 (Keras后端)来教。他们还有一个非常好的包，关于Python和R语言的<a class="ae jg" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">机器学习(基础+高级)</a>。</p><p id="cb9f" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><em class="ki">推荐阅读:</em></p><div class="ou ov fm fo ow ox"><a href="https://hackernoon.com/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab ej"><div class="oz ab pa cl cj pb"><h2 class="bd hv fv z el pc eo ep pd er et ht dt translated">用Python从头开始构建前馈神经网络</h2><div class="pe l"><h3 class="bd b fv z el pc eo ep pd er et ek translated">在没有任何框架的情况下，构建您的第一个通用前馈神经网络</h3></div><div class="pf l"><p class="bd b gc z el pc eo ep pd er et ek translated">hackernoon.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ja ox"/></div></div></a></div><div class="ou ov fm fo ow ox"><a href="https://towardsdatascience.com/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07" rel="noopener follow" target="_blank"><div class="oy ab ej"><div class="oz ab pa cl cj pb"><h2 class="bd hv fv z el pc eo ep pd er et ht dt translated">用数学解释的Sigmoid神经元学习算法</h2><div class="pe l"><h3 class="bd b fv z el pc eo ep pd er et ek translated">在本帖中，我们将详细讨论sigmoid神经元学习算法背后的数学直觉。</h3></div><div class="pf l"><p class="bd b gc z el pc eo ep pd er et ek translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pm l pi pj pk pg pl ja ox"/></div></div></a></div><p id="fe66" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">在我的下一篇文章中，我们将讨论如何使用numpy在python中从头实现前馈神经网络。所以确保你在媒体上跟踪我，以便在它下降时得到通知。</p><p id="28e0" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">直到那时和平:)</p><p id="55cb" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">NK。</p></div><div class="ab cl lw lx hc ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hn ho hp hq hr"><p id="7028" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated">Niranjan Kumar 在汇丰银行数据分析部门实习。他对深度学习和人工智能充满热情。他是<a class="pn po gr" href="https://medium.com/u/504c7870fdb6?source=post_page-----c34ae3f084f1--------------------------------" rel="noopener" target="_blank"> Medium </a>在<a class="ae jg" rel="noopener" href="/tag/artificial-intelligence/top-writers">人工智能</a>的顶尖作家之一。在<a class="ae jg" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，或者在<a class="ae jg" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我，了解关于深度学习和人工智能的最新文章。</p><p id="4d1a" class="pw-post-body-paragraph jh ji hu jj b jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke hn dt translated"><strong class="jj hv">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>