<html>
<head>
<title>Illustrative Proof of Universal Approximation Theorem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">普适逼近定理的图示证明</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/illustrative-proof-of-universal-approximation-theorem-5845c02822f6?source=collection_archive---------0-----------------------#2019-03-20">https://medium.com/hackernoon/illustrative-proof-of-universal-approximation-theorem-5845c02822f6?source=collection_archive---------0-----------------------#2019-03-20</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/fd17745bf523a93cd6490ef63de68898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tXF12cHUt5D_mfXxwjj6sQ.png"/></div></div></figure><p id="e08a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这篇文章中，我们将讨论通用逼近定理，我们也将用图表证明该定理。这是我上一篇关于<a class="ae ka" href="https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7" rel="noopener" target="_blank">乙状结肠神经元</a>的后续文章。</p><p id="1e98" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="kb">引用注:本文内容和结构基于四分之一实验室的深度学习讲座——</em><a class="ae ka" href="https://padhai.onefourthlabs.in" rel="noopener ugc nofollow" target="_blank"><em class="kb">pad hai</em></a><em class="kb">。</em></p><h1 id="48b3" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">乙状结肠神经元</h1><p id="cd09" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">在我们进入通用逼近定理的实际讨论之前，我将简要回顾Sigmoid神经元的工作以及它将如何处理非线性可分数据。</p><p id="b668" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">sigmoid神经元类似于感知器神经元，对于每个输入<code class="eh lf lg lh li b">xi</code>,它都具有与该输入相关联的权重<code class="eh lf lg lh li b">wi</code>。权重表明决策过程中输入的重要性。与感知器模型不同，sigmoid的输出不是0或1，而是0-1之间的真实值，可以解释为概率。最常用的sigmoid函数是逻辑函数，它具有“<strong class="je hv"> S </strong>形曲线的特征。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div class="ab fr cl ln"><img src="../Images/55096c9f08a702529cac584e0deafb49.png" data-original-src="https://miro.medium.com/v2/format:webp/1*N7dfPwbiXC-Kk4TCbfRerA.png"/></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Sigmoid Neuron Representation</figcaption></figure><h1 id="4296" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">处理非线性数据</h1><p id="2896" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">从我上一篇关于<a class="ae ka" href="https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7" rel="noopener" target="_blank"> Sigmoid神经元</a>的文章中，我们已经看到，即使我们引入了非线性Sigmoid，它仍然不能有效地区分1和0。重要的一点是，从<a class="ae ka" href="https://hackernoon.com/perceptron-deep-learning-basics-3a938c5f84b6" rel="noopener ugc nofollow" target="_blank">感知器</a>中的刚性决策边界开始，我们已经朝着创建适用于非线性可分离数据的决策边界的方向迈出了第一步。因此，乙状结肠神经元是深层神经网络的构建模块。</p><h1 id="30d3" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">复杂关系建模</h1><p id="29fe" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">在现实生活中，我们处理复杂的函数，其中输入和输出之间的关系可能是复杂的，不像简单的sigmoid神经元或感知器。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ls"><img src="../Images/fc7aca9ac98b674fa6d82a1c612ea1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e49COmHaPpL0TmaYG6r4sw.png"/></div></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Complex Functions</figcaption></figure><blockquote class="lt"><p id="ed4d" class="lu lv hu bd lw lx ly lz ma mb mc jz ek translated">我们是怎么想出如此复杂的函数的？</p></blockquote><p id="c770" class="pw-post-body-paragraph jc jd hu je b jf md jh ji jj me jl jm jn mf jp jq jr mg jt ju jv mh jx jy jz hn dt translated">很难想出这样的函数，这样的函数可能有无限的可能性，我们需要一个简单的方法。要解决这个问题，我们来打个盖房子的比方。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff fg"><img src="../Images/0aa84796de51b5c568f39b66d956fec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkkqkUiDqSE-dA0neU1YPQ.png"/></div></div></figure><p id="3aea" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">你可以把房子想象成一个复杂的功能，我们建造房子的方式是从一个积木开始，也就是说...在这种情况下是砖头。首先，我们将奠定基础，在此基础上，我们将铺设另一层，我们将继续以这种方式，直到我们得到非常复杂的输出，即…房子。在这个过程中，我们以一种有效的方式将非常简单的积木组合在一起，这样我们就可以得到这个房子。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mi"><img src="../Images/5ecab7b6c44913d06a4869c6f8c606bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xkbqUdU7rmix0jxXWm4Vcw.png"/></div></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Modeling Complex Functions</figcaption></figure><p id="f597" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在我们的例子中，我们对复杂的功能感兴趣，而不是砖块和房子。构建复杂函数的基础是sigmoid神经元。我们将创建复杂函数的方式是，我们将以有效的方式组合乙状结肠神经元，以便在不同层的神经元之间存在相互作用。因此预测的输出是输入的复函数，并且将存在跨多层关联的权重。现在问题来了，我们如何决定使用多少层，每层有多少个神经元等等…这些都是有效的问题，但对这些问题的讨论超出了本文的范围。</p><p id="9e59" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在我需要向你们证明，使用神经元网络创建的复杂函数的输出非常接近输入和输出之间的真实关系。</p><blockquote class="mj mk ml"><p id="7dad" class="jc jd kb je b jf jg jh ji jj jk jl jm mm jo jp jq mn js jt ju mo jw jx jy jz hn dt translated">具有一定数量隐层的深度神经网络应该能够逼近存在于输入和输出之间的任何函数。</p></blockquote><p id="70fe" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">普适逼近定理</strong>说:<br/>一个单隐层包含有限个神经元的前馈网络，在激活函数的温和假设下，可以逼近<strong class="je hv"> R </strong>的紧致子集上的连续函数——维基百科</p><p id="68d8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">简单地说，UAT说——你总是可以提出一个深度神经网络，它将逼近输入和输出之间的任何复杂关系。</p><h1 id="2d1d" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">普适逼近定理的证明</h1><p id="70f3" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">在这一节中，我们将看到通用逼近定理的说明性证明。</p><p id="15a1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了进行说明性证明，我们将考虑一个简单的例子，其中我们有一维输入<strong class="je hv"> x </strong>和输出<strong class="je hv"> y </strong>，下图显示了输入和输出之间的真实关系。让我们假设我不知道在<strong class="je hv"> x </strong>和<strong class="je hv"> y </strong>之间的函数方程，我不能给出这个方程的表达式。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div class="fe ff mp"><img src="../Images/333a227a18238b58baa45d36d019bd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*U1lpcOpovIt1x4LvTEXvTQ.png"/></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">True relationship between x and y</figcaption></figure><p id="541b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为了解决这个问题，我将把这个函数分解成多个更小的部分，这样每个部分都由一个更简单的函数来表示。通过组合一系列更小的函数(矩形条/塔),我可以尽可能接近真实的关系来逼近<strong class="je hv"> x </strong>和<strong class="je hv"> y </strong>之间的关系。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div class="fe ff mq"><img src="../Images/1ca97e1ec54b5d84bdbf2b81b254383a.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*93v_a6eBjse-w1LsUwh5QA.png"/></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Combination of simpler functions</figcaption></figure><p id="1a18" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这种情况下，需要注意的关键点是，我不必担心用复杂的方程来表示输入和输出之间的关系。我可以想出一个简单的函数，用这些函数的组合来逼近我的真实关系。我在这个方法中选择的函数越多，我的近似值就越好。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mr"><img src="../Images/61e34f8cce9dce70d5e713cd56710828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hVuJgUTLUFWTMmJhl_fomg.png"/></div></div></figure><blockquote class="lt"><p id="6a83" class="lu lv hu bd lw lx ly lz ma mb mc jz ek translated">我们如何想出这些矩形/塔，以及它将如何与乙状结肠神经元联系起来？</p></blockquote><p id="c9f3" class="pw-post-body-paragraph jc jd hu je b jf md jh ji jj me jl jm jn mf jp jq jr mg jt ju jv mh jx jy jz hn dt translated">让我们取两个斜率非常陡的sigmoid函数，注意它们的峰值位置不同。左边的s形峰值正好在零之前，右边的s形峰值正好在零之后。如果我减去这两个函数，最终结果将是一座塔(矩形输出)。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ms"><img src="../Images/54172bb9ec8fc6f8e41076637632608b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8eGR7VLwDXSHreIKjcPpQ.png"/></div></div></figure><p id="30c7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果我们能得到这些塔的级数，那么我们就能逼近输入和输出之间的任何真函数。</p><h1 id="d274" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">神经网络</h1><p id="5fa9" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">我们能不能想出一个神经网络来表示这种从一个sigmoid函数中减去另一个sigmoid函数的操作？。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mt"><img src="../Images/ec16760681d23542157e2dc45a169909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xC6gbRSzYpWN1hxhkZYSNg.png"/></div></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Neural network for subtraction</figcaption></figure><p id="517e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果我们有一个输入<strong class="je hv"> x </strong>，它通过两个乙状结肠神经元，这两个神经元的输出在另一个权重为+1和-1的神经元中合并，也就是说…这与减去这两个输出相同，那么我们将得到我们的塔。现在你可以看到我们已经准备好了构建模块，它是三个乙状结肠神经元的连接。如果我们能构建许多这样的积木，并把它们全部加起来，我们就能近似出输入和输出之间任何复杂的真实关系。这被称为深度神经网络的<em class="kb">代表能力</em>，可以近似输入和输出之间的任何类型的关系。</p><figure class="lj lk ll lm fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/7929f47c93b3976dfc5904f0e3374719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FvdRIptrsVnn1sQu"/></div></div><figcaption class="lo lp fg fe ff lq lr bd b be z ek">Photo by <a class="ae ka" href="https://unsplash.com/@sickle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sergey Pesterev</a> on <a class="ae ka" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="5d13" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">结论</h1><p id="74b9" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">在这篇文章中，我们简要介绍了sigmoid neuron的工作原理及其处理非线性数据的能力。然后，我们看了如何对复杂关系建模，并看到了通用逼近定理的正式定义，然后继续看通用逼近定理的说明性证明。</p><h1 id="2b45" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">了解更多信息</h1><p id="7eb0" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">如果你想学习更多的数据科学，机器学习。查看来自Starttechacademy<a class="ae ka" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">的Abhishek和Pukhraj的</a><a class="ae ka" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">机器学习基础知识</a>和<a class="ae ka" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">高级机器学习</a>。这些课程的一个优点是它们同时用Python和R语言授课，所以这是你的选择。</p><p id="a47a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="kb">推荐阅读</em></p><div class="mv mw fm fo mx my"><a href="https://towardsdatascience.com/sigmoid-neuron-deep-neural-networks-a4cd35b629d7" rel="noopener follow" target="_blank"><div class="mz ab ej"><div class="na ab nb cl cj nc"><h2 class="bd hv fv z el nd eo ep ne er et ht dt translated">Sigmoid神经元—深度神经网络</h2><div class="nf l"><h3 class="bd b fv z el nd eo ep ne er et ek translated">深层神经网络的构建模块被称为乙状结肠神经元。</h3></div><div class="ng l"><p class="bd b gc z el nd eo ep ne er et ek translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ja my"/></div></div></a></div><div class="mv mw fm fo mx my"><a href="https://hackernoon.com/@niranjankumarc" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab ej"><div class="na ab nb cl cj nc"><h2 class="bd hv fv z el nd eo ep ne er et ht dt translated">Niranjan Kumar -黑客正午</h2><div class="nf l"><h3 class="bd b fv z el nd eo ep ne er et ek translated">阅读《黑客正午》中Niranjan Kumar的文章。汇丰分析的实习生。ML和DL爱好者。作家在…</h3></div><div class="ng l"><p class="bd b gc z el nd eo ep ne er et ek translated">hackernoon.com</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm ja my"/></div></div></a></div><blockquote class="mj mk ml"><p id="e647" class="jc jd kb je b jf jg jh ji jj jk jl jm mm jo jp jq mn js jt ju mo jw jx jy jz hn dt translated">在我的下一篇文章中，我们将详细讨论用于回归和分类任务的前馈神经网络。敬请关注。</p></blockquote></div><div class="ab cl no np hc nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="hn ho hp hq hr"><p id="409a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><a class="nv nw gr" href="https://medium.com/u/3e4fb2985698?source=post_page-----5845c02822f6--------------------------------" rel="noopener" target="_blank"> Niranjan Kumar </a>在汇丰银行分析部门实习。他对深度学习和人工智能充满热情。他是<a class="ae ka" rel="noopener" href="/tag/artificial-intelligence/top-writers">人工智能</a>中<a class="nv nw gr" href="https://medium.com/u/504c7870fdb6?source=post_page-----5845c02822f6--------------------------------" rel="noopener" target="_blank">媒体</a>的顶尖作家之一。在LinkedIn<a class="ae ka" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank">上与我联系，或者在Twitter</a><a class="ae ka" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank">上关注我，了解关于深度学习和人工智能的最新文章。</a></p><p id="0624" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p></div></div>    
</body>
</html>