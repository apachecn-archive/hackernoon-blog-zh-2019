<html>
<head>
<title>Building a Bigram Hidden Markov Model for Part-Of-Speech Tagging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建词性标注的二元隐马尔可夫模型</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/building-a-bigram-hidden-markov-model-for-part-of-speech-tagging-1b784a87ab2c?source=collection_archive---------0-----------------------#2019-05-19">https://medium.com/hackernoon/building-a-bigram-hidden-markov-model-for-part-of-speech-tagging-1b784a87ab2c?source=collection_archive---------0-----------------------#2019-05-19</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/8300c43c22985da5044395a4a78ab028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JxGuQ-I436JQPLh-UFTcSA.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Image credits: Google Images</figcaption></figure><p id="3876" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">词性标注是许多自然语言处理管道的重要组成部分，其中句子中的单词用它们各自的词性来标记。词性标注的一个例子是组块。组块是标记句子中的多个单词以将它们组合成更大的“组块”的过程。这些组块随后可以用于诸如命名实体识别之类的任务。让我们深入探讨词性标注，看看如何使用隐马尔可夫模型和维特比解码算法来构建一个词性标注系统。</p><p id="5211" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在这篇文章的底部可以找到一个示例实现的链接。</p><h2 id="49b8" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">什么是POS标签？</h2><p id="9c4a" class="pw-post-body-paragraph jg jh hu ji b jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz le kb kc kd hn dt translated">如下图所示，有9个主要的词类。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff lf"><img src="../Images/9029346d46c282cc35d4c6d58dd70b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*vgCmmpkmdW6_6OOnVlSqkw.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Image credits: Google Images</figcaption></figure><p id="d116" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">大多数NLP应用程序中使用的POS标签比这更细粒度。这方面的一个例子是NN和NNS，其中NN用于单数名词，如“桌子”，而NNS用于复数名词，如“桌子”。最突出的标记集是由36个POS标记组成的Penn Treebank标记集。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lk"><img src="../Images/7da4183fea4fcbe6652e99d3ddee6ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*UTGp9grePRvgvekpULT0Wg.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Subset of the Penn Treebank tagset</figcaption></figure><p id="d507" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">完整的宾州树木银行标签集可以在<a class="ae ll" href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="39cc" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">隐马尔可夫模型</h2><p id="a9ec" class="pw-post-body-paragraph jg jh hu ji b jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz le kb kc kd hn dt translated">幸运的是，我们不必手动执行POS标记。相反，我们将使用隐马尔可夫模型进行词性标注。</p><blockquote class="lm"><p id="21af" class="ln lo hu bd lp lq lr ls lt lu lv kd ek translated">对于我们这些从未听说过隐马尔可夫模型(hmm)的人来说，hmm是具有隐藏状态的马尔可夫模型。那么什么是马尔可夫模型，我们所说的隐藏状态是什么意思呢？</p></blockquote><p id="a498" class="pw-post-body-paragraph jg jh hu ji b jj lw jl jm jn lx jp jq jr ly jt ju jv lz jx jy jz ma kb kc kd hn dt translated">马尔可夫模型是一种随机(概率)模型，用于表示未来状态仅取决于当前状态的系统。出于词性标注的目的，我们进行简化假设，我们可以使用有限状态转移网络来表示马尔可夫模型。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mb"><img src="../Images/fd9c13a6d866060da73b477194dd3ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOUQAjMacTx18R6py1r9tA.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">A finite state transition network representing a Markov model. Image credits: Professor Ralph Grishman at NYU.</figcaption></figure><p id="1920" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">有限状态转移网络中的每个节点代表一个状态，离开节点的每个有向边代表从该状态到另一个状态的可能转移。请注意，每条边都标有一个数字，代表当前状态下给定转换发生的概率。还要注意，从任何给定状态转移出来的概率总和总是1。</p><p id="14b9" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在上面的有限状态转移网络中，每个状态都是可观察的。我们可以看到一只猫在一只狗汪汪叫后喵喵叫的频率。如果我们的猫和狗是双语的。也就是说，如果猫和狗都能喵叫和汪汪叫呢？此外，让我们假设给定了狗和猫的状态，我们想要从这些状态预测喵喵叫和汪汪叫的序列。在这种情况下，我们只能观察狗和猫，但我们需要预测未被观察到的喵喵叫声。喵喵叫是隐藏状态。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mc"><img src="../Images/9e183c0660d2364c3e7df27f4c4a8e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TkEVO0CDxYKTUJa3hnGSw.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">A finite state transition network representing an HMM. Image credits: Professor Ralph Grishman at NYU.</figcaption></figure><p id="3f21" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">上图是一个有限状态转移网络，代表我们的HMM。黑色箭头代表未被观察到的状态“汪”和“喵”的发射。</p><p id="98a3" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">现在让我们来看看如何计算我们状态的跃迁和发射概率。回到猫和狗的例子，假设我们观察到以下两个状态序列:</p><pre class="lg lh li lj fq md me mf mg aw mh dt"><span id="ebc0" class="kf kg hu me b fv mi mj l mk ml">dog, cat, cat<br/>dog, dog, dog</span></pre><p id="7906" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">那么可以使用最大似然估计来计算转移概率:</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mm"><img src="../Images/a6a43501b47b89b92affb9c61e01e63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*hE6LoDhYsLzCGwx4TPvA0Q.png"/></div></figure><p id="4cd0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，这表示从状态i-1到状态I的转移概率由我们观察到状态i-1转移到状态I的总次数除以我们观察到状态i-1的总次数给出。</p><p id="20fc" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">例如，从状态序列中，我们可以看到序列总是以dog开始。因此，我们处于起始状态两次，两次都是狗，从不猫。因此，从起始状态到dog的转移概率是1，从起始状态到cat的转移概率是0。</p><p id="890b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">让我们再试一次。我们来计算一下从状态狗到状态端的转移概率。从我们的示例状态序列中，我们看到dog只转换到结束状态一次。我们还看到有四个观察到的狗的实例。因此，从狗状态到结束状态的转移概率是0.25。其他转移概率可以用类似的方式计算。</p><p id="e6a0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">也可以使用最大似然估计来计算排放概率:</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mn"><img src="../Images/c347435f146aef41ca370bc18a96f582.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*IlvB_B9xHXnKwzGONgCwPQ.png"/></div></figure><p id="724a" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，这表示给定状态I的标签I的发射概率是我们观察到状态I发射标签I的总次数除以我们观察到状态I的总次数。</p><p id="55da" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">给定上述两个状态序列的以下发射，让我们计算狗发射woof的发射概率:</p><pre class="lg lh li lj fq md me mf mg aw mh dt"><span id="03bb" class="kf kg hu me b fv mi mj l mk ml">woof, woof, meow<br/>meow, woof, woof</span></pre><p id="1a6f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">也就是说，对于第一个状态序列，狗汪汪叫，然后猫汪汪叫，最后猫喵喵叫。我们从状态序列中看到，狗被观察了四次，我们可以从发射中看到，狗汪汪叫了三次。因此，假设我们处于狗的状态，woof的发射概率是0.75。其他发射概率可以用同样的方法计算。为完整起见，完整的有限状态转移网络如下所示:</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mo"><img src="../Images/f53b25b8651d5c590695e7e4cceb4a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11L9I-zxDO7FwAntaWLuEw.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Finite state transition network of the hidden Markov model of our example.</figcaption></figure><p id="536b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">那么我们如何使用hmm进行词性标注呢？当我们执行词性标注时，我们的目标是找到标签T的序列，使得给定单词序列W，我们得到</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mp"><img src="../Images/1b882a97276bc9da4557cd8801bd9864.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*iD1eeKwGH2wx8an8QFvA7Q.png"/></div></figure><p id="fb57" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，我们说的是我们想要找到给定单词序列的概率最高的POS标签序列。由于在我们的隐马尔可夫模型中没有观察到标签发射，我们应用Baye规则将这种概率改变为我们可以使用最大似然估计来计算的方程:</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mq"><img src="../Images/b035668c72f97ce3c962a713ae66ff5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LM6553CDMrsmlkay4YFi1w.png"/></div></div></figure><p id="b43f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">第二个等式是我们应用贝耶法则的地方。看起来像一个被切掉一块的无穷符号的符号意味着与成比例。这是因为对于我们的目的来说P(W)是一个常数，因为改变序列T不会改变概率P(W)。因此，丢弃它不会对最大化概率的最终序列T产生影响。</p><p id="6d5a" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，概率P(W|T)是给定标签序列，我们得到单词序列的概率。为了能够计算这个，我们仍然需要做一个简化的假设。我们需要假设一个单词出现的概率只取决于它自己的标签，而不取决于上下文。也就是说，该单词不依赖于相邻的标签和单词。那么我们有</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mr"><img src="../Images/20b08570ed10f0256fd7d420a8bf2f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*ZS9Miqc8u-3bPxfYvPxsRQ.png"/></div></figure><p id="d14f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，概率P(T)是获得标签序列T的概率。为了计算这个概率，我们还需要做一个简化的假设。这个假设给了我们的二元模型HMM它的名字，所以它通常被称为二元模型假设。我们必须假设得到一个标签的概率只取决于前一个标签，不取决于其他标签。那么我们可以计算P(T)为</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff ms"><img src="../Images/c4b476ced48e932c5a4d598ebe9eff17.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*vy7DvSNCwEKg3Fi5TAtHmg.png"/></div></figure><p id="081a" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">注意，我们可以使用三元模型假设，即一个给定的标签依赖于它之前的两个标签。事实证明，由于需要平滑，计算HMM的三元模型概率比计算二元模型概率需要更多的工作。三元模型确实比二元模型有一些性能优势，但是为了简单起见，我们使用二元模型假设。</p><p id="7c10" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">最后，我们现在能够使用</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mt"><img src="../Images/f1fc779fa859f237d7c8e0aaa0075db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*2FncGysR3Mbvl_iVdYjHXw.png"/></div></figure><p id="13d3" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这个等式中的概率应该看起来很熟悉，因为它们分别是发射概率和跃迁概率。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/44d0ba977d7596eb0166a208a676372f.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*Stec8ut7NQ6h7kwI5mvVJw.png"/></div></figure><p id="592e" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">因此，如果我们要为这个HMM画一个有限状态转移网络，观察到的状态将是标签，单词将是发出的状态，类似于我们的woof和meow示例。我们已经看到，我们可以使用最大似然估计来计算这些概率。</p></div><div class="ab cl mv mw hc mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hn ho hp hq hr"><p id="a012" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">给定一个由句子组成的数据集，这些句子用相应的词性标签进行标记，训练HMM就像计算上述的发射和转移概率一样简单。对于一个示例实现，请查看这里实现的<a class="ae ll" href="https://github.com/AaronCCWong/HMM-POS-Tagger-with-Viterbi-Decoding/blob/master/src/viterbi/BigramModel.java#L195" rel="noopener ugc nofollow" target="_blank">二元模型</a>。这种实现的基本思想是，它主要记录训练期间最大似然估计所需的值。然后，该模型在评估过程中使用训练过程中收集的计数来计算概率。</p></div><div class="ab cl mv mw hc mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hn ho hp hq hr"><p id="4878" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">一个敏锐的读者会想，面对训练中没有看到的单词，模型会做什么。我们稍后将回到处理未知单词的主题，因为我们将看到，能够正确处理未知单词对于模型的性能至关重要。</p><h2 id="976b" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">维特比解码</h2><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nc"><img src="../Images/ec8813b63f8d2f5aa6f14a059c29f1b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e06inZ9V7doHEVCIr_Sj8A.png"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Image credits: Google Images</figcaption></figure><p id="4b18" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">HMM给了我们概率，但是我们想要的是实际的标签序列。我们需要一种算法，在给定一个单词序列的情况下，它能给出正确概率最高的标签序列。一种称为贪婪解码的直观算法会为每个单词选择概率最高的标签，而不考虑上下文，如后续标签。正如我们所知，贪婪算法并不总是返回最优解，事实上，在词性标注的情况下，它会返回次优解。这是因为在为当前单词选择了标签之后，下一个单词的可能标签可能是有限的和次优的，从而导致整体次优的解决方案。</p><p id="76dd" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">相反，我们使用动态规划算法称为维特比。维特比从创建两个表开始。第一个表用于跟踪到达给定小区所需的最大序列概率。如果这还没有意义，那也没关系。我们将看一个例子。第二个表用于跟踪导致第一个表中给定单元概率的实际路径。</p><p id="884e" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">让我们看一个例子来帮助解决这个问题。回到我们之前的汪汪和喵喵的例子，给定序列</p><pre class="lg lh li lj fq md me mf mg aw mh dt"><span id="9f69" class="kf kg hu me b fv mi mj l mk ml">meow woof</span></pre><p id="0e20" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们将使用维特比来寻找导致该序列的最可能的状态序列。首先我们需要创建我们的第一个维特比表。在我们的有限状态转换网络中，每个状态都需要一行。因此，我们的表有4行用于状态开始、狗、猫和结束。我们试图解码一个长度为2的序列，所以我们需要四列。一般来说，我们需要的列数就是我们试图解码的序列的长度。我们需要四列的原因是因为我们试图解码的完整序列实际上是</p><pre class="lg lh li lj fq md me mf mg aw mh dt"><span id="c23e" class="kf kg hu me b fv mi mj l mk ml">&lt;start&gt; meow woof &lt;end&gt;</span></pre><p id="0f80" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">第一个表包含从先前状态到达给定状态的概率。更准确地说，表中每个单元格的值由下式给出</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/da5b9ffd3674cbd178daee15e849c84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*8YeY_WyVczR75_-1agVhPA.png"/></div></figure><p id="56e8" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">让我们使用我们为HMM模型的有限状态转移网络计算的概率来填写我们的示例的表格。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ne"><img src="../Images/9dd2026931048d7fa4db641f57799c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*Nuv2CSLNSGqZYoUE7erJjg.png"/></div></div></figure><p id="11a8" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">请注意，除了起始行之外，第一列的所有位置都是0。这是因为我们例子中的序列总是以<start>开始。从我们的有限状态转移网络中，我们看到起始状态以概率1转移到狗状态，并且永远不会进入猫状态。我们也看到狗发出喵的概率是0.25。同样重要的是要注意，我们不能从开始状态到达开始状态或结束状态。因此我们得到下一列值</start></p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/686059b670cff2fdeef7a325e7c3c662.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*Lmp0iYg3V9Wtedt5gVb_Zg.png"/></div></figure><p id="67ff" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">注意，我们不能从初始状态到达的所有状态的概率都是0。此外，喵列到达狗状态的概率是1 * 1 * 0.25，其中第一个1是前一个单元的概率，第二个1是从前一个状态到狗状态的转移概率，0.25是从当前状态狗发出喵的概率。因此0.25是目前最大的序列概率。继续下一列:</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/8c72d38b8a64f12ca50dd05a94271fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*1uYJUbrHgIQ6mM3pcZ-mOw.png"/></div></figure><p id="e102" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">注意，我们不能从狗的状态到达开始状态，结束状态也不会发出汪汪声，所以这两行的概率都是0。同时，狗和猫状态的单元得到概率0.09375和0.03125，计算方法与我们之前看到的相同，前一个单元的概率0.25乘以各自的跃迁和发射概率。最后，我们得到</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nh"><img src="../Images/1233fe4d1c1938f1fef2fabe98307021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*j6qu28Sw8vRZR8AXfZJyCw.png"/></div></figure><p id="af81" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">至此，猫和狗都可以到<end>。因此，我们必须计算猫和狗到达终点的概率，然后选择概率较高的路径。</end></p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff ni"><img src="../Images/6d39c99f12e13b6787489417640b7002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*3yB11K6Fvf1_Zxp3qsb5DA.png"/></div></figure><p id="4e81" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">从狗到终点的概率比从猫到终点的概率高，所以这是我们走的路。因此，我们得到的答案应该是</p><pre class="lg lh li lj fq md me mf mg aw mh dt"><span id="aeb9" class="kf kg hu me b fv mi mj l mk ml">&lt;start&gt; dog dog &lt;end&gt;</span></pre><p id="ceeb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在Viterbi实现中，在我们填写概率表的整个过程中，还应该填写另一个被称为后指针表的表。backpointer表中每个单元格的值等于导致当前状态的最大概率的前一状态的行索引。因此，在我们的示例中，后指针表中的结束状态单元将具有值1 (0起始索引),因为行1处的状态dog是给予结束状态最高概率的前一状态。为了完整起见，下面给出了我们例子的后指针表。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nj"><img src="../Images/1d28aa23fdba76b6c4f0a9034b7c5225.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*NrG5fnQh0tvlE9_48xPXDQ.png"/></div></figure><p id="6aa1" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">请注意，起始状态的值为-1。这是我们在回溯反向指针表时使用的停止条件，以获得给定HMM时为我们提供正确概率最高的序列的路径。为了得到状态序列<start> dog dog <end>，我们从表格右下角的结束单元格开始。这个单元格中的1告诉我们woof列中的前一个状态在第1行，因此前一个状态必须是dog。从dog中，我们看到单元格再次标记为1，因此dog之前的喵列中的先前状态也是dog。最后，在meow列中，我们看到dog单元格被标记为0，因此前一个状态必须是第0行，也就是<start>状态。我们看到-1所以我们停在这里。我们的顺序是<end>狗狗<start>。逆转这一点给了我们最有可能的序列。<a class="ae ll" href="https://github.com/AaronCCWong/HMM-POS-Tagger-with-Viterbi-Decoding/blob/master/src/viterbi/Viterbi.java" rel="noopener ugc nofollow" target="_blank">查看示例实现</a>。</start></end></start></end></start></p></div><div class="ab cl mv mw hc mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hn ho hp hq hr"><p id="c19c" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">当使用一个算法时，知道算法的复杂度总是好的。在维特比的情况下，时间复杂度等于O(s * s * n ),其中s是状态的数量，n是输入序列中的字数。这是因为对于概率表中的每个s * n条目，我们需要查看前一列中的s条目。所需的空间复杂度为O(s * n)。这是因为有s行(每个状态一行)和n列(输入序列中的每个单词一列)。</p><h2 id="22f1" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">性能赋值</h2><p id="5a0c" class="pw-post-body-paragraph jg jh hu ji b jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz le kb kc kd hn dt translated">让我们看看当我们试图在WSJ语料库上训练HMM时会发生什么。我没有被允许分享文集，所以不能在这里给你指出一个，但如果你寻找它，应该不难找到…</p><p id="b883" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">训练HMM，然后使用Viterbi进行解码，在验证集上获得了71.66%的准确率。与此同时，当前基准分数为97.85%。</p><h2 id="8ba7" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">处理未知单词</h2><p id="8789" class="pw-post-body-paragraph jg jh hu ji b jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz le kb kc kd hn dt translated">我们如何缩小这一差距？我们已经知道，使用三元模型可以带来改进，但最大的改进来自于正确处理未知单词。我们使用Brants在论文<a class="ae ll" href="http://www.coli.uni-saarland.de/~thorsten/publications/Brants-ANLP00.pdf" rel="noopener ugc nofollow" target="_blank"> TnT中采用的方法——一种统计词性标注器</a>。更具体地说，我们执行后缀分析来尝试猜测未知单词的正确标签。</p><p id="e5cb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们创建了两个后缀树。一个后缀树跟踪小写单词的后缀，一个后缀树跟踪大写单词的后缀。(Brants，2000年)发现，对大写单词和小写单词使用不同的概率估计对性能有积极的影响。这是有道理的，因为大写的单词更有可能是首字母缩写词之类的东西。</p><p id="bc97" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们只使用出现在语料库中的频率小于某个指定阈值的单词的后缀。要使用的最大后缀长度也是一个可以调整的超参数。根据经验，<a class="ae ll" href="https://github.com/AaronCCWong/cs-2590/tree/master/viterbi" rel="noopener ugc nofollow" target="_blank">当使用最大后缀长度5和最大词频25时，这里的</a>标记器实现表现最佳，在验证集上给出95.79%的标记准确度。</p><p id="82a0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">为了计算给定单词后缀的标签的概率，我们遵循(Brants，2000)并使用</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nk"><img src="../Images/0f8b024c991d6bd8ec470f16e2ea4759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*9NJvqNkuNDO_PHUj4yUd7A.png"/></div></figure><p id="5ed4" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在哪里</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nl"><img src="../Images/26be7639168e5a7ae416ebef82d168c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*D5A36f1wn602eDkfk_MfAA.png"/></div></figure><p id="22b4" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">是使用最大似然估计来计算的，就像我们在前面的例子中所做的那样</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/b663be4d1d2dec80fcceb13f0b0e119a.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*l7tKmxJF-CVplJ57UaaDUg.png"/></div></figure><p id="6fe7" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在英语中，给定后缀的标签的概率等于给定后缀的所有后缀的最大似然估计的平滑和归一化和。</p><p id="e3c0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">因此，在计算维特比概率的过程中，如果我们遇到HMM以前没有见过的单词，我们可以使用未知单词的后缀来查询我们的后缀树。在给定未知单词后缀的情况下，我们使用后缀树来计算标签的发射概率，而不是使用HMM来计算单词标签的发射概率。</p><p id="3fa7" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">要查看后缀树的示例实现，<a class="ae ll" href="https://github.com/AaronCCWong/HMM-POS-Tagger-with-Viterbi-Decoding/blob/master/src/viterbi/SuffixTreeBuilder.java" rel="noopener ugc nofollow" target="_blank">查看这里的代码</a>。如前所述，这将验证集的准确率从71.66%提高到了95.79%。</p><h2 id="4377" class="kf kg hu bd kh ki kj kk kl km kn ko kp jr kq kr ks jv kt ku kv jz kw kx ky kz dt translated">示例实现</h2><p id="5219" class="pw-post-body-paragraph jg jh hu ji b jj la jl jm jn lb jp jq jr lc jt ju jv ld jx jy jz le kb kc kd hn dt translated"><a class="ae ll" href="https://www.aaronccwong.com/pos-tagger" rel="noopener ugc nofollow" target="_blank">点击此处试用一个HMM POS标记器，该标记器带有在WSJ语料库上训练的Viterbi解码</a>。</p><p id="b9eb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ll" href="https://github.com/AaronCCWong/HMM-POS-Tagger-with-Viterbi-Decoding" rel="noopener ugc nofollow" target="_blank">点击这里查看模型实现的代码</a>。</p><p id="c679" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ll" href="https://github.com/AaronCCWong/hmm-pos-tagger-service" rel="noopener ugc nofollow" target="_blank">点击此处查看托管POS tagger </a>的Spring Boot应用程序的代码。</p><p id="5e0b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><a class="ae ll" href="https://blog.aaronccwong.com/2019/building-a-bigram-hidden-markov-model-for-part-of-speech-tagging/" rel="noopener ugc nofollow" target="_blank">点击这里查看原帖。</a></p><figure class="lg lh li lj fq iv"><div class="bz el l di"><div class="nn no l"/></div></figure></div></div>    
</body>
</html>