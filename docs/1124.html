<html>
<head>
<title>(tutorial 3)What is seq2seq for text summarization and why</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">(教程3)什么是seq2seq用于文本摘要，为什么</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=collection_archive---------2-----------------------#2019-02-15">https://medium.com/hackernoon/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=collection_archive---------2-----------------------#2019-02-15</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/3fc3df5be0596e76fed4a97fc6094426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/1*ggAEvTp61wiHO7xTYbheCQ.gif"/></div></div></figure><p id="9378" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程是一系列教程中的第三个，将帮助您使用tensorflow构建一个抽象的文本摘要器，今天我们将讨论文本摘要任务的主要构建模块，从RNN开始，我们为什么使用它，而不仅仅是一个普通的神经网络，直到最后达到seq2seq模型</p><h2 id="cac9" class="ka kb hu bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku dt translated">关于系列</h2><p id="2676" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">这是一系列教程，将帮助您使用tensorflow使用多种方法构建一个抽象的文本摘要器，<strong class="je hv"> <em class="la">您不需要下载数据，也不需要在您的设备</em> </strong>上本地运行代码，因为<strong class="je hv">数据</strong>可以在<strong class="je hv"> google drive </strong>上找到，(您可以简单地将其复制到您的google drive，在此了解更多<a class="ae lb" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank"/>)，本系列的<strong class="je hv">代码</strong>是用Jupyter笔记本编写的，可以在<strong class="je hv">上运行</strong></p><p id="5b9b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae lb" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">0.<a class="ae lb" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a></p><ol class=""><li id="4346" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated"><a class="ae lb" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">概述文本摘要任务和用于该任务的不同技术</a></li><li id="0ee3" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated"><a class="ae lb" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">使用的数据以及如何表示我们的任务</a></li></ol><p id="1ec0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们开始吧</p></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="ab fr cl mb"><img src="../Images/16577fbdd2beff9f3d1dcb3f38417358.png" data-original-src="https://miro.medium.com/v2/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="8b81" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我在一个网站上添加了一个文本摘要模型<a class="ae lb" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>，这样你就可以自己尝试生成你自己的摘要(看看你能构建什么)，它可以通过简单的api调用来调用，并且通过<a class="ae lb" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python包</a>，这样文本摘要就可以很容易地集成到你的应用程序中，而不需要设置tensorflow环境的麻烦，你可以免费注册，并享受免费使用这个api的乐趣。</p></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><h1 id="694c" class="mg kb hu bd kc mh mi mj kg mk ml mm kk mn mo mp kn mq mr ms kq mt mu mv kt mw dt translated">快速回顾</h1><p id="adf6" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">我们的任务是文本摘要，我们称之为抽象，因为我们教神经网络生成单词，而不仅仅是复制单词。</p><p id="990e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">将被使用的数据将是新闻和它们的标题，可以在我的google drive上找到，所以你只需将它复制到你的google drive上，而不需要下载它</p><p id="2ee6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将使用单词嵌入来表示数据，这只是简单地将每个单词转换成一个特定的向量，我们将为我们的单词创建一个字典(<a class="ae lb" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)</p><p id="865a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">有<a class="ae lb" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">种不同的方法</a>来完成这项任务，它们建立在一个基石概念之上，它们不断发展和建立，它们从一个名为seq2seq的网络开始，然后它们组合成不同的网络来提高整体精度，这些不同方法的代码可以在<a class="ae lb" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里找到</a></p></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><p id="7926" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt mx translated">今天我们将讨论什么是seq2seq以及为什么首先使用它，所以让我们开始吧！！</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ng"><img src="../Images/27da833977deaa64b49c490ffb3ad1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0l4om4NAe6VzoQQybHmW4Q.jpeg"/></div></div></figure><blockquote class="nh ni nj"><p id="56f6" class="jc jd la je b jf jg jh ji jj jk jl jm nk jo jp jq nl js jt ju nm jw jx jy jz hn dt translated">这个教程是基于Andrew NG的惊人工作，他关于RNN的课程非常有用，我推荐你去看</p></blockquote></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><h1 id="9fca" class="mg kb hu bd kc mh mi mj kg mk ml mm kk mn mo mp kn mq mr ms kq mt mu mv kt mw dt translated">1-为什么我们使用复杂的网络结构而不是简单的神经网络</h1><p id="2326" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">这确实是一个要问的重要问题，在自然语言任务中，重要的是网络理解单词本身，而不是将单词链接到特定位置，这就是我们所说的(<strong class="je hv">在文本的不同部分共享特征</strong></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nn"><img src="../Images/a9dd3d22c05d9607d8b3dc8c85d8061b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/1*UCFuiWrLtUSyH9F9cW5jXA.gif"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">normal neural network inefficient for nlp</figcaption></figure><p id="3939" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">假设我们的任务是识别文本中的命名实体，正如我们在前面的gif中看到的，如果名字<strong class="je hv">哈利</strong>出现在文本的不同部分，普通的神经网络将无法识别它</p><p id="cf7c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以这就是为什么我们需要一个新的网络来完成这个任务，这个网络叫做(递归神经网络)RNN</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff no"><img src="../Images/b5e3763534dc4689d8f8d3759c328804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/1*XYyKYn71g8flAXegcYsA1A.gif"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">RNN for nlp</figcaption></figure><p id="ab3c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这里使用RNN，网络能够识别哈利这个名字，如果它出现在文本的不同部分。</p><p id="a901" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如我们所见，RNN是seq2seq的碱基</p><h1 id="8b21" class="mg kb hu bd kc mh np mj kg mk nq mm kk mn nr mp kn mq ns ms kq mt nt mv kt mw dt translated">2-什么是RNN(递归神经网络)</h1><p id="8e43" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">递归神经网络是一种神经网络，<strong class="je hv">将时间考虑在内</strong>，每个框(gif中的圆圈框)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/bd47da9a36e7db86e21e42418b975e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*xYkzes6ElpC9EHnbPpOrVg.png"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">RNN network</figcaption></figure><p id="147e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">实际上是我们的网络，我们多次使用它，每一次，都是一个时间步长，因为每一个时间步长我们都用我们句子中的一个词来填充它，它也接受前一个时间步长的输出，</p><p id="a94c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">概括一下，RNN是</p><ol class=""><li id="0c09" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated">考虑时间因素(及时运行多次)</li><li id="5cdc" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">获取上一步的输出</li></ol><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nv"><img src="../Images/fc8cf752ba40b2f73d26e9292654b6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/1*o2i1AQnWOyQDBAdI2ScYTQ.gif"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">RNN ex 1</figcaption></figure><p id="e3e3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如我们所见，它从前面的步骤中获取输入</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/bd58b4ef87fd75499ff03b409aecf574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*nVGNidH08D4qos_b91rbag.gif"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">RNN ex 2</figcaption></figure><p id="eab6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">它可以理解独立于位置的命名实体识别，这是我们需要的行为</p><h1 id="8bce" class="mg kb hu bd kc mh np mj kg mk nq mm kk mn nr mp kn mq ns ms kq mt nt mv kt mw dt translated">3- RNN前馈步骤</h1><p id="8ec9" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">像任何其他神经网络一样，我们需要一个前馈步骤</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/b71eea32af78972e656f2b53d91b4d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/1*h6uMPsXIBq8A1d23hw8z5A.gif"/></div></figure><p id="003b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们会有</p><ol class=""><li id="8966" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated">x向量(蓝色向量)(输入，是我们句子中的单词)</li><li id="3e10" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">y向量(绿色向量)(输出，将是从每个时间步导出的单词</li><li id="df2e" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">a矢量(红色矢量)(每个时间步的激活)</li></ol><p id="10b7" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">还有3种重量</p><ol class=""><li id="a2ea" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated">蜡向量(蓝色)(将乘以输入)，<strong class="je hv">对所有时间步长都一样</strong></li><li id="4065" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">Wya向量(绿色)，(将乘以输出)，<strong class="je hv">对于所有时间步长都相同</strong></li><li id="7fb3" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">魏如萱向量(红色)(将乘以激活)，<strong class="je hv">所有时间步长都相同</strong></li></ol><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ny"><img src="../Images/89b4035d6ff2e85de91ba2a4b948a0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEIC8EtQnJpDmITSc7AFhA.png"/></div></div></figure><p id="f6a8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">管理我们工作的两个主要职能是</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nz"><img src="../Images/2db5b40f4bb14fdca57b694ce5d80672.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*__8QSENV_CQlAZCk0RDCcg.jpeg"/></div></figure><p id="0099" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">其使用先前的激活参数和具有偏差的先前输入来计算下一个激活参数，这里我们使用通常为tanh或relu的激活函数g</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/0527d2d7e443d654139e897fd791f26d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*7aJ8pGzwkucwwWLqM3O6SA.jpeg"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">a</figcaption></figure><p id="f540" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">另一个函数用于计算每个时间步长的输出，这里我们使用激活参数和偏差，也使用g激活函数tanh或relu</p><p id="353c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后，我们需要计算用于反向传播的损耗</p><p id="0cf0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">主要使用的函数是(训练损失)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ob"><img src="../Images/e8c41831527c2aea64ef5ef79db29778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*1AylTsdF3kKqo_wFiKq7Cg.png"/></div></div></figure><p id="41df" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们使用<strong class="je hv">产生的输出</strong> yhat和<strong class="je hv">给定的输出</strong> y</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oc"><img src="../Images/e5d2f8c2cb786609bc2d15949e236577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/1*lbgxUtRzUYDl9wSWMpWQ2A.gif"/></div></figure><p id="dc84" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后我们简单地将它们相加，得到总损耗</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff od"><img src="../Images/305d1dd7123adf2209d3adc24198c2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*t2TVeX9G0wrRlBsZeuV4wg.png"/></div></figure><p id="4e30" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，在我们讨论了培训阶段之后，我们需要讨论如何运营我们的网络</p><h1 id="1f04" class="mg kb hu bd kc mh np mj kg mk nq mm kk mn nr mp kn mq ns ms kq mt nt mv kt mw dt translated">4- RNN跑步阶段</h1><p id="ca61" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">现在，在训练完我们的网络之后，我们需要运行它，这个阶段也被称为采样(<em class="la">这里我们将根据训练好的语言模型对随机单词进行采样，以说明rnn如何运行</em></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oe"><img src="../Images/75d80608ec4b344b4a986d19b5c80101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/1*BlZ-a3PGa8YYZDQiKFiObA.gif"/></div></figure><p id="c013" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">正如我们看到的，来自一个时间步骤的输入被转发到另一个时间步骤，直到我们到达最终输出，我们将需要一个tokken <eot>，文本结束，然后我们将停止我们的运行。</eot></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ny"><img src="../Images/c3949f5afcecfe1f61cd4d1b102a75ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyWCGMbdvnrPW-gnZskBlQ.png"/></div></div></figure><p id="61d4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这里，我们将计算这次运行的成本</p><p id="2e9c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">其主要功能是</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff of"><img src="../Images/de9e1e30f870cf35e86f1ff8525792a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*QBNwmf0CT0RIV3oNCQyShA.png"/></div></figure><p id="8a77" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将同时使用生成的输出和原始输出</p><p id="beef" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后我们简单地将它们相加，得到总输出</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff og"><img src="../Images/07ab7a597f1e3e98a8cbc53ef91c85c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/1*r8o6ZiJ4C9MTAjBdqKu5Sg.gif"/></div></figure><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oh"><img src="../Images/43a477ec9bf5cebb9d22e2321a6e8f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*ZgIntuGQAO8jtcB9T2E2xA.png"/></div></figure><p id="9d85" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在上面的所有内容中，我们只讨论了一种类型的RNN，即输入和输出长度相同的多对多架构，这不是我们的情况</p><p id="9954" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">至于文本摘要，我们需要拥有输入和输出不同长度的能力，为此我们最后会谈到<strong class="je hv"> Seq2Seq </strong></p><h1 id="a9da" class="mg kb hu bd kc mh np mj kg mk nq mm kk mn nr mp kn mq ns ms kq mt nt mv kt mw dt translated">5-我们终于到了Seq2Seq</h1><p id="994b" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">我们需要一个特殊的网络，它接收长度为(Tx)的输入，并产生另一个不同长度(Ty)的输出，这种结构称为编码器解码器。</p><p id="5c90" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里的两个编码器解码器都是RNN网络，但是编码器使用输入，并产生输出状态，该输出状态然后被用作解码器级的输入</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff oi"><img src="../Images/1d0972f4db0854e14d8bec4ac72fdf24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/1*bnRvZDDapHF8Gk8soACtCQ.gif"/></div></figure><p id="8310" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这种架构用于两种任务</p><ol class=""><li id="9abf" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated">机器翻译</li><li id="bde5" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">文本摘要</li></ol></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><h1 id="69a0" class="mg kb hu bd kc mh mi mj kg mk ml mm kk mn mo mp kn mq mr ms kq mt mu mv kt mw dt translated">概述</h1><p id="404c" class="pw-post-body-paragraph jc jd hu je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hn dt translated">今天我们讨论了</p><ol class=""><li id="64fe" class="lc ld hu je b jf jg jj jk jn le jr lf jv lg jz lh li lj lk dt translated">为什么我们使用RNN进行文本摘要而不是简单的神经网络，</li><li id="587d" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">什么是RNN(前馈，跑步)</li><li id="a8f2" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz lh li lj lk dt translated">然后，我们最终使用编码器解码器实现了seq2seq架构</li></ol></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><p id="d72e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但是，我们甚至可以有一个更好的文本摘要架构，我们可以对RNN进行修改以提高其效率，并解决它的一些问题，我们还可以添加注意机制，这证明对我们的任务非常有益，我们还可以使用波束搜索</p><p id="adf1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果上帝愿意，所有这些概念都将在接下来的教程中讨论。</p><blockquote class="nh ni nj"><p id="5c27" class="jc jd la je b jf jg jh ji jj jk jl jm nk jo jp jq nl js jt ju nm jw jx jy jz hn dt translated">我真诚地希望你喜欢阅读这个教程，我希望我已经把这些概念讲清楚了，这一系列教程的所有代码都可以在这里找到<a class="ae lb" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"/>，你可以简单地使用google colab来运行它，请查看教程并告诉我你对它的看法，希望再次见到你</p></blockquote><h1 id="3e11" class="mg kb hu bd kc mh np mj kg mk nq mm kk mn nr mp kn mq ns ms kq mt nt mv kt mw dt translated">后续教程</h1><ul class=""><li id="e4bd" class="lc ld hu je b jf kv jj kw jn oj jr ok jv ol jz om li lj lk dt translated"><a class="ae lb" href="http://bit.ly/eazysum_tu4" rel="noopener ugc nofollow" target="_blank">多层双向LSTM/GRU使文本摘要变得简单(教程4) </a></li><li id="1a9b" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz om li lj lk dt translated"><a class="ae lb" href="http://bit.ly/2G4XCo3" rel="noopener ugc nofollow" target="_blank">波束搜索&amp;注意让文本摘要变得简单(教程5) </a></li><li id="4b66" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz om li lj lk dt translated"><a class="ae lb" href="http://bit.ly/2ZeEmvO" rel="noopener ugc nofollow" target="_blank">在Tensorflow的94行中构建一个抽象的文本摘要器！！(教程6) </a></li><li id="7e8a" class="lc ld hu je b jf ll jj lm jn ln jr lo jv lp jz om li lj lk dt translated"><a class="ae lb" href="http://bit.ly/2EhcRIZ" rel="noopener ugc nofollow" target="_blank">用于文本摘要的抽象&amp;提取方法的组合(教程7) </a></li></ul></div></div>    
</body>
</html>