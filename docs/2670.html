<html>
<head>
<title>I See What You’re Saying: From Audio-only to Audio-visual Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我明白你的意思:从纯音频到视听语音识别</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/i-see-what-youre-saying-from-audio-only-to-audio-visual-speech-recognition-25ab6e0f0b4a?source=collection_archive---------23-----------------------#2019-04-25">https://medium.com/hackernoon/i-see-what-youre-saying-from-audio-only-to-audio-visual-speech-recognition-25ab6e0f0b4a?source=collection_archive---------23-----------------------#2019-04-25</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/bfb34b796c6632ffaf3ce196372eb028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cf7gwaKtWD58F-DCFbeweg.jpeg"/></div></div></figure><p id="62ff" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="ka">本文是</em><a class="ae kb" rel="noopener" href="/@alitech_2017/academic-alibaba-b56f4176a838"><strong class="je hv"><em class="ka">Academic Alibaba</em></strong></a><em class="ka">系列的一部分，摘自ICASSP的论文，题为“使用具有多条件训练和退出正则化的双峰DFSMN的鲁棒视听语音识别”，作者是张世良、雷明、马斌和谢磊。全文可以在这里阅读</em><a class="ae kb" href="https://www.researchgate.net/publication/332143510_ROBUST_AUDIO-VISUAL_SPEECH_RECOGNITION_USING_BIMODAL_DFSMN_WITH_MULTI-CONDITION_TRAINING_AND_DROPOUT_REGULARIZATION" rel="noopener ugc nofollow" target="_blank"><em class="ka"/></a><em class="ka">。</em></p><p id="1885" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">自动语音识别(ASR)是一个近年来取得巨大进展的领域——现在可用的大量语音操作智能手机助手就是这一事实的证明。然而，在嘈杂环境中理解语音的能力是机器仍然远远落后于人类的一个领域。</p><p id="71ea" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">为什么？首先，传统的纯音频语音识别模型没有视觉信息的优势来帮助破译所说的内容。(换句话说，与人类不同，它们不能读唇语。)这种识别促使研究人员探索视听语音识别(AVSR)的方法，但这仍然是一个相对较新的领域。一方面，由于缺乏公开可用的视听语料库来训练和测试新系统，另一方面，由于先进的神经网络模型的采用缓慢，进展受到了阻碍。</p><p id="ca04" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，随着近年来新AVSR语料库的出现，阿里巴巴科技团队与西北工业大学合作，提出了一种新的方法。</p><h1 id="34c6" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">睁大眼睛，竖起耳朵</h1><p id="9cc7" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">该团队方法的一个关键方面是采用纯音频语音识别领域的最佳实践，并将其应用于AVSR。</p><p id="ce4b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">虽然现有的ASVR模型使用相对简单的深度神经网络，但最新的纯音频模型使用更强大的神经网络，能够对语音信号中的长期依赖性进行建模。例子包括长短期记忆递归神经网络(LSTM-RNNs)、时间延迟神经网络和前馈顺序记忆网络(FSMNs)。</p><p id="3a58" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">该团队采用了一种称为深度FSMN (DFSMN)的FSMN变体，并复制了该架构来处理音频和视觉信息。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div class="fe ff lf"><img src="../Images/ca7c477657541f18c1dd73730adeaad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*8hKqM-_w1XSnSE0yqdmLIQ.jpeg"/></div><figcaption class="lk ll fg fe ff lm ln bd b be z ek">Bimodal DFSMN offers an optimal approach to integrating audio and visual information</figcaption></figure><p id="a675" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">称为双峰DFSMN的新模型通过音频网络和视觉网络独立地捕获音频和视觉信号的深度表示，然后将它们连接在一个联合网络中。通过这种方式，该模型实现了声音和视觉信息的最佳整合。</p><h1 id="f13e" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">提高噪音</h1><p id="ad83" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">该模型的进一步改进是引入了多条件训练；即在训练数据中使用各种各样的背景噪声。</p><p id="cc22" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">最新推出的ASVR NTCD-蒂米特语料库包含了56名爱尔兰人的视听记录，除了原始的“干净”记录外，它还包含了每位发言者的36个“嘈杂”版本。这些有噪声的版本是通过六种噪声类型(白噪声、牙牙学语、汽车、客厅、咖啡馆、街道)和六种信噪比(SNR)的组合产生的。为了产生多条件训练数据，该团队使用了来自30个嘈杂场景的150个小时的记录。</p><h1 id="dcd6" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">弥补盲点</h1><p id="e972" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">该模型相对于其前身改进的最后一个方面是在面对不完整的视觉数据时提供稳健的性能。</p><p id="d58a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">实际上，ASVR模型在视频的某些片段中很难捕捉到说话者的嘴部区域。为了解决这一问题，该团队在训练数据中加入了每帧的丢失，以模拟丢失视觉信息的影响，从而提高了模型的泛化能力。</p><h1 id="f113" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">真机唇读？(用于命令)等待下面发表的消息</h1><p id="0e9a" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">实验结果表明，与以前的模型相比，双峰DSFMN取得了显著的性能改善。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lo"><img src="../Images/c9f976d5c5cb17ad21dca7ac37aa59e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84t89Q56MFTfP6FFbDk_1A.png"/></div></div></figure><p id="6d10" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如上表所示，测试中的平均电话错误率(PER)在干净测试和多模态测试期间的所有测试模型中是最低的，甚至前身模型也受益于多模态训练的引入。独立测试证实，每帧丢失可提高高信噪比水平(10%及以上)的性能。</p><p id="5055" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但测试也证实，在最纯粹意义上的机器唇读成为可能之前，机器还有很长的路要走:所有模型在纯视频语音识别方面的表现都很差。该团队的结论是，希望该领域的进一步研究将集中在更强大的视觉前端处理和建模上，以提高该领域的性能。</p><figure class="lg lh li lj fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff lp"><img src="../Images/ba13c8acc14074f1c6b1761a6ac5c9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAnW5LbhzRZ7S0MAZ5QUZQ.png"/></div></div></figure><p id="a36a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="ka">全文可在此阅读</em><a class="ae kb" href="https://www.researchgate.net/publication/332143510_ROBUST_AUDIO-VISUAL_SPEECH_RECOGNITION_USING_BIMODAL_DFSMN_WITH_MULTI-CONDITION_TRAINING_AND_DROPOUT_REGULARIZATION" rel="noopener ugc nofollow" target="_blank"><em class="ka"/></a><em class="ka">。</em></p></div><div class="ab cl lq lr hc ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hn ho hp hq hr"><h1 id="c375" class="kc kd hu bd ke kf lx kh ki kj ly kl km kn lz kp kq kr ma kt ku kv mb kx ky kz dt translated">阿里巴巴科技</h1><p id="01dc" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">关于阿里巴巴最新技术的第一手深度资料→脸书:<a class="ae kb" href="http://www.facebook.com/AlibabaTechnology" rel="noopener ugc nofollow" target="_blank"> <strong class="je hv">【阿里巴巴科技】</strong> </a>。Twitter:<a class="ae kb" href="https://twitter.com/AliTech2017" rel="noopener ugc nofollow" target="_blank"><strong class="je hv">【AlibabaTech】</strong></a>。</p></div></div>    
</body>
</html>