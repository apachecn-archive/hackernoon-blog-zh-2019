<html>
<head>
<title>Full Article On svm From classification to kernel selection to outlier detection with code in R and python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于 svm 的整篇文章，从分类到核选择再到离群点检测，用 R 和 python 编写代码</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/full-article-on-svm-from-classification-to-kernel-selection-to-outlier-detection-with-code-in-r-7f069fcaf820#2019-05-29">https://medium.com/hackernoon/full-article-on-svm-from-classification-to-kernel-selection-to-outlier-detection-with-code-in-r-7f069fcaf820#2019-05-29</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/eca24a89c77ade08cfabb53a77280caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EQR47stD2O5bODRs.png"/></div></div></figure><h1 id="8647" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">基本信息:</h1><p id="6acd" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated"><strong class="kc hv">支持向量机</strong>是一套监督学习方法，用于<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-classification" rel="noopener ugc nofollow" target="_blank">分类</a>、<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-regression" rel="noopener ugc nofollow" target="_blank">回归</a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection" rel="noopener ugc nofollow" target="_blank">离群点检测</a>。</p><p id="0267" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">支持向量机的优势在于:</p><ul class=""><li id="d427" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx lj lk ll lm dt translated">在高维空间有效。</li><li id="528d" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated">在维数大于样本数的情况下仍然有效。</li><li id="5f23" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated">在决策函数中使用训练点的子集(称为支持向量)，因此它也是内存高效的。</li><li id="277d" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated">通用:可以为决策函数指定不同的<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" rel="noopener ugc nofollow" target="_blank">内核函数</a>。提供了通用内核，但是也可以指定定制内核。</li></ul><p id="f94d" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">支持向量机的缺点包括:</p><ul class=""><li id="b6a3" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx lj lk ll lm dt translated">如果特征的数量远大于样本的数量，在选择<a class="ae ky" href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels" rel="noopener ugc nofollow" target="_blank">核函数</a>时避免过度拟合，正则项至关重要。</li><li id="5b03" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated">支持向量机不直接提供概率估计，而是使用昂贵的五重交叉验证来计算。</li></ul><p id="b660" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">sci kit-learn(ML 的 python 包)中的支持向量机既支持密集(<code class="eh ls lt lu lv b">numpy.ndarray</code>并可通过<code class="eh ls lt lu lv b">numpy.asarray</code>转换)样本向量，也支持稀疏(【任何】T2)样本向量作为输入。但是，要使用 SVM 对稀疏数据进行预测，它必须适合此类数据。为了获得最佳性能，使用 C 排序的<code class="eh ls lt lu lv b">numpy.ndarray</code>(密集)或<code class="eh ls lt lu lv b">scipy.sparse.csr_matrix</code>(稀疏)和<code class="eh ls lt lu lv b">dtype=float64</code>。</p><h2 id="a768" class="lw jd hu bd je lx ly lz ji ma mb mc jm kl md me jq kp mf mg ju kt mh mi jy mj dt translated">什么是支持向量机？</h2><p id="6aa1" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">支持向量机算法用于在 N 维空间(N-特征的数量)中寻找超平面，该超平面清楚地分类数据点。</p><figure class="mk ml mm mn fq iv"><div class="bz el l di"><div class="mo mp l"/></div><figcaption class="mq mr fg fe ff ms mt bd b be z ek">watch this video by caltech’s Professor Yaser Abu-Mostafa to understand the math behind it</figcaption></figure><p id="848b" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">支持向量机(SVM)是一种由分离超平面形式定义的判别分类器。换句话说，给定标记的训练数据(监督学习)，算法输出一个分类新例子的最佳超平面。</p><p id="68c6" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">SVM 模型是将示例表示为空间中的点，通过映射，各个类别的示例被尽可能宽的间隙分开。除了执行线性分类，支持向量机还可以有效地执行非线性分类，将它们的输入隐式地映射到高维特征空间。现在你想知道一个线性专家是如何解决非线性问题的，让我用一个简单的例子来说明，这样你就能明白我懂一些数学。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/c34cc5c0892e537dd84335fec2365709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hNaohfuBLGPrXzSI"/></div></div></figure><p id="de90" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">这是一个典型的 x，y 坐标图，在一个 x -y 平面上，但是如果你把这个平面换成另一个平面，这个图就会变成这样</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mv"><img src="../Images/a0026e16ae7e7797c7aa1d2a6914f06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S5y0XJIOWKePaNdv"/></div></div></figure><p id="a007" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">给定一组训练样本，每个样本被标记为属于两个类别中的一个或另一个，SVM 训练算法建立一个模型，将新样本分配给一个类别或另一个类别，使其成为非概率二元线性分类器。</p><h1 id="29f6" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">超平面:</h1><p id="933e" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">超平面是帮助分类数据点的决策边界。落在超平面任一侧的数据点可以归属于不同的类别。此外，超平面的维数取决于特征的数量。如果输入特征的数量是 2，那么超平面只是一条线。如果输入特征的数量是 3，则超平面变成二维平面。当特征的数量超过 3 时，变得难以想象。</p><h1 id="6621" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">Svm 分类器如何工作？</h1><p id="1d9a" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">对于由特征集和标签集组成的数据集，SVM 分类器建立模型来预测新示例的类。它将新的示例/数据点分配给其中一个类。如果只有两个类，那么它可以被称为二进制 SVM 分类器。</p><p id="00b8" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">有两种 SVM 分类器:</p><ol class=""><li id="7c97" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx mw lk ll lm dt translated"><strong class="kc hv">线性 SVM 分类器</strong></li><li id="4e22" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx mw lk ll lm dt translated"><strong class="kc hv">非线性 SVM 分类器</strong></li></ol><h2 id="cb34" class="lw jd hu bd je lx ly lz ji ma mb mc jm kl md me jq kp mf mg ju kt mh mi jy mj dt translated">Svm 线性分类器:</h2><p id="fe37" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">在线性分类器模型中，我们假设训练样本在空间中绘制。预计这些数据点之间会有明显的差距。它预测了划分两个类的直超平面。绘制超平面时的主要焦点是最大化从超平面到任一类的最近数据点的距离。画出的超平面称为最大边缘超平面。</p><div class="mk ml mm mn fq ab cb"><figure class="mx iv my mz na nb nc paragraph-image"><img src="../Images/b229db3fe72ca6907b5bbc34d16ee3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/0*ZwAIUw0oRplPknDZ"/></figure><figure class="mx iv nd mz na nb nc paragraph-image"><img src="../Images/634ed16d88b0d33ec41abbdfc4eecaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/0*XNMDMNwXDx4JUytS"/></figure></div><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/f637913c9a9fbe54040f2859cb398d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uQ2pf3VNNETy8PhM"/></div></div></figure><h2 id="293d" class="lw jd hu bd je lx ly lz ji ma mb mc jm kl md me jq kp mf mg ju kt mh mi jy mj dt translated">SVM 非线性分类器；</h2><p id="56d7" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">在现实世界中，我们的数据集通常在一定程度上是分散的。为了解决这个问题，基于直的线性超平面将数据分成不同的类被认为不是一个好的选择。为此，Vapnik 建议通过对最大间隔超平面应用核技巧来创建非线性分类器。在非线性 SVM 分类中，在更高维度空间中绘制的数据点。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff ne"><img src="../Images/047ab52745eba33ccda90f9dfee90fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*RjxZu60aI55dlTF0"/></div></figure><h1 id="4e64" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">线性支持向量机分类器</h1><p id="c327" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">在线性分类器中，一个数据点被认为是一个 p 维向量(p 数列表),我们使用(p-1)维超平面来分隔点。可以有许多超平面以线性顺序分隔数据，但是最佳超平面被认为是最大化裕度的超平面，裕度即超平面和任一类的最近数据点之间的距离。</p><p id="b4c9" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">最大边际超平面由最接近它的数据点决定。因为我们必须最大化超平面和数据点之间的距离。这些影响我们超平面的数据点被称为支持向量。</p><h1 id="ccf8" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">非线性支持向量机分类器</h1><p id="bd76" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">经常发生的情况是，我们的数据点在 p 维(有限)空间中不是线性可分的。为了解决这个问题，有人提出将 p 维空间映射到一个更高维的空间。我们可以使用核技巧来绘制定制的/非线性的超平面。每个核都有一个非线性核函数。</p><p id="42fb" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">该函数有助于建立高维特征空间。已经开发了很多内核。一些标准内核是:</p><ol class=""><li id="41a4" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx mw lk ll lm dt translated"><strong class="kc hv">多项式(齐次)核:</strong>多项式核函数可以用上面的表达式来表示。</li></ol><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff nf"><img src="../Images/44d1d4922c8818689d50f51bd3ee8f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*DnGUVNwr6AhupL1r27R3UA.png"/></div></figure><p id="f38b" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">其中 k(xi，xj)是核函数，xi 和 xj 是特征空间的向量，d 是多项式函数的次数。</p><p id="2b6f" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><strong class="kc hv"> 2。多项式(非齐次)核:</strong> <br/>在非齐次核中，还增加了一个常数项。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff ng"><img src="../Images/91d267fc2bc6bf8e7a5502bcb954e0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*z6gvQg4vOOqNLYI-QEiXlQ.png"/></div></figure><p id="d467" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">常数项“c”也称为自由参数。它影响特征的组合。x 和 y 是特征空间的向量。</p><p id="fe4e" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><strong class="kc hv">径向基函数核:</strong> <br/>又称为 RBF 核。它是最受欢迎的内核之一。对于距离度量平方，这里使用欧几里得距离。它用于绘制完全非线性的超平面。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff nh"><img src="../Images/8335aad459d0e048b01a1da5e626f79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*iNl3TU-Xl8uBNrgjckUA9Q.png"/></div></figure><p id="4a42" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">其中 x &amp; x’是特征空间的向量。是一个自由参数。参数的选择是一个关键的选择。使用参数的典型值会导致数据过度拟合。</p><h1 id="2190" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">支持向量机库/包:</h1><p id="9055" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">为了在数据集上实现支持向量机，我们可以使用库。有许多可用的库或包可以帮助我们顺利实现 SVM。我们只需要根据需要调用带参数的函数。</p><p id="66a5" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">在 Python 中，我们可以使用 sklearn 这样的库。对于分类，Sklearn 提供了类似 SVC、NuSVC &amp; LinearSVC 这样的函数。</p><p id="1ecf" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><strong class="kc hv"> SVC() </strong>和<strong class="kc hv"> NuSVC() </strong>方法几乎相似，只是在参数上有所不同。我们传递核心参数、gamma 和 C 参数等的值。默认情况下，内核参数使用“rbf”作为它的值，但我们可以传递像“poly”，“linear”，“sigmoid”或可调用函数的值。</p><p id="716a" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><strong class="kc hv"> LinearSVC() </strong>是一个只使用线性核的分类 SVC。在<strong class="kc hv"> LinearSVC() </strong>中，我们不传递内核的值，因为它专门用于线性分类。</p><p id="8c16" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">在 R 编程语言中，我们可以使用“e1071”或“caret”这样的包。要使用一个包，我们需要先安装它。要安装“e1071”，我们可以在控制台中键入 install . packages(“e 1071”)。<br/> e1071 提供了一个<strong class="kc hv"> SVM() </strong>方法，它既可以用于回归，也可以用于分类。<strong class="kc hv"> SVM() </strong>方法接受数据、伽玛值和内核等。</p><h2 id="560d" class="lw jd hu bd je lx ly lz ji ma mb mc jm kl md me jq kp mf mg ju kt mh mi jy mj dt translated">SVM 的成本函数和梯度:</h2><p id="0072" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">我们希望最大化数据点和超平面之间的差距。有助于最大化裕量的损失函数是铰链损失。如果预测值和实际值符号相同，则成本为 0。如果不是，我们就计算损失值。我们还在代价函数中加入了一个正则化参数。正则化参数的目标是平衡余量最大化和损失。添加正则化参数后，成本函数如下所示。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ni"><img src="../Images/af3a00c74a888e9114ecdecf0f0c88dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ifAxLkl9xmrahyUs"/></div></div></figure><p id="8810" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">现在我们有了损失函数，我们对权重求偏导数来求梯度。使用梯度，我们可以更新我们的权重。</p><p id="de5e" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">现在让我们编写代码来实现我们的学习:</p><p id="f4cd" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><strong class="kc hv"> PYTHON: </strong></p><p id="aed1" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">让我们从婴儿的例子鸢尾开始，你知道，如果你遵循决策树，你会有一个很好的想法，如何用层次格式分类不同的物种，只有当你必须分类时，才会出现问题，因为我们有神经网络，但再次让我们看看花瓣长度和萼片长度的分布，宽度得到激励，然后我们将使用不同的核，如上所述，以获得 svm 分类。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nj"><img src="../Images/9fd566c3e3b2e624152e7b6edf669261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w6TaRnr863J4f4Ks"/></div></div></figure><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nk"><img src="../Images/9f0723032cbaa88611c15954c8a4cafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q2u_wnwHXVTGgc52"/></div></div></figure><p id="0395" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">最后使用支持向量机分类器</p><pre class="mk ml mm mn fq nl lv nm nn aw no dt"><span id="bc88" class="lw jd hu lv b fv np nq l nr ns">X = iris_dataset.data[:,:]  <br/>y = iris_dataset.target<br/>C = 1.0  <em class="nt"># SVM regularization parameter</em><br/> <br/><em class="nt"># SVC with linear kernel</em><br/>svc = svm.SVC(kernel='linear', C=C).fit(X, y)<br/><em class="nt"># LinearSVC (linear kernel)</em><br/>lin_svc = svm.LinearSVC(C=C).fit(X, y)<br/><em class="nt"># SVC with RBF kernel</em><br/>rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)<br/><em class="nt"># SVC with polynomial (degree 3) kernel</em><br/>poly_svc3 = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)</span><span id="5c69" class="lw jd hu lv b fv nu nq l nr ns"><em class="nt"># SVC with polynomial (degree 4) kernel</em><br/>poly_svc4 = svm.SVC(kernel='poly', degree=4, C=C).fit(X, y)</span></pre><p id="f014" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">现在我们将处理来自 cs.toronto 的图像分类问题 CIFAR-10<a class="ae ky" href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">这里</a>数据集看起来如何</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nv"><img src="../Images/1fbeb579556e301af3056d309f78433e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YjF9bZrBMWtXLtUX"/></div></div></figure><p id="3f42" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">在终端中运行 get_datasets.sh 下载数据集，或者从<a class="ae ky" href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> Alex Krizhevsky </a>下载。</p><p id="9a34" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">get_datasets.sh</p><p id="28c7" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><code class="eh ls lt lu lv b"># Get CIFAR10</code></p><p id="0a01" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><code class="eh ls lt lu lv b">wget <a class="ae ky" href="http://www.cs.toronto.edu/" rel="noopener ugc nofollow" target="_blank">http://www.cs.toronto.edu/</a>~kriz/cifar-10-python.tar.gz</code></p><p id="a89c" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><code class="eh ls lt lu lv b">tar -xzvf cifar-10-python.tar.gz</code></p><p id="2897" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated"><code class="eh ls lt lu lv b">rm cifar-10-python.tar.gz</code></p><p id="8caf" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">下载的结果如下图所示。</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/3aa073079d8836134d04ba67cdf65b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/0*RwrXzkjlSLTftron.png"/></div></figure><p id="c84a" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">编写完加载数据的代码后，您可以编写自定义可视化代码</p><pre class="mk ml mm mn fq nl lv nm nn aw no dt"><span id="b197" class="lw jd hu lv b fv np nq l nr ns"><strong class="lv hv">def</strong> visualize_sample(X_train, y_train, classes, samples_per_class=7):<br/>    <em class="nt">"""visualize some samples in the training datasets """</em><br/>    num_classes = len(classes)<br/>    <strong class="lv hv">for</strong> y, cls <strong class="lv hv">in</strong> enumerate(classes):<br/>        idxs = np.flatnonzero(y_train == y) <em class="nt"># get all the indexes of cls</em><br/>        idxs = np.random.choice(idxs, samples_per_class, replace=False)<br/>        <strong class="lv hv">for</strong> i, idx <strong class="lv hv">in</strong> enumerate(idxs): <em class="nt"># plot the image one by one</em><br/>            plt_idx = i * num_classes + y + 1 <em class="nt"># i*num_classes and y+1 determine the row and column respectively</em><br/>            plt.subplot(samples_per_class, num_classes, plt_idx)<br/>            plt.imshow(X_train[idx].astype('uint8'))<br/>            plt.axis('off')<br/>            <strong class="lv hv">if</strong> i == 0:<br/>                plt.title(cls)<br/>    plt.show()</span></pre><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div class="fe ff nx"><img src="../Images/e566be67e6b387fff2b675ab9e940bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/0*IEf_E9GdU8Tgjdje"/></div></figure><p id="ed05" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">然后你可以写分类器函数</p><pre class="mk ml mm mn fq nl lv nm nn aw no dt"><span id="b52b" class="lw jd hu lv b fv np nq l nr ns"><em class="nt"># Test the loss and gradient</em><br/><strong class="lv hv">from</strong> <strong class="lv hv">algorithms.classifiers</strong> <strong class="lv hv">import</strong> loss_grad_svm_vectorized<br/><strong class="lv hv">import</strong> <strong class="lv hv">time</strong></span><span id="5e20" class="lw jd hu lv b fv nu nq l nr ns"><em class="nt"># generate a rand weights W </em><br/>W = np.random.randn(10, X_train.shape[0]) * 0.001</span><span id="1f6d" class="lw jd hu lv b fv nu nq l nr ns">tic = time.time()<br/>loss_vec, grad_vect = loss_grad_svm_vectorized(W, X_train, y_train, 0)<br/>toc = time.time()<br/><strong class="lv hv">print</strong> 'Vectorized loss: <strong class="lv hv">%f</strong>, and gradient: computed in <strong class="lv hv">%f</strong>s' % (loss_vec, toc - tic)</span></pre><p id="f8c7" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">计算梯度损失后，您可以在测试数据集上检查性能</p><pre class="mk ml mm mn fq nl lv nm nn aw no dt"><span id="ea0a" class="lw jd hu lv b fv np nq l nr ns">y_test_predict_result = best_svm.predict(X_test)<br/>y_test_predict = y_test_predict_result[0]<br/>test_accuracy = np.mean(y_test == y_test_predict)<br/><strong class="lv hv">print</strong> 'The test accuracy is: <strong class="lv hv">%f</strong>' % test_accuracy</span></pre><h1 id="7791" class="jc jd hu bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz dt translated">稀有</h1><p id="5943" class="pw-post-body-paragraph ka kb hu kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx hn dt translated">要使用 svm，我们需要来自 R</p><p id="d35a" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">让我们从使用泰坦尼克号数据集开始，如果你不熟悉泰坦尼克号数据集，你可以在这里看到<a class="ae ky" href="https://www.kaggle.com/c/titanic/rules" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="ba5a" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">我们将只采用年龄和费用栏来预测存活率</p><p id="1b9b" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">train_f </p><p id="6499" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">we will use linear kernel for prediction</p><p id="6b2c" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">classifier = svm(formula = Survived ~ ., <br/> data = train_c，<br/> type = 'C-classification '，<br/> kernel = 'linear') <br/>分类器调用会给我们关于调用的细节</p><blockquote class="ny nz oa"><p id="7350" class="ka kb nt kc b kd kz kf kg kh la kj kk ob lb kn ko oc lc kr ks od ld kv kw kx hn dt translated">分类者</p></blockquote><p id="1718" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">调用:<br/> svm(公式=幸存~。，data = train_c，type = "C-classification "，kernel = "linear ")</p><p id="fa68" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">参数:<br/> SVM 类型:C 分类<br/> SVM 内核:线性<br/>代价:1 <br/>伽玛:0.5</p><p id="2be5" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">支持向量数量:517</p><p id="a534" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">支持向量边界看起来像这样</p><figure class="mk ml mm mn fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff oe"><img src="../Images/327ee3475286b9e11accc73f659b2959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MbIvDDipClgR5RWF"/></div></div></figure><p id="f3c7" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">你现在能做什么</p><ol class=""><li id="6421" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx mw lk ll lm dt translated">下载库 git 克隆<a class="ae ky" href="https://github.com/MachineLearningWithHuman/R" rel="noopener ugc nofollow" target="_blank">https://github.com/MachineLearningWithHuman/R</a>和 git 克隆<a class="ae ky" href="https://github.com/MachineLearningWithHuman/python" rel="noopener ugc nofollow" target="_blank">https://github.com/MachineLearningWithHuman/python</a></li><li id="eb21" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx mw lk ll lm dt translated">去各自的 svm 文件夹玩数据集，看看你的 SVM 核点和边界如何变化。</li></ol><p id="8cd4" class="pw-post-body-paragraph ka kb hu kc b kd kz kf kg kh la kj kk kl lb kn ko kp lc kr ks kt ld kv kw kx hn dt translated">参考:</p><ul class=""><li id="ae8a" class="le lf hu kc b kd kz kh la kl lg kp lh kt li kx lj lk ll lm dt translated"><a class="ae ky" href="https://www.datacamp.com/community/tutorials/support-vector-machines-r" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/support-vector-machines-r</a></li><li id="91c0" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated"><a class="ae ky" href="http://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/</a></li><li id="d64b" class="le lf hu kc b kd ln kh lo kl lp kp lq kt lr kx lj lk ll lm dt translated"><a class="ae ky" href="https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-python/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/classified-data-using-support-vector-machines SVMs-in-python/</a></li></ul></div></div>    
</body>
</html>