<html>
<head>
<title>Stepping into NLP — Word2Vec with Gensim</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Gensim步入NLP — Word2Vec</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/stepping-into-nlp-word2vec-with-gensim-e7c54d9a450a?source=collection_archive---------4-----------------------#2019-02-27">https://medium.com/hackernoon/stepping-into-nlp-word2vec-with-gensim-e7c54d9a450a?source=collection_archive---------4-----------------------#2019-02-27</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="2e55" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">word2vec嵌入和用例介绍</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/ea429112c985ba71a652bb3b6b4cc0ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2Fp2VASiqU8bEhLt"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Photo by <a class="ae jz" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dmitry Ratushny</a> on <a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d0f6" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">自然语言处理(NLP)是计算机科学和人工智能的一个领域，也就是现在与计算机和自然语言中的人类互动有关的T2。目标是让系统像我们一样完全理解各种语言。它是自然语言处理产品/技术的驱动力，如虚拟助手、语音识别、机器翻译、情感分析、自动文本摘要等等。在这篇文章中，我们将使用Gensim框架研究一种叫做Word2Vec的单词嵌入技术。</p><h2 id="d911" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">单词嵌入…什么！！</h2><p id="02e4" class="pw-post-body-paragraph ka kb hu kc b kd ls iv kf kg lt iy ki kj lu kl km kn lv kp kq kr lw kt ku kv hn dt translated">单词嵌入是一种NLP技术，能够捕捉文档中单词的上下文、语义和句法相似性、与其他单词的关系等。一般来说，它们是特定单词的矢量表示。已经说过，接下来是创建单词嵌入的技术。有许多技术可以创建单词嵌入。一些受欢迎的是:</p><ol class=""><li id="8076" class="lx ly hu kc b kd ke kg kh kj lz kn ma kr mb kv mc md me mf dt translated">二进制编码。</li><li id="8489" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mc md me mf dt translated">TF编码。</li><li id="9a5f" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mc md me mf dt translated">TF-IDF编码。</li><li id="fd83" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mc md me mf dt translated">潜在语义分析编码。</li><li id="bc98" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mc md me mf dt translated">Word2Vec嵌入。</li></ol><p id="e0ff" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们将在以后的文章中讨论不同的嵌入技术，现在，我们将坚持使用Word2Vec嵌入。</p><h2 id="6453" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">Word2Vec嵌入简介</h2><p id="20a2" class="pw-post-body-paragraph ka kb hu kc b kd ls iv kf kg lt iy ki kj lu kl km kn lv kp kq kr lw kt ku kv hn dt translated"><em class="kw"> Word2vec </em>是产生<a class="ae jz" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>最广泛使用的模型之一。这些模型是浅层的两层神经网络，被训练来重建单词的语言环境。Word2Vec可以通过两种方式实现，一种是Skip Gram，另一种是Common Bag Of Words (CBOW)</p><h2 id="44f4" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">连续单词包(CBOW)</h2><p id="97ad" class="pw-post-body-paragraph ka kb hu kc b kd ls iv kf kg lt iy ki kj lu kl km kn lv kp kq kr lw kt ku kv hn dt translated">CBOW正在学习根据上下文预测单词。这里输入将是上下文<strong class="kc hv"><em class="kw">#邻近单词</em> </strong>，输出将是目标单词。每个上下文中的字数限制由一个名为“<strong class="kc hv">窗口大小</strong>的参数决定。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ml"><img src="../Images/a742794c0c297e379e28d9c49dd2274d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ugorDZ6nOgSqQq1dirY8Q.png"/></div></div></figure><p id="4aae" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">例子:敏捷的棕色狐狸跳过懒惰的狗<strong class="kc hv"><em class="kw">#是的同样的例子:-) </em> </strong></p><p id="06c7" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">型号:CBOW</p><p id="75c4" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">输入层:白盒内容</p><p id="a5fa" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">目标层:蓝色框字</p><p id="c7a7" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">窗户尺寸:5</p><h2 id="969f" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">跳过克</h2><p id="7cd7" class="pw-post-body-paragraph ka kb hu kc b kd ls iv kf kg lt iy ki kj lu kl km kn lv kp kq kr lw kt ku kv hn dt translated"><strong class="kc hv"> Skip Gram </strong>正在学习通过单词预测上下文。这里输入将是单词，输出将是目标上下文<strong class="kc hv"><em class="kw">#邻近单词</em> </strong>。每个上下文中的字数限制由一个名为“<strong class="kc hv">窗口大小</strong>的参数决定。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ml"><img src="../Images/a742794c0c297e379e28d9c49dd2274d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ugorDZ6nOgSqQq1dirY8Q.png"/></div></div></figure><p id="16d7" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">例子:敏捷的棕色狐狸跳过懒惰的狗<strong class="kc hv"><em class="kw">#是的同样的例子:-) </em> </strong></p><p id="8c42" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">型号:跳过克</p><p id="06e3" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">输入层:蓝色框字</p><p id="57a1" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">目标层:白盒内容窗口大小:5</p><h2 id="cbce" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">在幕后</h2><ul class=""><li id="22a4" class="lx ly hu kc b kd ls kg lt kj mm kn mn kr mo kv mp md me mf dt translated">如上所述，我们将使用两层<a class="ae jz" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>。对于这个模型，后面的输入将是关于目标单词的上下文，后面是隐藏层，该隐藏层最后构建目标层与目标单词的关系。</li><li id="e1b1" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mp md me mf dt translated">在训练模型之后，语料库中的每个单词将具有其自己的关于上下文和含义的向量嵌入。</li><li id="d38f" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mp md me mf dt translated">现在我们可以使用<em class="kw"> Matplotlib </em> <strong class="kc hv"> </strong>来映射单词嵌入，这可能会给我们一个如何建立关系和分配向量的清晰图像。</li></ul><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mq"><img src="../Images/9a8665b906c23d72c1a4beba39392b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*zKPGYZJ42yZ65pT3stDhRw.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Word2Vec Embeddings</figcaption></figure><pre class="jk jl jm jn fq mr ms mt mu aw mv dt"><span id="8c29" class="kx ky hu ms b fv mw mx l my mz">INPUT CORPUS</span><span id="7ae9" class="kx ky hu ms b fv na mx l my mz">1. this is the first sentence for word2vec</span><span id="48d2" class="kx ky hu ms b fv na mx l my mz">2. this is the second sentence</span><span id="fc6f" class="kx ky hu ms b fv na mx l my mz">3. yet another sentence</span><span id="ba87" class="kx ky hu ms b fv na mx l my mz">4. one more sentence<br/>                             5. and the final sentence</span></pre><ul class=""><li id="93c2" class="lx ly hu kc b kd ke kg kh kj lz kn ma kr mb kv mp md me mf dt translated">正如我们可以看到的，相似的单词基于它们的上下文被映射到附近，如“第一个”&amp;“第二个”，“一个”&amp;“另一个”，单词“句子”从簇中分离出来，因为它与任何其他单词都不相似。</li><li id="545f" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mp md me mf dt translated">从这里，我们可以使用这些嵌入来拥有相似的单词、句子或具有相同内容的文档，这样的例子不胜枚举…</li></ul><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/3a92bb7cbfc624db6351ffc5c6868337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*l9hW3ytPI3dmI3R0_NpLfg.jpeg"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Literally everywhere!!</figcaption></figure><p id="b7c2" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">事情说明到此，相信<strong class="kc hv"> <em class="kw">各位</em> </strong>已经对word2vec有所了解了。如果没有，也不用担心！看完下面的例子，你就能有一个清晰的概念了。让我们深入了解一些python🐍。</p><p id="2941" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated"><strong class="kc hv">让我们添加一些Python </strong></p><p id="2547" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">正如我们之前讨论的，我们将使用python中的Gensim框架实现word2vec。<strong class="kc hv"> Gensim </strong>是一个健壮的<a class="ae jz" href="https://en.wikipedia.org/wiki/Open-source_software" rel="noopener ugc nofollow" target="_blank">开源</a>向量空间建模和<a class="ae jz" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank">主题建模</a>工具包，在<a class="ae jz" href="https://en.wikipedia.org/wiki/Python_(programming_language)" rel="noopener ugc nofollow" target="_blank"> Python </a>中实现。它使用<a class="ae jz" href="https://en.wikipedia.org/wiki/NumPy" rel="noopener ugc nofollow" target="_blank"> NumPy </a>、<a class="ae jz" href="https://en.wikipedia.org/wiki/SciPy" rel="noopener ugc nofollow" target="_blank"> SciPy </a>和可选的<a class="ae jz" href="https://en.wikipedia.org/wiki/Cython" rel="noopener ugc nofollow" target="_blank"> Cython </a>来提高性能。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc nd l"/></div></figure><p id="6ff3" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">这里我们已经导入了需求，接下来我们将定义文本语料库。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc nd l"/></div></figure><p id="35c5" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在这一步，我们定义了我们的<strong class="kc hv"> Word2vec模型</strong>。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc nd l"/></div></figure><p id="56f4" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">这里，</p><ul class=""><li id="e96d" class="lx ly hu kc b kd ke kg kh kj lz kn ma kr mb kv mp md me mf dt translated"><code class="eh ne nf ng ms b">size</code>向量的维数越高，嵌入的尺寸越密(理想情况下<code class="eh ne nf ng ms b">size</code>必须低于vocab长度。使用比词汇量更高的维度或多或少会保证“过度拟合”。)</li><li id="ee8b" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mp md me mf dt translated">低于<code class="eh ne nf ng ms b">min_count</code>频率的单词在训练发生之前被丢弃(因为我们只有几行输入语料库，所以我们取每个单词)。</li><li id="23a6" class="lx ly hu kc b kd mg kg mh kj mi kn mj kr mk kv mp md me mf dt translated">语料库被添加到vocab用于训练，并且训练完成。</li></ul><p id="cf65" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">PCA模型用于将我们的vocab中每个单词的<strong class="kc hv"> n </strong>维向量减少到2d向量(我们这样做是为了绘制/可视化我们的结果)。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc nd l"/></div></figure><p id="7763" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们将在matplotlib中使用散点图来绘制单词嵌入，</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc nd l"/></div></figure><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mq"><img src="../Images/9a8665b906c23d72c1a4beba39392b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*zKPGYZJ42yZ65pT3stDhRw.png"/></div></figure><p id="b90b" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">正如我们所看到的，单词嵌入是相互映射的。这个结果是通过使用预训练模型得到的，为了简单起见讨论了基本模型。原始模型可以在这里找到<a class="ae jz" href="https://github.com/vj-09/pretrained_vs_customtrained/blob/master/pretrained%20vs%20customtrained%20.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="b962" class="kx ky hu bd kz la lb lc ld le lf lg lh kj li lj lk kn ll lm ln kr lo lp lq lr dt translated">让我们找到一些相似之处</h2><p id="1968" class="pw-post-body-paragraph ka kb hu kc b kd ls iv kf kg lt iy ki kj lu kl km kn lv kp kq kr lw kt ku kv hn dt translated">使用这个单词嵌入，我们可以在我们的语料库中找到单词之间的相似性。</p><pre class="jk jl jm jn fq mr ms mt mu aw mv dt"><span id="5b6b" class="kx ky hu ms b fv mw mx l my mz">&gt;&gt;&gt;model_1.most_similar(positive=[‘first’], topn=1)</span><span id="1b16" class="kx ky hu ms b fv na mx l my mz">[('second', 0.8512464761734009)]</span></pre><p id="eb1a" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">最相似功能是在我们的嵌入中找到与目标单词相似的单词。</p><pre class="jk jl jm jn fq mr ms mt mu aw mv dt"><span id="6d72" class="kx ky hu ms b fv mw mx l my mz">&gt;&gt;&gt;model_1.similarity('one', 'another')</span><span id="8b79" class="kx ky hu ms b fv na mx l my mz">0.80782</span></pre><p id="1a51" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在这里，我们发现这两个词在我们的嵌入中有相似之处。像这样，有许多有用的功能可以使用。他们可以在这里找到。</p></div><div class="ab cl nh ni hc nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="hn ho hp hq hr"><p id="a75b" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们到此结束，希望我已经对word2vec嵌入做了一些介绍。在这里查看其他作品<a class="ae jz" rel="noopener" href="/@athithyavijay">。</a></p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nb"><img src="../Images/3b4017befed4da03dd99f0f668b61b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*m-QzmucEeah2H-RzD7u_rQ.jpeg"/></div></figure><p id="ae9a" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">哈哈，如果你这么认为，我们就在同一页上。下面我们连线<a class="ae jz" rel="noopener" href="/@athithyavijay">中</a>、<a class="ae jz" href="https://www.linkedin.com/in/vijay-athithya-79830ba1/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>、<a class="ae jz" href="https://www.facebook.com/vakky.vj" rel="noopener ugc nofollow" target="_blank">脸书</a>。</p></div></div>    
</body>
</html>