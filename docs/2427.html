<html>
<head>
<title>Build an Abstractive Text Summarizer in 94 Lines of Tensorflow !! (Tutorial 6)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Tensorflow的94行中构建一个抽象的文本摘要器！！(教程6)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55?source=collection_archive---------2-----------------------#2019-04-16">https://medium.com/hackernoon/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55?source=collection_archive---------2-----------------------#2019-04-16</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/23ce061852f4a063c4bfecece52ec0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1aNTqz6Dkial9djoJELfA.jpeg"/></div></div></figure><p id="2825" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程是一系列教程中的第六个，将帮助您使用tensorflow构建一个抽象的文本摘要器，今天我们将以优化的方式在tensorflow中构建一个抽象的文本摘要器。</p><blockquote class="ka kb kc"><p id="01e3" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">今天，我们将浏览一个为这项任务构建的最优化的模型，这个模型是由<a class="ae kh" href="https://github.com/dongjun-Lee" rel="noopener ugc nofollow" target="_blank"> <strong class="je hv">董军-李</strong> </a>编写的，这是<a class="ae kh" href="https://github.com/dongjun-Lee/text-summarization-tensorflow" rel="noopener ugc nofollow" target="_blank"> <strong class="je hv">到他的模型的链接</strong> </a>，我已经在不同的数据集(不同的语言)上使用了他的模型，它产生了真正惊人的结果，所以我真的要感谢他的努力</p></blockquote><p id="b46b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我对模型进行了多次修改，使其能够在google colab上无缝运行(<a class="ae kh" href="https://github.com/theamrzaki/text_summurization_abstractive_methods/tree/master/Implementation%20A%20(seq2seq%20with%20attention%20and%20feature%20rich%20representation)/Model%202" rel="noopener ugc nofollow" target="_blank">链接到我的模型</a>)，并且我已经将数据托管到google drive上(<a class="ae kh" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">更多关于如何将google drive链接到google colab </a>)，因此不需要下载代码和数据，您只需要一个google colab会话来运行代码， 把我的谷歌硬盘里的数据复制到你的硬盘里(关于这个还有<a class="ae kh" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">，把谷歌硬盘连接到你的谷歌笔记本上)</a></p></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><figure class="kp kq kr ks fq iv fe ff paragraph-image"><div class="ab fr cl kt"><img src="../Images/16577fbdd2beff9f3d1dcb3f38417358.png" data-original-src="https://miro.medium.com/v2/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div><figcaption class="ku kv fg fe ff kw kx bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="8b81" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我在一个网站上添加了一个文本摘要模型<a class="ae kh" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>，这样你就可以实际尝试自己生成摘要(看看你能构建什么)，它可以通过简单的api调用来调用，并且通过一个<a class="ae kh" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python包</a>，这样文本摘要就可以很容易地集成到你的应用程序中，而不需要设置tensorflow环境的麻烦，你可以免费注册<a class="ae kh" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank">，并享受免费使用这个api的乐趣。</a></p></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="fa9a" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">0-简介</h1><h2 id="da23" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">0-A关于系列</h2><p id="2676" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这是一系列教程，将帮助您使用tensorflow使用多种方法构建一个抽象的文本摘要器，我们称之为抽象，因为我们教导神经网络生成单词，而不仅仅是复制单词。</p><p id="5b9b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae kh" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">0.<a class="ae kh" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a>(如何使用google colab和google drive)</p><ol class=""><li id="4346" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated"><a class="ae kh" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">概述文本摘要任务和用于该任务的不同技术</a></li><li id="0ee3" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated"><a class="ae kh" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank"> <strong class="je hv">使用的数据以及如何表示我们的任务</strong> </a> <strong class="je hv">(本教程的先决条件)</strong></li><li id="7bcc" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated"><a class="ae kh" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">什么是seq2seq文本摘要，为什么</a></li><li id="8e99" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated"><a class="ae kh" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">多层双向LSTM/GRU </a></li><li id="9884" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated"><a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">波束搜索&amp;注意文本摘要</a></li></ol><h2 id="c67d" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">0-B关于使用的数据</h2><p id="3d0d" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">将被使用的数据将是新闻和它们的标题，它可以在我的google drive上找到，所以你只需将它复制到你的google drive上，而不需要下载它</p><blockquote class="ka kb kc"><p id="2ee6" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated">我们将使用单词嵌入来表示数据，这只是将每个单词转换为一个特定的向量，我们将为我们的单词创建一个字典(<a class="ae kh" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">更多关于这个</a> ) <strong class="je hv">(本教程的先决条件)</strong></p></blockquote><h2 id="9278" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">0-C关于使用的型号</h2><p id="f679" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">有不同的方法来完成这项任务，它们是建立在一个基础概念上的，并且它们会继续发展和建设。</p><p id="5de0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">今天，我们将开始构建这个基石实现，这是一种称为RNN的网络，它被安排在一个称为seq2seq的编码器/解码器架构中(<a class="ae kh" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">详见本</a>)，然后我们将在一个多层双向结构中构建seq2seq，其中rnn小区将是一个LSTM小区(<a class="ae kh" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">详见本</a>)， 然后，我们将添加一个注意机制，以更好地连接编码器和解码器(更多关于的<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">)，然后，为了生成更好的输出，我们使用了波束搜索的巧妙概念(更多关于</a>的<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086"/></p><p id="b86c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所有这些不同方法的代码可以在<a class="ae kh" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="8017" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们开始吧！！</p></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="5f6b" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">模型结构</h1><p id="5e89" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">我们的模型被构造成不同的模块，这些模块是</p><figure class="kp kq kr ks fq iv fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/99b6a0337a5ddfea0ac266c228792176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*xGRVCRpegxqHbbC0OGabBg.jpeg"/></div></figure><h2 id="b6bc" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">初始化块:</h2><p id="c952" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这里我们将初始化所需的张量流<strong class="je hv">占位符</strong> &amp; <strong class="je hv">变量</strong>，这里我们将定义将在整个模型中使用的<strong class="je hv"> RNN单元格</strong></p><h2 id="8080" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">嵌入块:</h2><p id="53f3" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这里我们将定义在<strong class="je hv">编码器</strong> &amp;和<strong class="je hv">解码器</strong>中使用的嵌入矩阵</p><h2 id="996c" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">编码器模块:</h2><p id="adf4" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">在这里，我们将定义<strong class="je hv">多层双向RNN </strong>(在这个上有更多<a class="ae kh" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">)，它构成了我们模型的编码器部分，我们<strong class="je hv">输出编码器</strong>状态作为解码器部分的输入</a></p><h2 id="9e47" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">解码器模块:</h2><p id="1d01" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这里，解码器实际上分为两个不同的部分</p><ol class=""><li id="d179" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated"><strong class="je hv">注意机制</strong> ( <a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于此</a>)用于更好地连接编码器和解码器，这将用于<strong class="je hv">训练</strong>阶段</li><li id="738b" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated"><strong class="je hv">光束搜索</strong> ( <a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于这个</a>)用于从我们的模型产生更好的输出，这将用于<strong class="je hv">测试</strong>阶段</li></ol><h2 id="18af" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">损失区块:</h2><p id="c2d3" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这个模块将只在<strong class="je hv">训练</strong>阶段使用，这里我们将对我们的梯度应用剪辑，我们将实际运行我们的优化器(这里使用Adam优化器)，这里是我们将梯度应用到优化器的地方。</p></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="ccbf" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">1-初始化块</h1><p id="700f" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">首先，我们需要导入我们将使用的库</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="55f6" class="lw kz hu nf b fv nj nk l nl nm">import tensorflow as tf<br/>from tensorflow.contrib import rnn  <strong class="nf hv">#cell that we would use</strong></span></pre><p id="8404" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在构建我们的模型类之前，我们需要先定义一些张量流的概念</p><figure class="kp kq kr ks fq iv fe ff paragraph-image"><div class="fe ff nd"><img src="../Images/523036760cb790b18486d16eab406aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*k7QUtdqCClleuLEkMhvN4Q.jpeg"/></div></figure><p id="f62b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以我们倾向于这样定义占位符</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="d9c0" class="lw kz hu nf b fv nj nk l nl nm">X = tf.placeholder(tf.int32, [None, article_max_len])<br/># here we define the input x as int32 , with promise to provide its # data in runtime<br/>#<br/># we also provide its shape , where None is used for a dimension of # any size </span></pre><p id="5806" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对于变量，我们倾向于把它们定义为</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="f2a8" class="lw kz hu nf b fv nj nk l nl nm">global_step = tf.Variable(0, trainable=False)<br/># a variable must be intialized , <br/># and we can set it to either be trainable or not</span></pre><p id="9d51" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后让我们构建我们的<strong class="je hv">模型类</strong></p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="ded7" class="lw kz hu nf b fv nj nk l nl nm">class Model(object):<br/>    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):<br/>        self.vocabulary_size = len(reversed_dict)<br/>        self.embedding_size = args.embedding_size<br/>        self.num_hidden = args.num_hidden<br/>        self.num_layers = args.num_layers<br/>        self.learning_rate = args.learning_rate<br/>        self.beam_width = args.beam_width</span></pre><p id="2d03" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将传递一个名为args的obj，它实际上包含来自</p><ol class=""><li id="50cc" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">嵌入大小(word2vector的大小)</li><li id="6f7f" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">num _ hidden(RNN的大小)</li><li id="a598" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">num _ layers(RNN的层数)(<a class="ae kh" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">更多关于这个</a>)</li><li id="ab9e" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">学习率</li><li id="1eaf" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">波束宽度(<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于这个</a></li><li id="5c5a" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">保留问题</li></ol><p id="f9f0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们还需要用其他参数初始化模型，如</p><ol class=""><li id="4061" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">逆序字典(关键字字典，每个关键字都是一个指向特定顺序的数字)</li><li id="913a" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">article _ max _ len &amp; article _ summary _ len(作为输入的文章句子的最大长度和作为输出的摘要句子的最大长度)</li><li id="29e3" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">仅向前(布尔值表示训练或测试阶段)(<strong class="je hv">仅向前=假→训练阶段</strong>)</li></ol><p id="15a8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后继续初始化</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="ff4f" class="lw kz hu nf b fv nj nk l nl nm">if not forward_only: <strong class="nf hv">#training phase</strong><br/>            <strong class="nf hv">#keep_prob as variable in training phase</strong><br/>            self.keep_prob = args.keep_prob<br/>else: <strong class="nf hv">#testing phase</strong><br/>            <strong class="nf hv">#keep_prob constant in testing phase</strong><br/>            self.keep_prob = 1.0<br/>   <br/>        <strong class="nf hv">#here we would use LSTM as our cell</strong><br/>        self.cell = tf.nn.rnn_cell.BasicLSTMCell <br/>  <br/>        <strong class="nf hv">#projection layer that would be used in decoder in both <br/>        #training and testing phase</strong><br/>        with tf.variable_scope("decoder/projection"):<br/>              self.projection_layer =  tf.layers.Dense(self.vocabulary_size, use_bias=False)<br/>   <br/>        <strong class="nf hv">#define batch size(our data would be provided in batches)</strong><br/>        self.batch_size = tf.placeholder(tf.int32, (), name="batch_size")<br/>  <br/>        <strong class="nf hv">#X as input , define as length of articles </strong><br/>        self.X = tf.placeholder(tf.int32, [None, article_max_len])<br/>        self.X_len = tf.placeholder(tf.int32, [None])<br/>  <br/>        <strong class="nf hv">#define decoder (input , target , length) <br/>        #using the summary length </strong><br/>        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])<br/>        self.decoder_len = tf.placeholder(tf.int32, [None])<br/>        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])<br/>  <br/>        <strong class="nf hv">#define global step beginning from zero </strong><br/>        self.global_step = tf.Variable(0, trainable=False)</span></pre></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="395a" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">2-嵌入块:</h1><p id="598a" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">在这里，我们将使用word2vector来表示我们的<strong class="je hv">输入条目，它们将是嵌入式输入</strong>和<strong class="je hv">解码器输入</strong>(更多关于此的<a class="ae kh" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank"/></p><figure class="kp kq kr ks fq iv fe ff paragraph-image"><div class="fe ff nn"><img src="../Images/365d51ffb5e581f0c979f8fb4fb1a571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*IqRc497uX7BoW4I2_KfSrA.jpeg"/></div></figure><p id="294f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将变量定义为嵌入在变量作用域中，我们将其命名为嵌入</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="1739" class="lw kz hu nf b fv nj nk l nl nm">with tf.name_scope("embedding"):<br/>            <strong class="nf hv">#if training , <br/>            #and you enable args.glove variable to true</strong><br/>            if not forward_only and args.glove:<br/>                <strong class="nf hv">#here we use tf.constant as we won't change it<br/>                #get_init_embedding is a function <br/>                #that returns the vector for each word in our dict</strong><br/>                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)<br/>            else: <strong class="nf hv">#else random define the word2vector for testing</strong><br/>                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)<br/>            self.embeddings = tf.get_variable("embeddings", initializer=init_embeddings)<br/>            <strong class="nf hv">#then define for both encoder input</strong><br/>            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])<br/>            <strong class="nf hv">#and define for decoder input</strong><br/>            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])</span></pre></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="485b" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">3-编码器模块:</h1><p id="296b" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">在这里，我们实际上为seq2seq的编码器部分定义了多层双向lstm(在这个中有更多的<a class="ae kh" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">)，我们将在我们称之为“编码器”的命名范围中定义我们的变量。</a></p><p id="7c42" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们将使用<strong class="je hv">退出</strong>的概念，我们将在我们架构中的每个单元后使用它，它用于随机激活我们网络的子集，并在正则化训练期间使用。</p><figure class="kp kq kr ks fq iv fe ff paragraph-image"><div class="fe ff no"><img src="../Images/9840e10655af34334fdb98856adc1f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/0*ShshXyr7nCNsQKD5.jpg"/></div></figure><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="b704" class="lw kz hu nf b fv nj nk l nl nm">with tf.name_scope("encoder"):</span><span id="3384" class="lw kz hu nf b fv np nk l nl nm">            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]</span><span id="11a0" class="lw kz hu nf b fv np nk l nl nm">            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]</span><span id="bd4f" class="lw kz hu nf b fv np nk l nl nm">            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]</span><span id="3110" class="lw kz hu nf b fv np nk l nl nm">            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]</span></pre><p id="89b0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，在定义了前向和后向单元之后，我们需要将它们连接在一起以形成双向结构，因此我们将使用<strong class="je hv">stack _ bidirectional _ dynamic _ rnn</strong>，它将以下所有参数作为其输入</p><ol class=""><li id="3206" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">前向单元格</li><li id="cabb" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">反向单元格</li><li id="0c72" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">编码器emb输入(以word2vector格式输入文章)</li><li id="9f35" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">X_len(文章长度)</li><li id="bf61" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">使用time_major = True会更有效一点，因为它避免了在RNN计算的开始和结束时进行转置。</li></ol><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="ae14" class="lw kz hu nf b fv nj nk l nl nm">encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(<br/>                fw_cells, bw_cells, self.encoder_emb_inp,<br/>                sequence_length=self.X_len, time_major=True, dtype=tf.float32)</span></pre><p id="4baf" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在我们需要实际使用这个<strong class="je hv">stack _ bidirectional _ dynamic _ rnn</strong>函数的输出，我们主要需要2个主输出</p><ol class=""><li id="922e" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">encoder_output(将用于注意力计算)(<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于注意力</a>)</li><li id="3132" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">编码器状态(将用于解码器的初始状态)</li></ol><p id="26f5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，要获得编码器输出，我们只需</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="ce7e" class="lw kz hu nf b fv nj nk l nl nm">self.encoder_output = tf.concat(encoder_outputs, 2)</span></pre><p id="7602" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后为了得到encoder_state，我们将使用LSTMStateTuple组合正向和反向的(encoder_state_c)和(encoder_state_h)</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="d9f3" class="lw kz hu nf b fv nj nk l nl nm">encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)</span><span id="a725" class="lw kz hu nf b fv np nk l nl nm">encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)</span><span id="fe48" class="lw kz hu nf b fv np nk l nl nm">self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)</span></pre></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="a3d6" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">4解码器模块:</h1><p id="ba17" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这里解码器分为两部分</p><ol class=""><li id="8314" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">训练部分(训练注意力模型)(<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于注意力模型</a>)</li><li id="2c7f" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">测试/运行部分(用于注意和光束搜索)(<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于光束搜索</a>)</li></ol><p id="a1ce" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，让我们首先为这两个部分定义out (name scope)和(variable scope)，我们还将定义一个多层单元结构，它也将用于这两个部分</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="2bdb" class="lw kz hu nf b fv nj nk l nl nm">with tf.name_scope("decoder"), tf.variable_scope("decoder") as decoder_scope:<br/>            decoder_cell = self.cell(self.num_hidden * 2)</span></pre><h2 id="6b2e" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">4 .训练部分(注意力模型)</h2><p id="9b09" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">首先，我们需要准备我们的注意力结构，这里我们将使用BahdanauAttention</p><p id="b030" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> encoder_output </strong>将在注意力计算中使用(更多关于注意力模型的<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086"/></p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="b49b" class="lw kz hu nf b fv nj nk l nl nm">attention_states = tf.transpose(self.encoder_output, [1, 0, 2])<br/>attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(<br/>                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)</span></pre><p id="c3d4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后我们将进一步定义解码器单元(从解码器的第一步开始，我们将解码器单元定义为一个简单的多层lstm，现在我们将添加attention)，为此我们将使用<strong class="je hv"> AttentionWrapper </strong>，它将attention_mechanism与解码器单元结合在一起</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="03e2" class="lw kz hu nf b fv nj nk l nl nm">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,                                                                 attention_layer_size=self.num_hidden * 2)</span></pre><p id="a98d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们需要定义解码器单元的输入，该输入实际上来自2个来源(seq2seq 上的<a class="ae kh" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">更多信息)</a></p><ol class=""><li id="8c8a" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">编码器输出(在初始步骤中使用)</li><li id="a9a7" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">解码器输入(训练阶段的总结句子)</li></ol><p id="5e8b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们首先定义来自编码器的初始状态</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="d2c0" class="lw kz hu nf b fv nj nk l nl nm">initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)</span><span id="99db" class="lw kz hu nf b fv np nk l nl nm">initial_state = initial_state.clone(cell_state=self.encoder_state)</span></pre><p id="e737" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们将初始状态与解码器输入(总结句子)结合起来，这里使用BasicDecoder，我们需要通过一个助手提供解码器输入，这个助手将所有(decoder_emb_inp，decoder_len)结合在一起</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="a1b9" class="lw kz hu nf b fv nj nk l nl nm">helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)</span><span id="33fc" class="lw kz hu nf b fv np nk l nl nm">decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)</span><span id="dba1" class="lw kz hu nf b fv np nk l nl nm">outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)</span></pre><p id="875e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，对于训练阶段的最后一步，我们需要定义解码器的输出(logits ),在训练的丢失块中使用</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="9904" class="lw kz hu nf b fv nj nk l nl nm"><strong class="nf hv">#just use the rnn_outputs from aall the outputs<br/></strong>self.decoder_output = outputs.rnn_output</span><span id="6fa5" class="lw kz hu nf b fv np nk l nl nm"><strong class="nf hv">#then get logits , by performing a transpose on decoder output<br/></strong>self.logits = tf.transpose(self.projection_layer(self.decoder_output), perm=[1, 0, 2])</span><span id="7372" class="lw kz hu nf b fv np nk l nl nm"><strong class="nf hv">#then reshape the logits <br/></strong>self.logits_reshape = tf.concat(<br/>                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)</span></pre><h2 id="5d8c" class="lw kz hu bd la lx ly lz le ma mb mc li jn md me lm jr mf mg lq jv mh mi lu mj dt translated">4.b测试/运行部分(注意和光束搜索)</h2><p id="5c45" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">在此阶段，有两个主要目标</p><ol class=""><li id="92f2" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">将编码器输出和编码器状态&amp; x_len(物品长度)分成多个部分，以实际执行光束搜索方法(<a class="ae kh" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">更多关于光束搜索的信息</a></li><li id="e507" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">构建一个独立于解码器输入的解码器，因为在测试阶段我们没有摘要句子作为输入，所以我们需要以不同于上面的方式构建解码器</li></ol><p id="e230" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先，让我们将编码器输出和编码器状态&amp; x_len(物品长度)分成几部分，以实际执行光束搜索方法，这里我们将使用上面已经定义的<strong class="je hv">光束宽度</strong>变量</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="a857" class="lw kz hu nf b fv nj nk l nl nm">tiled_encoder_output = tf.contrib.seq2seq.tile_batch(<br/>                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)</span><span id="828c" class="lw kz hu nf b fv np nk l nl nm">tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)<br/>                <br/>tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)</span></pre><p id="883d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后让我们定义注意机制(就像前面一样，但是要考虑平铺变量)</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="0952" class="lw kz hu nf b fv nj nk l nl nm">attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(<br/>                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)</span><span id="9604" class="lw kz hu nf b fv np nk l nl nm">decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,                                                                 attention_layer_size=self.num_hidden * 2)</span><span id="cdcb" class="lw kz hu nf b fv np nk l nl nm">initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)<br/>                <br/>initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)</span></pre><p id="bff4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后让我们定义我们的解码器，但是这里我们将使用<strong class="je hv"> BeamSearchDecoder </strong>，这考虑了所有的</p><ol class=""><li id="11de" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">解码器单元(先前定义)</li><li id="9ded" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">嵌入word2vector(在嵌入部分定义)</li><li id="3661" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">投影层(在课程开始时定义)</li><li id="9223" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">解码器初始状态(先前定义)</li><li id="5d7c" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">光束宽度(用户定义)</li><li id="846e" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">开始令牌和结束令牌</li></ol><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="1788" class="lw kz hu nf b fv nj nk l nl nm">decoder = tf.contrib.seq2seq.BeamSearchDecoder(<br/>                    cell=decoder_cell,<br/>                    embedding=self.embeddings,<br/>                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),<br/>                    end_token=tf.constant(3),<br/>                    initial_state=initial_state,<br/>                    beam_width=self.beam_width,<br/>                    output_layer=self.projection_layer )</span></pre><p id="83bb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">剩下要做的就是定义输出，它实际上会直接反映整个seq2seq架构的实际输出，因为这一阶段是实际计算预测的阶段</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="6fe9" class="lw kz hu nf b fv nj nk l nl nm">outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(<br/>                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)</span><span id="2074" class="lw kz hu nf b fv np nk l nl nm">self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])</span></pre></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><h1 id="3643" class="ky kz hu bd la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv dt translated">5-损失区块:</h1><p id="cbfb" class="pw-post-body-paragraph jc jd hu je b jf mk jh ji jj ml jl jm jn mm jp jq jr mn jt ju jv mo jx jy jz hn dt translated">这一块是训练实际发生的地方，这里训练实际上是通过多个步骤发生的</p><ol class=""><li id="b414" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">计算损失(<a class="ae kh" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">关于损失计算的更多信息</a>)</li><li id="da5c" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">计算渐变并在渐变上应用剪辑(更多关于分解渐变的内容<a class="ae kh" href="https://hackernoon.com/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener ugc nofollow" target="_blank"/></li><li id="30b9" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">应用优化器(这里我们将使用Adam优化器)</li></ol><p id="ea22" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先，我们定义我们的名称范围，我们将指定这个块只在训练阶段有效</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="3566" class="lw kz hu nf b fv nj nk l nl nm">with tf.name_scope("loss"):<br/>            if not forward_only:</span></pre><p id="08f2" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">其次，我们将计算损失(<a class="ae kh" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">更多关于损失计算</a></p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="3681" class="lw kz hu nf b fv nj nk l nl nm">crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(<br/>                    logits=self.logits_reshape, labels=self.decoder_target)</span><span id="631a" class="lw kz hu nf b fv np nk l nl nm">weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)</span><span id="2e3f" class="lw kz hu nf b fv np nk l nl nm">self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))</span></pre><p id="619d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第三，我们将计算我们的梯度，并应用剪辑梯度，以解决问题的爆炸梯度(<a class="ae kh" href="https://hackernoon.com/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener ugc nofollow" target="_blank">更多关于爆炸梯度</a>)</p><blockquote class="ka kb kc"><p id="feca" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated"><strong class="je hv"> ( </strong> <a class="ae kh" href="https://hackernoon.com/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f" rel="noopener ugc nofollow" target="_blank"> <strong class="je hv">出自教程4 </strong> </a> <strong class="je hv"> ) </strong></p><p id="157d" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated"><strong class="je hv">爆炸梯度:</strong>发生在深层网络中(即:有许多层<em class="hu">的网络，就像我们的例子一样)，当我们应用反向传播时，梯度会变得太大。实际上，使用<strong class="je hv">梯度削波</strong>的概念，可以很容易地解决这个误差，这只是简单地设置一个特定的阈值，当梯度超过阈值时，我们会将其削波到某个值。</em></p></blockquote><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="3649" class="lw kz hu nf b fv nj nk l nl nm">params = tf.trainable_variables()<br/>gradients = tf.gradients(self.loss, params)<br/>clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)</span></pre><p id="5a06" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第四，我们将应用我们的优化器，这里我们将使用Adam优化器，这里我们将使用之前定义的learning_rate</p><pre class="kp kq kr ks fq ne nf ng nh aw ni dt"><span id="1a5e" class="lw kz hu nf b fv nj nk l nl nm">optimizer = tf.train.AdamOptimizer(self.learning_rate)</span><span id="1a41" class="lw kz hu nf b fv np nk l nl nm">self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)</span></pre></div><div class="ab cl ki kj hc kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hn ho hp hq hr"><p id="c8f3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">如果上帝愿意，下一次我们会去的</p><ol class=""><li id="a4f7" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">将我们的数据分成批次所需的代码</li><li id="993b" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">需要代码来使用此模型进行培训</li></ol><p id="03b5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">然后，在我们完成了这个核心模型的实现之后，如果上帝愿意的话，我们将使用其他的现代实现来实现文本摘要，比如</p><ol class=""><li id="6255" class="mp mq hu je b jf jg jj jk jn mr jr ms jv mt jz mu mv mw mx dt translated">指针发生器</li><li id="a5c2" class="mp mq hu je b jf my jj mz jn na jr nb jv nc jz mu mv mw mx dt translated">通过seq2seq使用强化学习</li></ol><p id="56a4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">(<a class="ae kh" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">更多关于文本摘要seq2seq的不同实现</a>)</p><p id="9a3f" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程的所有代码都可以在这里找到。</p><blockquote class="ka kb kc"><p id="a6b5" class="jc jd kd je b jf jg jh ji jj jk jl jm ke jo jp jq kf js jt ju kg jw jx jy jz hn dt translated"><em class="hu">我真心希望你喜欢阅读这个教程，我希望我已经把这些概念讲清楚了，这一系列教程的所有代码都可以在这里找到</em><a class="ae kh" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"><em class="hu"/></a><em class="hu">，你可以简单地使用google colab来运行它，请查看教程并告诉我你对它的看法，希望再次见到你</em></p></blockquote><h1 id="bc58" class="ky kz hu bd la lb nq ld le lf nr lh li lj ns ll lm ln nt lp lq lr nu lt lu lv dt translated">后续教程</h1><ul class=""><li id="c01a" class="mp mq hu je b jf mk jj ml jn nv jr nw jv nx jz ny mv mw mx dt translated"><a class="ae kh" href="http://bit.ly/2EhcRIZ" rel="noopener ugc nofollow" target="_blank">用于文本摘要的抽象&amp;提取方法的组合(教程7) </a></li></ul><figure class="kp kq kr ks fq iv"><div class="bz el l di"><div class="nz oa l"/></div></figure></div></div>    
</body>
</html>