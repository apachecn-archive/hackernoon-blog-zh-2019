<html>
<head>
<title>Analyze MongoDB Logs Using PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PySpark 分析 MongoDB 日志</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/analyze-mongodb-logs-using-pyspark-97a915547da0#2019-07-27">https://medium.com/hackernoon/analyze-mongodb-logs-using-pyspark-97a915547da0#2019-07-27</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/b95f2b0a03ebd6bf9baf199fc94fe202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ho0ganbpVAwGe8_9yM9vDw.png"/></div></div></figure><blockquote class="jc jd je"><p id="606d" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">“如果没有大数据，你会又瞎又聋，还在高速公路上。”—杰弗里·摩尔</p></blockquote><p id="acf1" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">如今，大数据已经成为计算机世界最重要的部分。一个重要的部分是读取和分析由我们使用的不同类型的数据库或其他服务或产品生成的大量日志。既然 MongoDB 已经进入了我们的生态系统，那么理解 mongo 日志并从中提取一些有用的信息就变得至关重要了。我阅读并研究了如何评价这些引导我走向 PySpark 的诗句。</p><p id="1174" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">在本文中，我假设已经安装了 PySpark(如果没有安装，我在文章末尾添加了 URL，它将帮助您安装 PySpark)。MongoDB 日志消息采用以下格式:</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="8d2c" class="kq kr hu km b fv ks kt l ku kv">&lt;timestamp&gt; &lt;severity&gt; &lt;component&gt; [&lt;context&gt;] &lt;message&gt;</span><span id="078e" class="kq kr hu km b fv kw kt l ku kv">ex: 2019-07-08T06:26:01.021+0000 I CONTROL  [signalProcessingThread] Replica Set Member State: PRIMARY</span></pre><p id="2607" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">你可以在<a class="ae kx" href="https://docs.mongodb.com/manual/reference/log-messages/" rel="noopener ugc nofollow" target="_blank">这个链接</a>上阅读更多关于 MongoDB 日志消息的细节。你将会对我们将要构建的消息和正则表达式有更多的了解。<br/>首先在程序中加载 MongoDB 日志文件</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="d619" class="kq kr hu km b fv ks kt l ku kv">sc = SparkContext()<br/>sqlContext = SQLContext(sc)<br/>spark = SparkSession(sc)<br/>base_df = spark.read.text("path_to_log_file")</span><span id="7059" class="kq kr hu km b fv kw kt l ku kv"># base_df is now a spark data frame<br/>print(base_df)                    # pyspark.sql.dataframe.DataFrame<br/>base_df1.count()                  # count number of logs<br/>base_df1.show(10, truncate=True)  # showing first 10 logs</span></pre><p id="3a0e" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">我们将使用正则表达式来分隔消息中的不同部分。</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="c872" class="kq kr hu km b fv ks kt l ku kv">timestamp_regex = "\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{3}[+]\d{4}"</span><span id="18ec" class="kq kr hu km b fv kw kt l ku kv">severity_levels_regex = "[F, E, W, I, D]\s"</span><span id="e035" class="kq kr hu km b fv kw kt l ku kv">component_regex = "(ACCESS|COMMAND|CONTROL|FTDC|GEO|INDEX|NETWORK|QUERY|REPL|REPL_HB|ROLLBACK|SHARDING|STORAGE|RECOVERY|JOURNAL|TXN|WRITE)"</span><span id="f045" class="kq kr hu km b fv kw kt l ku kv">context_regex = "\[(.*?)\]"</span><span id="123e" class="kq kr hu km b fv kw kt l ku kv">message_regex = "\]([\s\S]*)"</span></pre><p id="2e5d" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">在此之后，我们将构建一个表格数据框架，在此框架上可以应用不同的过滤器并执行查询。</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="1708" class="kq kr hu km b fv ks kt l ku kv">logs_df = base_df1.select(<br/>regexp_extract('value', timestamp_regex, 0).alias('timestamp'),<br/>regexp_extract('value', severity_levels_regex, 0).alias('severity'),<br/>regexp_extract('value', component_regex, 0).alias('component'),<br/>regexp_extract('value', context_regex, 1).alias('context'),<br/>regexp_extract('value', message_regex, 1).alias('message')<br/>)</span><span id="8bb3" class="kq kr hu km b fv kw kt l ku kv">print(logs_df)<br/>logs_df.show(10, truncate=True)</span></pre><p id="1047" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">下面是上面两条语句的输出:<br/> DataFrame【时间戳:字符串，严重性:字符串，组件:字符串，上下文:字符串，消息:字符串】</p><figure class="kh ki kj kk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ky"><img src="../Images/03f91e7b3743aa02134065cb7b67aafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSz749Kr2PTjuUrQInd7fA.png"/></div></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Output: logs_df.show(10, truncate=True)</figcaption></figure><p id="1f47" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">现在，我们在数据框中有了完整的文件，因此我们可以执行不同的操作，例如:</p><p id="f057" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">a.因为 MongoDB 记录了每个执行时间超过 100 毫秒的查询。我们可以过滤掉执行时间较长的查询。以下是一个查询的日志:</p><p id="83e9" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">| 2019–07–15t 05:55:52.236+0000 | I | COMMAND | conn 24971548 | COMMAND<dbname>。<collection_name>命令:find { find: " <collection_name>"，filter:{ request id:" 0286 aedf-e042–4961–8 ffc-578847 fabf 15 " }，projection: { _id: 0 }，limit: 1，singleBatch: true，LSID:{ id:UUID(" 70382 CDE-19 a2 e2f 74 fbfd ")}，$ cluster time:{ cluster time:Timestamp(1563170121，8)，签名:{ keyId:66588</collection_name></collection_name></dbname></p><p id="5455" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">正如您在日志中看到的，时间记录在语句的末尾，我们可以使用这个时间来过滤掉这样的查询。我们可以使用以下函数过滤掉耗时超过 3000 毫秒的查询(根据您的要求进行更改)。此外，这些类型的日志中的“组件”相当于“命令”或“写入”，因此，我们也需要过滤掉这两个组件。</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="cffd" class="kq kr hu km b fv ks kt l ku kv"># Function to filter queries taking longer than 3000ms<br/>def eval_string(string):<br/>    time_taken = string.split(' ')[-1]<br/>    if time_taken[:-2].isdigit() and int(time_taken[:-2]) &gt; 3000:<br/>        return string<br/>    return ""</span><span id="82c3" class="kq kr hu km b fv kw kt l ku kv"># Filtering out the COMMAND and WRITE components from data frame<br/>components = logs_df[ ((logs_df['component'] == "COMMAND") | (logs_df['component'] == "WRITE")) ]</span><span id="4a16" class="kq kr hu km b fv kw kt l ku kv"># udf is used to create a user defined function<br/>udf_myFunction = udf(eval_string) # if the function returns an int</span><span id="5fbf" class="kq kr hu km b fv kw kt l ku kv">df = components.withColumn("message", udf_myFunction("message"))</span><span id="7042" class="kq kr hu km b fv kw kt l ku kv"># eval_string will return "" in case of time&lt;3000ms, <br/># filter out the queries in which message is not equal to ""<br/>df.filter(df.message != "").show(10, truncate=True)</span></pre><figure class="kh ki kj kk fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ld"><img src="../Images/0e03e2f2e8177df1669c400ade8f21e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ww6fXL7g30qyabSOd59RFQ.png"/></div></div><figcaption class="kz la fg fe ff lb lc bd b be z ek">Showing 10 rows of filtered queries. Mention truncate=False to see complete queries</figcaption></figure><p id="4b65" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">以下是完整的程序:</p><pre class="kh ki kj kk fq kl km kn ko aw kp dt"><span id="8588" class="kq kr hu km b fv ks kt l ku kv">from pyspark.context import SparkContext<br/>from pyspark.sql.context import SQLContext<br/>from pyspark.sql.session import SparkSession<br/>from pyspark.sql.functions import regexp_extract, lit, col, udf<br/>from pyspark.sql import DataFrame<br/>import re<br/>from functools import reduce<br/>import json</span><span id="0b12" class="kq kr hu km b fv kw kt l ku kv">def eval_string(string):<br/>    time_taken = string.split(' ')[-1]<br/>    if time_taken[:-2].isdigit() and int(time_taken[:-2]) &gt; 3000:<br/>        return string<br/>    return ""</span><span id="53d2" class="kq kr hu km b fv kw kt l ku kv">sc = SparkContext()<br/>sqlContext = SQLContext(sc)<br/>spark = SparkSession(sc)</span><span id="fe1d" class="kq kr hu km b fv kw kt l ku kv">base_df = spark.read.text("/home/himanshu/Downloads/mongolog/mongod.log.1")</span><span id="42f4" class="kq kr hu km b fv kw kt l ku kv">timestamp_regex = "\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d{3}[+]\d{4}"</span><span id="1a30" class="kq kr hu km b fv kw kt l ku kv">severity_levels_regex = "[F, E, W, I, D]\s"</span><span id="b4f1" class="kq kr hu km b fv kw kt l ku kv">component_regex = "(ACCESS|COMMAND|CONTROL|FTDC|GEO|INDEX|NETWORK|QUERY|REPL|REPL_HB|ROLLBACK|SHARDING|STORAGE|RECOVERY|JOURNAL|TXN|WRITE)"</span><span id="90ea" class="kq kr hu km b fv kw kt l ku kv">context_regex = "\[(.*?)\]"<br/>message_regex = "\]([\s\S]*)"</span><span id="91a9" class="kq kr hu km b fv kw kt l ku kv">logs_df = base_df1.select(<br/>regexp_extract('value', timestamp_regex, 0).alias('timestamp'),<br/>regexp_extract('value', severity_levels_regex, 0).alias('severity'),<br/>regexp_extract('value', component_regex, 0).alias('component'),<br/>regexp_extract('value', context_regex, 1).alias('context'),<br/>regexp_extract('value', message_regex, 1).alias('message'))</span><span id="f7e3" class="kq kr hu km b fv kw kt l ku kv"># Show in desc order according to timestamp logs_df.sort(col("timestamp").desc()).show(10, truncate=True)<br/>logs_df.show(10, truncate=True)</span><span id="e37c" class="kq kr hu km b fv kw kt l ku kv">components = logs_df[ ((logs_df['component'] == "COMMAND") | (logs_df['component'] == "WRITE")) ]</span><span id="49d3" class="kq kr hu km b fv kw kt l ku kv">udf_myFunction = udf(eval_string) # if the function returns an int<br/>df = components.withColumn("message", udf_myFunction("message"))<br/>df.filter(df.message != "").show(10, truncate=True)</span><span id="89be" class="kq kr hu km b fv kw kt l ku kv"># Count the filtered logs<br/>df.filter(df.message != "").count()</span></pre><p id="74de" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated">因为我们有完整的数据框架，所以我们可以根据我们的要求过滤掉日志。如果对过滤日志有任何疑问或任何其他问题，请务必让我知道。此外，评论我是否可以改善这一点，或者如果有一个程序中的问题。</p></div><div class="ab cl le lf hc lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hn ho hp hq hr"><p id="c272" class="pw-post-body-paragraph jf jg hu ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hn dt translated"><strong class="ji hv">参考资料:<br/> </strong>我所用的一些资料来源:</p><div class="ll lm fm fo ln lo"><a href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab ej"><div class="lq ab lr cl cj ls"><h2 class="bd hv fv z el lt eo ep lu er et ht dt translated">pyspark.sql 模块— PySpark 2.1.0 文档</h2><div class="lv l"><h3 class="bd b fv z el lt eo ep lu er et ek translated">Spark SQL 和 DataFrames 的重要类别:使用 Dataset 和 DataFrame API 编程 Spark 的入口点…</h3></div><div class="lw l"><p class="bd b gc z el lt eo ep lu er et ek translated">spark.apache.org</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc ja lo"/></div></div></a></div><div class="ll lm fm fo ln lo"><a rel="noopener follow" target="_blank" href="/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0"><div class="lp ab ej"><div class="lq ab lr cl cj ls"><h2 class="bd hv fv z el lt eo ep lu er et ht dt translated">在 Ubuntu 上安装 Spark(PySpark)</h2><div class="lv l"><h3 class="bd b fv z el lt eo ep lu er et ek translated">上面的视频演示了一种在 Ubuntu 上安装 Spark (PySpark)的方法。以下说明将指导您完成…</h3></div><div class="lw l"><p class="bd b gc z el lt eo ep lu er et ek translated">medium.com</p></div></div><div class="lx l"><div class="md l lz ma mb lx mc ja lo"/></div></div></a></div></div></div>    
</body>
</html>