<html>
<head>
<title>Teach seq2seq models to learn from their mistakes using deep curriculum learning (Tutorial 8)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度课程学习，教导seq2seq模型从错误中学习(教程8)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754?source=collection_archive---------4-----------------------#2019-06-08">https://medium.com/hackernoon/teach-seq2seq-models-to-learn-from-their-mistakes-using-deep-curriculum-learning-tutorial-8-a730a387754?source=collection_archive---------4-----------------------#2019-06-08</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/9a94ef5113c107bbccdbed2af8352558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtKuKztKttAtlR-c6c4x4A.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">scheduled sampling to help seq2seq model learn from its mistakes</figcaption></figure><p id="03f0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">本教程是一系列教程中的第八篇，将帮助您使用tensorflow构建一个抽象的文本摘要器。</p><p id="6b31" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">今天我们将使用<strong class="ji hv">课程学习</strong>来解决<a class="ae ke" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank"> seq2seq </a>模型所遭遇的一个主要问题。</p><p id="9750" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">seq2seq模型通过最大化给定两者的下一个令牌的可能性来<strong class="ji hv">训练</strong></p><ol class=""><li id="0adf" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated">上一个令牌(来自上一个<a class="ae ke" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f"> LSTM </a></li><li id="0c20" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated">地面实况总结</li></ol><p id="13fd" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">而在<strong class="ji hv">推论(测试)中，</strong>它只能依赖于</p><ol class=""><li id="3739" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated">前一个令牌</li></ol><p id="c143" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">在测试中不能提供基本事实摘要，</p><blockquote class="kt ku kv"><p id="e5a1" class="jg jh kw ji b jj jk jl jm jn jo jp jq kx js jt ju ky jw jx jy kz ka kb kc kd hn dt translated">seq2seq模型已被训练为依赖外部。</p><p id="b5ed" class="jg jh kw ji b jj jk jl jm jn jo jp jq kx js jt ju ky jw jx jy kz ka kb kc kd hn dt translated">在测试的时候，它被迫<strong class="ji hv">只依靠自己</strong>，这是它没有被养大去做的事情！</p></blockquote><p id="c383" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这实际上造成了一个主要问题，即训练和推断(测试)之间的差异，这被称为(<strong class="ji hv">暴露问题</strong>)</p></div><div class="ab cl la lb hc lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hn ho hp hq hr"><p id="f5c8" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">有多种方法可以解决这个问题。其中之一是，在训练期间，通过 <strong class="ji hv">向模型暴露其自身的错误，使模型<strong class="ji hv">开始学习依赖自身</strong> <strong class="ji hv">，以便它试图优化它们</strong>(即:在训练阶段从其错误中学习)。这就是所谓的“<strong class="ji hv">预定抽样”</strong>，这是一种课程学习的形式，我们将使用它来帮助我们的<a class="ae ke" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank"> seq2seq </a>模型。</strong></p><p id="b255" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这个模型是使用tensorflow实现的(<strong class="ji hv">代码可以在这里找到</strong><a class="ae ke" href="http://bit.ly/2HZP6aB" rel="noopener ugc nofollow" target="_blank"><strong class="ji hv"/></a><strong class="ji hv">)</strong>在jupyter笔记本上运行，并与google drive无缝连接，因此不需要在您的机器上运行代码或下载数据，因为所有这些都可以在google colab上免费完成(<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">更多关于此</a>)。</p><blockquote class="kt ku kv"><p id="9e87" class="jg jh kw ji b jj jk jl jm jn jo jp jq kx js jt ju ky jw jx jy kz ka kb kc kd hn dt translated">本教程建立在google的bengio，vinyals，ndjaitly，noamg在他们的<a class="ae ke" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">论文</a> ( <a class="ae ke" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">用递归神经网络</a>进行序列预测的预定采样)中提出的概念之上</p><p id="7ee4" class="jg jh kw ji b jj jk jl jm jn jo jp jq kx js jt ju ky jw jx jy kz ka kb kc kd hn dt translated"><strong class="ji hv">代码来自</strong> <a class="ae ke" href="https://github.com/yaserkl/RLSeq2Seq#scheduled-sampling-soft-scheduled-sampling-and-end2endbackprop" rel="noopener ugc nofollow" target="_blank"> <strong class="ji hv">亚斯特克</strong> </a> <strong class="ji hv">，我已经将其修改为在google colab上运行(</strong> <a class="ae ke" href="http://bit.ly/2HZP6aB" rel="noopener ugc nofollow" target="_blank"> <strong class="ji hv">我的代码</strong> </a> <strong class="ji hv"> ) </strong></p></blockquote></div><div class="ab cl la lb hc lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hn ho hp hq hr"><h1 id="40e5" class="lh li hu bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me dt translated">0.关于系列</h1><p id="e26a" class="pw-post-body-paragraph jg jh hu ji b jj mf jl jm jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd hn dt translated">这是一系列教程，可以帮助你在多种方法中使用tensorflow构建一个抽象的文本摘要器，我们称之为抽象，因为我们教导神经网络生成单词，而不仅仅是复制单词</p><p id="5b9b" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">0.<a class="ae ke" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a>(如何使用google colab和google drive)</p><ol class=""><li id="4346" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated"><a class="ae ke" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">文本摘要任务的概述以及用于该任务的不同技术</a></li><li id="0ee3" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">使用的数据以及如何表示我们的任务</a>(本教程的先决条件)</li><li id="7bcc" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">什么是seq2seq文本摘要，为什么</a></li><li id="8e99" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">多层双向LSTM/GRU </a></li><li id="9884" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" rel="noopener" href="/@theamrzaki/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086">光束搜索&amp;注意文本摘要</a></li><li id="e0f1" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" href="https://hackernoon.com/build-an-abstractive-text-summarizer-in-94-lines-of-tensorflow-tutorial-6-f0e1b4d88b55" rel="noopener ugc nofollow" target="_blank">建立seq2seq模型，注意&amp;波束搜索</a></li><li id="7fcb" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated"><a class="ae ke" href="http://bit.ly/2EhcRIZ" rel="noopener ugc nofollow" target="_blank">用于文本摘要的抽象&amp;提取方法的组合</a></li></ol><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mk"><img src="../Images/3feaf3ab1fbd34a0a8a28d43c1f5f667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="ffbc" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">实际上，你可以通过<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>尝试使用这些系列的输出生成你自己的摘要，看看你最终能够构建什么。您还可以通过简单的API调用，以及通过一个<a class="ae ke" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python包</a>来调用它，这样就可以轻松地将文本摘要集成到您的应用程序中，而无需设置tensorflow环境。你可以免费<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank">注册</a>，免费享受使用这个API的乐趣。</p><p id="91bb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们开始吧！</p><h1 id="b2a4" class="lh li hu bd lj lk mp lm ln lo mq lq lr ls mr lu lv lw ms ly lz ma mt mc md me dt translated">1.暴露偏差问题</h1><p id="8b6f" class="pw-post-body-paragraph jg jh hu ji b jj mf jl jm jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd hn dt translated">模型从来没有上升到依赖自己。</p><p id="8364" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">seq2seq模型被训练为依赖于:</p><ol class=""><li id="25bc" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated">解码器前一节点的输出，因此取决于前一状态的输出</li><li id="0e43" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated">和输入摘要</li></ol><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/1a93a4c4002a79132510bed0ddbe71e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XKx6jXo8wjgEnau_HThKGQ.gif"/></div></div></figure><p id="023c" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">问题出现在推理(测试)步骤中，其中没有向模型提供输入概要。它只取决于:</p><ol class=""><li id="1a25" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated">前一节点的输出(前一lstm解码器步骤)</li></ol><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/af14c3ac2be4fb90ce4c3ee1a7c6db91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Xiclx7BF0U5iPTa0hzKK6g.gif"/></div></div></figure><p id="3938" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">这导致了模型的训练方式和推理(测试)方式之间的差异。这个问题叫做<strong class="ji hv"> <em class="kw">曝光偏差。</em>T11】</strong></p><h1 id="e3bf" class="lh li hu bd lj lk mp lm ln lo mq lq lr ls mr lu lv lw ms ly lz ma mt mc md me dt translated">2.暴露偏差问题会如何影响我们的模型？</h1><p id="eda8" class="pw-post-body-paragraph jg jh hu ji b jj mf jl jm jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd hn dt translated">在推断(测试)阶段，正如我们刚才所说，模型只依赖于前一步，这意味着它完全依赖于它自己。</p><p id="f2d7" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">当模型在<strong class="ji hv"> (t-1) </strong>中产生不良输出时(即前一时间步产生不良输出)，问题实际上出现了。这实际上会影响所有即将到来的序列。它会把模型带到一个完全不同的状态空间，从它在训练阶段看到和训练的地方，所以它根本不知道该做什么。这只会导致累积的不良输出决策。</p><h1 id="e6a5" class="lh li hu bd lj lk mp lm ln lo mq lq lr ls mr lu lv lw ms ly lz ma mt mc md me dt translated">3.让我们通过课程学习来解决它</h1><p id="680f" class="pw-post-body-paragraph jg jh hu ji b jj mf jl jm jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd hn dt translated">google research的<a class="ae ke" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">Ben gio et al</a>提出了一个解决这个问题的方案，将模型的依赖性从完全依赖于提供给它的基本事实逐渐改变为依赖于它自己(即，仅依赖于解码器中从先前时间步骤生成的先前令牌)。</p><p id="c6c9" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">通过时间使学习路径变得困难的概念(即，使模型仅依赖于它本身)被称为课程学习。</p><p id="4cfb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">他们实现这一点的技术真是天才。他们称之为‘预定抽样’。</p><p id="6b18" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">他们建立了一个简单的采样机制，随机选择(在训练期间)从哪里采样。要么:</p><ol class=""><li id="b7a5" class="kf kg hu ji b jj jk jn jo jr kh jv ki jz kj kd kk kl km kn dt translated">地面实况(<strong class="ji hv">与概率ei </strong> ) (i代表批次号)</li><li id="41d3" class="kf kg hu ji b jj ko jn kp jr kq jv kr jz ks kd kk kl km kn dt translated">模型本身(<strong class="ji hv">用概率(1-ei) </strong>)</li></ol><p id="6adc" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">所以让我们抛硬币决定。</p><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/0c09420c533298b54e062dbf345862f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/1*Wh1k8G3DKnm9M7zSi46NVw.gif"/></div></figure><p id="8dbb" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">如果是正面(<strong class="ji hv">带概率ei </strong> )→那么我们用地面真相汇总。</p><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/669eee50445d90946f76a3eca7612287.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/1*euP5XFBrgXWlKWHRNHtzAQ.gif"/></div></figure><p id="6aa3" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">如果是tails ( <strong class="ji hv">概率为(1-ei) </strong> )→我们使用前一时间步的输出。</p><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff mu"><img src="../Images/b10023cc5d3ffc17cf81092ce35f58bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tzsAetmEoUjv35-HbMl0Ww.gif"/></div></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">coin animation borrowed from google search results</figcaption></figure><p id="803e" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">凭直觉，我们可以有一个更好的方法。不仅仅是有一个常数<strong class="ji hv"> e </strong>，它也可以是可变的，就像在培训的开始，我们<strong class="ji hv">可以倾向于使用基础事实总结。</strong>在训练结束时<strong class="ji hv">，我们可以倾向于使用模型本身的输出</strong>，因为模型会学到更多。所以我们<strong class="ji hv">安排e </strong>(概率)的衰减。</p><figure class="ml mm mn mo fq iv fe ff paragraph-image"><div class="fe ff mw"><img src="../Images/6d120cc000e4ad80ca9c07c58eae1d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*P8Au5OdDlNzmFWWD264sPQ.png"/></div><figcaption class="jc jd fg fe ff je jf bd b be z ek">Borrowed from <a class="ae ke" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">bengio et ai</a> from google research</figcaption></figure><p id="a43f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">e本身的衰减可以是迭代次数的函数。</p><p id="ee95" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated"><strong class="ji hv">定期抽样一词由此而来。</strong></p><h1 id="6cc8" class="lh li hu bd lj lk mp lm ln lo mq lq lr ls mr lu lv lw ms ly lz ma mt mc md me dt translated">4.在Tensorflow中执行<strong class="ak">定时采样</strong></h1><p id="abda" class="pw-post-body-paragraph jg jh hu ji b jj mf jl jm jn mg jp jq jr mh jt ju jv mi jx jy jz mj kb kc kd hn dt translated"><a class="ae ke" href="https://github.com/yaserkl/RLSeq2Seq#scheduled-sampling-soft-scheduled-sampling-and-end2endbackprop" rel="noopener ugc nofollow" target="_blank">亚斯特克</a>在tensorflow中建立了一个很棒的库，使你能够实现多篇关于文本摘要的论文，其中一篇是(<a class="ae ke" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank">用递归神经网络进行序列预测的预定采样</a>)，我已经将其修改为在google colab上运行(<a class="ae ke" href="http://bit.ly/2HZP6aB" rel="noopener ugc nofollow" target="_blank">我的代码</a>)。</p><p id="1673" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">可以通过修改标志来调整库以实现多篇论文，这里(在<a class="ae ke" href="http://bit.ly/2HZP6aB" rel="noopener ugc nofollow" target="_blank">我的代码</a> jupyter notebook中)我已经修改了所需的标志，并且还启用了一个名为intradecoder的解码器版本(以限制单词重复)，所以您只需运行示例(使用设置的标志)。</p><p id="8af8" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">我们处理<a class="ae ke" href="https://github.com/abisee/cnn-dailymail" rel="noopener ugc nofollow" target="_blank"> CNN /每日新闻</a>的新闻数据。它是这个任务广泛使用的数据集，或者你可以直接从<a class="ae ke" href="https://drive.google.com/open?id=15c2wPpL4MGCooDx8Y1dw9M0o05P-EJto" rel="noopener ugc nofollow" target="_blank">我的google drive </a>复制数据集，到你自己的google drive(不需要下载然后上传)，无缝连接到你的google colab ( <a class="ae ke" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)。</p></div><div class="ab cl la lb hc lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hn ho hp hq hr"><p id="ad2f" class="pw-post-body-paragraph jg jh hu ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hn dt translated">下一次，我们将通过强化学习与深度学习的结合来解决暴露问题，以解决seq2seq遭受的另一个问题。</p><blockquote class="kt ku kv"><p id="3a6e" class="jg jh kw ji b jj jk jl jm jn jo jp jq kx js jt ju ky jw jx jy kz ka kb kc kd hn dt translated">我真心希望你喜欢阅读这篇教程，我希望我已经把这些概念讲清楚了。这一系列教程的所有代码都可以在<a class="ae ke" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到。您可以简单地使用google colab来运行它，请查看教程和代码并告诉我您对它的看法，不要忘记尝试免费的文本摘要生成<a class="ae ke" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>，希望再次见到您。</p></blockquote></div></div>    
</body>
</html>