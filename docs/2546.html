<html>
<head>
<title>Demystifying Different Variants of Gradient Descent Optimization Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘梯度下降优化算法的不同变体</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc#2019-04-22">https://medium.com/hackernoon/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc#2019-04-22</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="aa77" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">了解对梯度下降的不同改进，并使用 2D 等高线图比较它们的更新规则。</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/fc6536f9d94766f37562dec2d063556c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*locdfEJuknGNUEBF"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Photo by <a class="ae jz" href="https://unsplash.com/@willianjusten?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Willian Justen de Vasconcellos</a> on <a class="ae jz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="be3f" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">代表监督学习方法的神经网络需要包括目标变量在内的完整记录的大型训练集。训练深度神经网络以找到该网络的最佳参数是一个迭代过程，但是在大型数据集上迭代地训练深度神经网络是非常慢的。所以我们需要的是通过一个好的优化算法来更新网络的参数(权重和偏差)可以加快网络的学习过程。深度学习中优化算法的选择会影响网络的训练速度和性能。</p><p id="e074" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在本文中，我们将讨论改进梯度下降优化技术的必要性，我们还将讨论梯度下降优化算法的不同变体。</p><blockquote class="kw kx ky"><p id="afa8" class="ka kb kz kc b kd ke iv kf kg kh iy ki la kk kl km lb ko kp kq lc ks kt ku kv hn dt translated">引用说明:本文的内容和结构基于四分之一实验室的深度学习讲座— <a class="ae jz" href="https://padhai.onefourthlabs.in" rel="noopener ugc nofollow" target="_blank"> Padhai </a>。</p></blockquote><p id="425b" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">文章的其余部分结构如下，</p><ul class=""><li id="460b" class="ld le hu kc b kd ke kg kh kj lf kn lg kr lh kv li lj lk ll dt translated"><strong class="kc hv">梯度下降</strong></li><li id="35c6" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">动量梯度下降</strong></li><li id="8999" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">内斯特罗夫加速梯度下降</strong></li><li id="3891" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">小批量&amp;随机梯度下降</strong></li><li id="dd5f" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">自适应梯度下降— AdaGrad </strong></li><li id="7c33" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">均方根传播梯度下降— RMSProp </strong></li><li id="fbaa" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated"><strong class="kc hv">自适应矩估计— Adam </strong></li></ul></div><div class="ab cl lr ls hc lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hn ho hp hq hr"><h1 id="382c" class="ly lz hu bd ma mb mc md me mf mg mh mi ja mj jb mk jd ml je mm jg mn jh mo mp dt translated">梯度下降</h1><p id="9720" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">梯度下降是优化神经网络最常用的优化技术之一。梯度下降算法通过相对于网络参数在与目标函数的梯度相反的方向上移动来更新参数。用于单个 sigmoid 神经元梯度下降算法是这样工作的，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/6208a6cef0dde6271735b7276be5dc33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*YH_vv5-qf7UAwBHu4fJwOw.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Learning Algorithm</figcaption></figure><p id="2072" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">参数更新规则将由下式给出:</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mw"><img src="../Images/82a4a1053181af6c2e7f367baa3f522c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*232NNP054vumijGc3BFpVQ.png"/></div></div></figure><ul class=""><li id="8475" class="ld le hu kc b kd ke kg kh kj lf kn lg kr lh kv li lj lk ll dt translated">随机初始化参数<strong class="kc hv"> w </strong>和<strong class="kc hv"> b </strong>并迭代数据中的所有观察值</li><li id="226c" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">计算每个参数的梯度值(即，我们需要向哪个方向移动以减少损失)</li><li id="913c" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">根据每个参数的梯度值更新其值</li><li id="e788" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">继续执行步骤 2 和 3，直到损失函数最小化</li></ul><p id="be3c" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">学习率定义了我们达到最小值所走的步数。换句话说，它控制着我们应该以多快或多慢的速度收敛到最小值。</p><blockquote class="mx"><p id="eca7" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">能不能想出一个更好的更新规则？</p></blockquote><p id="f382" class="pw-post-body-paragraph ka kb hu kc b kd nh iv kf kg ni iy ki kj nj kl km kn nk kp kq kr nl kt ku kv hn dt translated">为了更好地理解梯度下降更新规则，我采用了一些玩具数据，并在所有数据点上迭代了 1000 个时期，并计算了不同值的<strong class="kc hv"> w </strong>和<strong class="kc hv">b</strong>的损失。一旦我获得了<strong class="kc hv"> w </strong>和<strong class="kc hv"> b </strong>的所有可能组合的损失值，我就能够生成一个动画来显示梯度下降规则的作用。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nm"><img src="../Images/53a964fab2d0ffbce6e365c5fb4a7be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*r5iF0ohxmlgIM1eqKqk5Qw.gif"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Gradient Descent Rule in Action (Animation)</figcaption></figure><p id="201b" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">图表中较深的红色阴影表示损失值较高的区域，而山谷中较深的蓝色阴影表示损失的最低点。底部的点表示<strong class="kc hv"> w </strong> &amp; <strong class="kc hv"> b </strong>(参数)的不同组合，轮廓上的点表示相应参数值的损失值。</p><p id="9460" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">从渐变下降损失动画中，您会观察到，对于初始迭代，当曲线仍然在平坦的浅红色表面上时，轮廓图底部的<strong class="kc hv"> w </strong>和<strong class="kc hv"> b </strong>值在每个时期变化很小。这意味着我们朝着我们的目标迈出了非常小的步伐，但是一旦我们的损失曲线进入每个时期的谷底(浅蓝色区域)，w 和 b<strong class="kc hv">的值就会发生巨大的变化(黑色机器人彼此相距很远)</strong></p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nn"><img src="../Images/42cc9e1d4caf0fc3967b9bd98dc6d86b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKIKJmkZvMmlronwPodb6g.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Small Steps towards Loss (Left) &amp; Large Steps towards Loss (Right)</figcaption></figure><p id="a792" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">简而言之，在损失表面的平缓区域，运动非常缓慢，在陡峭区域，运动很快，在底部的平缓区域，运动也非常缓慢。</p><h1 id="3244" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">但是为什么呢？</h1><p id="8f8c" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">我们已经对梯度下降更新规则进行了非常有趣的观察，但是我们不知道为什么在轮廓表面的某些部分移动很慢，而在某些部分移动很快。</p><p id="87ba" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">让我们看一个如下所示的简单函数，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nt"><img src="../Images/689d881ab5286446bdde17f5a8f0e911.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*oRFDMhqyTRk7Lt9TTDQHuQ.png"/></div></figure><p id="8e24" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">从图中可以看出，曲线是非常陡峭区域和平缓区域的组合。陡峭区域中曲线的斜率(导数)评估为值 3(y 的变化= 3，x 的变化= 1)。类似地，平缓区域中曲线的斜率评估为值 1。因此，我们推断，当斜率非常陡时，导数(梯度)高，当斜率平缓时，导数低。</p><p id="1068" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">陡坡处的导数数值较大，而缓坡处的导数数值较小。现在将这些信息与梯度下降动画联系起来，当我们处于缓坡的平台时，该区域的导数将会很小。如果导数很小，那么参数的更新也会很小。</p><blockquote class="mx"><p id="9435" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">因此，在曲线平缓的区域，更新较小，而在曲线陡峭的区域，更新较大</p></blockquote><h1 id="9b60" class="ly lz hu bd ma mb no md me mf np mh mi ja nu jb mk jd nv je mm jg nw jh mo mp dt translated">那又怎样？</h1><p id="071c" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">我们已经找出了梯度下降更新在轮廓的某些区域移动缓慢而在某些区域移动较快的原因。更新规则中的这种移动有什么问题？</p><p id="ac9d" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">问题是，在梯度下降中，我们随机初始化参数，如果发生这种情况，我们在表面非常平缓的地方初始化<strong class="kc hv"> w </strong>和<strong class="kc hv"> b </strong>，那么我们需要运行许多许多个时期，以便从平坦的表面出来，并从那里进入稍微陡峭的区域，您将开始看到损失函数的一些改进。为了避免这种情况，我们需要改进我们的更新规则。</p><h1 id="5930" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">等高线地图</h1><p id="9465" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">在我们继续查看梯度下降更新规则的一些改进之前，我们将首先绕一小段路，并了解如何解释等值线图。等高线图有助于我们使用等高线或颜色编码区域在二维空间中可视化 3D 数据。等值线图根据 2D 等值线图中相应等值线之间的距离来显示坡度沿特定方向的移动。</p><ul class=""><li id="fcd3" class="ld le hu kc b kd ke kg kh kj lf kn lg kr lh kv li lj lk ll dt translated">等高线之间的小距离表示沿该方向的陡坡。</li><li id="f94b" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">等高线之间的大距离表示沿该方向的缓坡。</li></ul><h1 id="cc1e" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">使用等高线图可视化梯度下降</h1><p id="0cc0" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">梯度下降误差表面的 3D 表示看起来像这样，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff nx"><img src="../Images/5fdfb4d11751a631f520f3fe009ab046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BriRrnqNJ_1TRVKwBFhdaA.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Gradient Descent Convergence in 3D</figcaption></figure><p id="f32a" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们可以从上图中注意到，随着曲线到达更陡峭的表面，它朝着用稀疏点表示的收敛迈出了越来越大的步伐。一旦曲线到达深蓝色平坦区域，该方向上的斜率变缓，这意味着深蓝色区域中的连续轮廓之间的距离将变小。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ny"><img src="../Images/46e71a4f23ca1799a5dea2ba2d07104a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*JDog6Qk_lZZYc0EgDW7Txg.gif"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Gradient Descent Animation (Contour Map)</figcaption></figure><p id="afca" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">正如您从动画中看到的，一旦曲线到达浅蓝色等高线簇，连续等高线之间的距离越来越小，曲线就越来越接近收敛。</p><h1 id="0315" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">基于动量的梯度下降</h1><p id="5c87" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">从我们对梯度下降的讨论中可以清楚地看出，由于这些区域中的梯度非常小，所以需要花费大量的时间来通过具有缓坡的区域。我们如何克服这一点？</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff nz"><img src="../Images/d3059e3d9c22b3c3883a32c7cf9efb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*C7QS71YPk2NAGoIENVzJrg.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">More confidence</figcaption></figure><p id="9fdd" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们来考虑一个情况，你要去一个未知区域的一个最近开业的商场。当你试图找到购物中心时，你问了很多人关于购物中心的位置，每个人都指引你去同一个位置。因为每个人都把你指向同一个方向，所以你会越来越自信地朝着那个方向前进。现在我们将在基于动量的梯度下降中使用相同的直觉，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oa"><img src="../Images/77b12eb34695d7542c7f4a4f6afcded7.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*CfP-qLaE7BPjboCPMdoGrA.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Momentum GD</figcaption></figure><p id="9a92" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在基于动量的梯度下降更新规则中，我们还包括历史组件 vₜ，它存储直到这个时间 t 的所有先前的梯度运动</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ob"><img src="../Images/d233d9f928e64270fc1dcb4d265ea04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmtYykikylF4L1W0N1G43w.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Exponential Weighted Average</figcaption></figure><p id="ad93" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在每一个时间步或每一次更新，而不是像普通的 GD 一样，只移动当前梯度的值。在动量 GD 中，我们以先前梯度和当前梯度的指数衰减累积平均值移动。现在让我们看看动量 GD 在相同的误差曲面上的表现，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oc"><img src="../Images/8b493a0194a0972f694ca11b3b1b69e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/1*pM98p3uZrAQ2DUTge3mTEA.gif"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Momentum-based GD Animation</figcaption></figure><p id="7aad" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">从动画中可以明显看出，基于动量的梯度下降在平面上比普通的梯度下降移动得更快，因为它不仅使用当前的梯度值来更新参数，还使用到该点为止的梯度值的历史记录。当它在平面上走了大约 5 到 6 步时，它的历史会发展到一定程度，允许动量梯度下降在平面上走越来越大的步。</p><blockquote class="mx"><p id="ed82" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">动作快总是好的吗？</p></blockquote><p id="e601" class="pw-post-body-paragraph ka kb hu kc b kd nh iv kf kg ni iy ki kj nj kl km kn nk kp kq kr nl kt ku kv hn dt translated">为了分析基于动量的梯度下降在不同误差表面上的局限性，我采用了一些其他的玩具数据集，并使用单个 sigmoid 神经元来寻找在<strong class="kc hv"> w </strong>和<strong class="kc hv"> b </strong>的不同组合下的损失值，以生成具有更窄的最小表面的 3D 损失表面。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff od"><img src="../Images/655b4b3a950acfc148996fa1b735b9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*WAa92OBQHZ-vvUs5pGaaZg.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Momentum-based GD convergence</figcaption></figure><p id="664e" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">基于动量的梯度下降在极小值内外振荡，因为当它达到极小值时，它已经积累了更多的历史，导致采取越来越大的步骤，显然导致超过目标。尽管有 u 形转弯，它仍然比普通梯度下降收敛得更快。</p><h1 id="4f29" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">内斯特罗夫加速梯度下降</h1><p id="fa7b" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">如何才能避免极小值的超调？。</p><p id="f820" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在基于动量的梯度下降中，我们在梯度历史的方向上做一个运动，在当前梯度的方向上做另一个运动，因为这个两步运动，我们超过了最小值。与其一次移动两步，为什么不先在梯度历史的方向上移动一点，计算该点的梯度并更新参数。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oe"><img src="../Images/8a5aec8deff7781e7a6b7c40690ac190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*EyCpeHyZf4H_kixkXgcCRg.png"/></div></figure><p id="325f" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">本质上，我们在内斯特罗夫加速梯度下降中所做的是，在我们基于当前梯度值采取另一步骤之前，我们期待看到我们是否接近最小值，以便我们可以避免超调的问题。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff of"><img src="../Images/a1ce36ca4489799d23100a4bb475a2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4vkGwOHFvU0NBwWwM3PbA.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Comparison between NAG(Red curve) &amp; Momentum(Black curve)</figcaption></figure><p id="dead" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们可以看到，由内斯特罗夫加速梯度引起的所有振荡，都比基于动量梯度下降的振荡小得多。与基于动量的梯度下降相比，向前看有助于 NAG 更快地纠正其路线。因此，振荡更小，逃离极小值谷的机会也更小。</p><h1 id="c0fd" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">小批量和随机梯度下降</h1><blockquote class="mx"><p id="aa71" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">我们应该使用全部数据来计算梯度吗？</p></blockquote><p id="6fcf" class="pw-post-body-paragraph ka kb hu kc b kd nh iv kf kg ni iy ki kj nj kl km kn nk kp kq kr nl kt ku kv hn dt translated">假设您有一个包含百万个数据点的庞大数据集，如果我们使用批量梯度，该算法将进行一百万次计算(计算百万个数据点中每个点的导数并累加所有这些导数)，然后对我们的参数进行一次微小的更新。</p><blockquote class="mx"><p id="4a55" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">我们能做得更好吗？</p></blockquote><p id="8b37" class="pw-post-body-paragraph ka kb hu kc b kd nh iv kf kg ni iy ki kj nj kl km kn nk kp kq kr nl kt ku kv hn dt translated">我们不是一次性查看所有数据点，而是将整个数据分成若干子集。对于每个数据子集，计算子集中每个点的导数，并更新参数。我们没有根据损失函数计算整个数据的导数，而是将其近似为更少的点或更小的批量。这种批量计算梯度的方法称为<strong class="kc hv">小批量梯度下降</strong>。如果我们有 100 万个批量大小为 10 的数据点，我们将有 100，000 个批量，我们将对参数进行 100，000 次更新。</p><p id="d306" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">如果我们将小批量梯度下降的想法发挥到极致，一次查看一个点，计算导数并更新每个点的参数。这种方法叫做<strong class="kc hv">随机梯度下降</strong>。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff og"><img src="../Images/abd4ee07793621c9f8c83d293aabb712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*xhvhXut4Sgo9nBjmRpC4oQ.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Stochastic Gradient Descent Convergence</figcaption></figure><p id="8c2a" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在随机模型中，我们更新每个点的参数，这些点并不一致。每个点都做出贪婪的决定，并更新最适合该点的参数，这可能是其余点的情况。我们可以通过考虑数据子集或数据批次来减少振荡问题，并在计算该批次内每个点的导数后更新参数。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff oh"><img src="../Images/27953d255c5a5e8f072965746ed63aa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJ9aOHYWDU9w7U-ueD2sfQ.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Comparison of Stochastic (Blue Curve) &amp; Mini-Batch GD (Red Curve)</figcaption></figure><p id="205e" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">我们可以看到，在小批量梯度下降中，振荡已经减少，振荡完全包含在蓝色曲线内。实际上，我们选择的批量大小等于 16、32 和 64。无论我们使用小批量还是随机梯度下降，要注意的关键点是，我们是在逼近损失函数的导数，而不是计算损失函数的真实导数。</p><p id="f7b3" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在一个时期内对参数进行更新(步骤)的次数，</p><ul class=""><li id="c995" class="ld le hu kc b kd ke kg kh kj lf kn lg kr lh kv li lj lk ll dt translated">批量梯度下降— 1</li><li id="6181" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">随机梯度下降— N(数据点的数量)</li><li id="a24e" class="ld le hu kc b kd lm kg ln kj lo kn lp kr lq kv li lj lk ll dt translated">小批量梯度下降— N/B(B =批量)</li></ul><h1 id="3db4" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">阿达格拉德</h1><p id="2ed7" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">AdaGrad —具有自适应学习速率的梯度下降</p><p id="682d" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">AdaGrad 背后的主要动机是针对数据集中不同要素的自适应学习率的想法，即，我们可能需要针对不同要素的不同学习率，而不是针对数据集中的所有要素使用相同的学习率。</p><blockquote class="mx"><p id="eabf" class="my mz hu bd na nb nc nd ne nf ng kv ek translated">为什么我们需要自适应学习率？</p></blockquote><p id="339f" class="pw-post-body-paragraph ka kb hu kc b kd nh iv kf kg ni iy ki kj nj kl km kn nk kp kq kr nl kt ku kv hn dt translated">考虑具有非常重要但稀疏的变量的数据集，如果该变量在大多数训练数据点中为零，则与该变量成比例的导数也将等于零。如果导数等于零，那么权重更新将为零。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oi"><img src="../Images/14d61ce0f7fcfad7e203c7fae9b963bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*qt6Ah0VTj5eOn97cZ66nmQ.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">if x = 0 then gradient of w is zero</figcaption></figure><p id="a844" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">如果我们的参数(权重)没有向最小值移动，那么模型将不会做出最佳预测。为了帮助这种稀疏特征，我们希望确保无论何时该特征值不为零，无论该点的导数是什么，它都应该通过更大的学习率来提高。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oj"><img src="../Images/d2b23b82f07b6b2e698530ec49153c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*KMystkKEMpKZ8J2lkg94tw.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">AdaGrad Intuition</figcaption></figure><p id="e054" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">如果一个特定的参数经常更新，这意味着导数大部分时间不为零，对于这样的参数，我们希望有一个较小的学习率，相反，如果我们有一个稀疏的参数，它将在大部分时间关闭(零)，这意味着导数在大部分时间为零。对于这样的特征，每当该特征开启(非零)时，我们希望以更高的学习率来促进梯度更新。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ok"><img src="../Images/79dc285a3c09b401dabf482f6a18cd23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*zKXHX8zZhzX2ust-Ua9ICQ.png"/></div></figure><p id="4c59" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在 AdaGrad 中，我们用梯度值的历史来划分学习，直到该点。非稀疏特征将具有大的历史值，因为它们将得到频繁的更新，通过将学习率除以大的历史，有效的学习率将非常小。在稀疏特征的情况下，梯度历史值将非常小，导致大的有效学习率。</p><p id="b420" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">考虑我们有两个特征的数据<strong class="kc hv"> w </strong>(稀疏特征)<strong class="kc hv"> </strong>和<strong class="kc hv"> b </strong>，这意味着<strong class="kc hv"> w </strong>经历较少的更新。现在我们将 AdaGrad 更新规则与 Vanilla GD(黑色曲线)、Momentum GD(红色)、NAG(蓝色)和 AdaGrad(绿色)进行比较</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ol"><img src="../Images/e5b5adc70a3d5cfeca4475e6b546ac74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XnQhZfgEO5kLJiNvE_tJQ.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Comparison of different GD variants</figcaption></figure><p id="2350" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">从上面的图中，我们可以看到 AdaGrad 在<strong class="kc hv"> w </strong>的方向上迈出了更大的步伐，尽管它与其他变体相比是一个稀疏的特征，因为它的梯度值是由学习速率提高的。AdaGrad 的缺点是学习速率随着分母的增长而急剧衰减，这对于对应于密集特征的参数是不利的。随着<strong class="kc hv"> b </strong>一次又一次地更新，分母增加了很多，有效学习率变得接近于零，因此，AdaGrad 不再能够向<strong class="kc hv"> b </strong>的方向移动，并且卡在收敛点附近。</p><h1 id="676a" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">RMSProp</h1><p id="95cd" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">RMSProp —均方根传播</p><p id="7306" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">为了防止分母快速增长，为什么不衰减分母并防止它快速增长呢？</p><p id="1d63" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">RMSProp 利用这种直觉来防止密集变量分母的快速增长，从而使有效学习率不会接近于零。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff om"><img src="../Images/e501f6411781306fcfd8cd2b47965eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*9v4BxT8pWHwJfbNXGqi7lQ.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">RMSProp Equation</figcaption></figure><p id="782f" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">与 AdaGrad 中的梯度总和不同，RMSProp 中的梯度历史是使用指数衰减平均值计算的，这有助于防止密集要素分母的快速增长。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff on"><img src="../Images/c8dbe6fa27f17033f1d06eb8a804dd7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cumpJTEpb0dYXW1WW9MA1w.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">RMSProp Brown Curve Next to AdaGrad Green Curve</figcaption></figure><p id="1fce" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">由于密集特征'<strong class="kc hv"> b </strong>'的分母不像 AdaGrad 那样急剧衰减，RMSProp 能够向<strong class="kc hv"> b </strong>方向移动，最终导致收敛。</p><h1 id="f2d0" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="eb4c" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">Adam 这个名字来源于自适应矩估计</p><p id="af4e" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在基于动量的梯度下降中，我们使用梯度的累积历史在平缓的表面上移动得更快，我们已经看到 RMSProp 也使用历史来衰减分母并防止其快速增长。这些算法使用历史的方式是不同的，在 Momentum GD 中，我们使用历史来计算当前更新，而在 RMSProp 中，使用历史来调整学习率(收缩或增强)。</p><p id="ecf2" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">亚当将这两个独立的历史结合成一个算法。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff oo"><img src="../Images/2aee5ee1a545083031a9d66b26aeb00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*t4Y72jekWclyPpAENAIaAQ.png"/></div></figure><p id="ee76" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">Adam 维护了两个历史，'mₜ'类似于 Momentum GD 中使用的历史，'vₜ'类似于 RMSProp 中使用的历史。实际上，亚当做了一些被称为偏差修正的事情。它对'mₜ'和'vₜ'使用以下等式，</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff op"><img src="../Images/ef873148d53968715160861f2f29d55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*fOF5P0CWqI0yBWNrtngOxg.png"/></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Bias Correction</figcaption></figure><p id="5d5d" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">偏差校正确保在训练更新开始时不会以奇怪的方式运行。Adam 中的关键点在于它结合了 Momentum GD(在平缓区域移动更快)和 RMSProp GD(调整学习速率)的优点。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff oq"><img src="../Images/475e9b2560e47e07297a53db7eb9473c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEiNTT0vEKL8Eht_p5Zn9w.png"/></div></div><figcaption class="jv jw fg fe ff jx jy bd b be z ek">Adam Optimizer (cyan color curve)</figcaption></figure><p id="699c" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">因为 Adam 具有动量 GD 的性质，我们可以看到它超过了最小值，然后回来，在收敛之前做了几个 u 形转弯。</p></div><div class="ab cl lr ls hc lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hn ho hp hq hr"><h1 id="caed" class="ly lz hu bd ma mb mc md me mf mg mh mi ja mj jb mk jd ml je mm jg mn jh mo mp dt translated">摘要</h1><p id="0db0" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">在这篇文章中，我们讨论了批量梯度下降，开发新的优化技术的必要性，然后我们简要讨论了如何解释等高线图。之后，我们看了六种不同的优化技术和三种不同的数据策略(批量、小批量和随机)，直观的理解有助于了解在哪里使用这些算法。在实践中，具有 32、64 和 128 大小的小批量的 Adam optimizer 是默认选择，至少对于处理 CNN 和大序列到序列模型的所有图像分类任务是如此。</p><h1 id="789d" class="ly lz hu bd ma mb no md me mf np mh mi ja nq jb mk jd nr je mm jg ns jh mo mp dt translated">下一步是什么？</h1><p id="ee1b" class="pw-post-body-paragraph ka kb hu kc b kd mq iv kf kg mr iy ki kj ms kl km kn mt kp kq kr mu kt ku kv hn dt translated">反向传播是神经网络如何学习它们所学内容的支柱。如果你有兴趣了解更多关于神经网络的知识，请查看来自<a class="ae jz" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank"> Starttechacademy </a>的 Abhishek 和 Pukhraj 的<a class="ae jz" href="https://courses.starttechacademy.com/full-site-access/?coupon=NKSTACAD" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>。本课程将使用最新版本的 Tensorflow 2.0 (Keras 后端)进行教学。</p></div><div class="ab cl lr ls hc lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hn ho hp hq hr"><p id="d5a3" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated"><em class="kz">推荐阅读</em></p><p id="0c08" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">如果你想理解梯度下降更新规则背后的数学原理，</p><div class="or os fm fo ot ou"><a href="https://towardsdatascience.com/sigmoid-neuron-learning-algorithm-explained-with-math-eb9280e53f07" rel="noopener follow" target="_blank"><div class="ov ab ej"><div class="ow ab ox cl cj oy"><h2 class="bd hv fv z el oz eo ep pa er et ht dt translated">用数学解释的 Sigmoid 神经元学习算法</h2><div class="pb l"><h3 class="bd b fv z el oz eo ep pa er et ek translated">在本帖中，我们将详细讨论 sigmoid 神经元学习算法背后的数学直觉。</h3></div><div class="pc l"><p class="bd b gc z el oz eo ep pa er et ek translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi jt ou"/></div></div></a></div><p id="29b3" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">一篇关于如何解读等高线图的深度文章</p><div class="or os fm fo ot ou"><a href="https://hackernoon.com/how-to-interpret-a-contour-plot-a617d45f91ba" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab ej"><div class="ow ab ox cl cj oy"><h2 class="bd hv fv z el oz eo ep pa er et ht dt translated">如何解释等高线图</h2><div class="pb l"><h3 class="bd b fv z el oz eo ep pa er et ek translated">在 2D 等高线图中可视化三维表面</h3></div><div class="pc l"><p class="bd b gc z el oz eo ep pa er et ek translated">hackernoon.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi jt ou"/></div></div></a></div><p id="d551" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">在我的下一篇文章中，我们将讨论如何在 python 中实现优化算法并可视化它们的更新规则。因此，请确保您跟随媒体上的<a class="ae jz" rel="noopener" href="/@niranjankumarc"> me </a>，以便在它下跌时得到通知。</p><p id="28e0" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">直到那时和平:)</p><p id="55cb" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated">NK。</p></div><div class="ab cl lr ls hc lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hn ho hp hq hr"><p id="3b63" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated"><a class="pk pl gr" href="https://medium.com/u/3e4fb2985698?source=post_page-----19ae9ba2e9bc--------------------------------" rel="noopener" target="_blank"> Niranjan Kumar </a>是汇丰银行分析部门的实习生。他对深度学习和人工智能充满热情。他是<a class="pk pl gr" href="https://medium.com/u/504c7870fdb6?source=post_page-----19ae9ba2e9bc--------------------------------" rel="noopener" target="_blank">媒体</a>人工智能<a class="ae jz" rel="noopener" href="/tag/artificial-intelligence/top-writers">的顶级作家之一。在 LinkedIn</a><a class="ae jz" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank">上与我联系，或者在 Twitter</a><a class="ae jz" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank">上关注我，了解关于深度学习和人工智能的最新文章。</a></p><p id="649d" class="pw-post-body-paragraph ka kb hu kc b kd ke iv kf kg kh iy ki kj kk kl km kn ko kp kq kr ks kt ku kv hn dt translated"><strong class="kc hv">免责声明</strong> —这篇文章中可能有一些相关资源的附属链接。你可以以尽可能低的价格购买捆绑包。如果你购买这门课程，我会收到一小笔佣金。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="pm pn l"/></div></figure></div></div>    
</body>
</html>