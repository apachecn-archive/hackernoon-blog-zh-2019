<html>
<head>
<title>Beam Search &amp; Attention for text Summarization made Easy (Tutorial 5)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">光束搜索和注意文本摘要变得容易(教程5)</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086?source=collection_archive---------0-----------------------#2019-04-06">https://medium.com/hackernoon/beam-search-attention-for-text-summarization-made-easy-tutorial-5-3b7186df7086?source=collection_archive---------0-----------------------#2019-04-06</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/52026801a4962d4934d7d00deaf1235b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-2JTAMKKuFGzTOjOvMaEg.png"/></div></div></figure><p id="6387" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">本教程是一系列教程中的第五个，将帮助您使用tensorflow构建一个抽象的文本摘要器，今天我们将讨论对核心RNN seq2seq模型的一些有用的修改，我们在上一个教程中已经介绍过<a class="ae ka" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f"/></p><p id="cf0c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这些修改是</p><ol class=""><li id="1669" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated"><strong class="je hv">光束搜索</strong></li><li id="4700" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><strong class="je hv">注意力模式</strong></li></ol></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h2 id="0ee5" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">关于系列</h2><p id="2676" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">这是一系列教程，将帮助您使用tensorflow使用多种方法构建一个抽象的文本摘要器，<strong class="je hv"> <em class="lw">您不需要下载数据，也不需要在您的设备</em> </strong>上本地运行代码，因为<strong class="je hv">数据</strong>可以在<strong class="je hv"> google drive </strong>上找到，(您可以简单地将其复制到您的google drive，在此了解更多<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank"/>，本系列的<strong class="je hv">代码</strong>是用Jupyter编写的</p><p id="5b9b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">到目前为止我们已经讨论过了(这个系列的代码可以在<a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank">这里</a>找到)</p><p id="52c0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">0.<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">深度学习免费生态系统概述</a>(如何使用google colab和google drive)</p><ol class=""><li id="4346" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/text-summarizer-using-deep-learning-made-easy-490880df6cd" rel="noopener ugc nofollow" target="_blank">概述文本摘要任务和用于该任务的不同技术</a></li><li id="0ee3" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">使用的数据以及如何表示我们的任务</a></li><li id="7bcc" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">什么是seq2seq文本摘要，为什么</a></li><li id="8e99" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated"><a class="ae ka" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">多层双向LSTM/GRU </a></li></ol><p id="1ec0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们开始吧</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="ab fr cl mb"><img src="../Images/16577fbdd2beff9f3d1dcb3f38417358.png" data-original-src="https://miro.medium.com/v2/format:webp/1*f1B-cGJMsFxL1gZ51ZPGlA.jpeg"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">EazyMind free Ai-As-a-service for text summarization</figcaption></figure><p id="8b81" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我在一个网站上添加了一个文本摘要模型<a class="ae ka" href="http://bit.ly/2VxhPqU" rel="noopener ugc nofollow" target="_blank"> eazymind </a>，这样你就可以自己尝试生成你自己的摘要(看看你能构建什么)，它可以通过简单的api调用来调用，并且通过<a class="ae ka" href="http://bit.ly/2Ef5XnS" rel="noopener ugc nofollow" target="_blank"> python包</a>，这样文本摘要就可以很容易地集成到你的应用程序中，而不需要设置tensorflow环境的麻烦，你可以免费注册，并享受免费使用这个api的乐趣。</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h1 id="fd4c" class="mg kx hu bd ky mh mi mj lc mk ml mm lg mn mo mp lj mq mr ms lm mt mu mv lp mw dt translated">快速回顾</h1><p id="adf6" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">我们的任务是文本摘要，我们称之为抽象，因为我们教神经网络生成单词，而不仅仅是复制单词。</p><p id="990e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">将被使用的数据将是新闻和它们的标题，可以在我的google drive上找到，所以你只需将它复制到你的google drive上，而不需要下载它</p><p id="2ee6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们将使用单词嵌入来表示数据，这只是简单地将每个单词转换成一个特定的向量，我们将为我们的单词创建一个字典(<a class="ae ka" href="https://hackernoon.com/abstractive-text-summarization-tutorial-2-text-representation-made-very-easy-ef4511a1a46" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)</p><p id="865a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">对于这项任务，有<a class="ae ka" href="https://hackernoon.com/begin-your-deep-learning-project-for-free-free-gpu-processing-free-storage-free-easy-upload-b4dba18abebc" rel="noopener ugc nofollow" target="_blank">种不同的方法</a>，它们建立在一个基石概念之上，并且它们继续开发和建立，它们从一种称为RNN的网络开始，这种网络被安排在一种称为seq2seq的编码器/解码器架构中(<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0" rel="noopener ugc nofollow" target="_blank">更多关于此</a>)，然后我们将在一个多层双向结构中建立seq2seq，其中rnn小区将是一个LSTM小区(<a class="ae ka" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">更多关于此</a>)，这些不同方法的代码可以在这里找到<a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"/></p><blockquote class="mx my mz"><p id="6b92" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated"><em class="hu">本教程一直基于</em> <strong class="je hv"> <em class="hu">吴君如</em></strong><em class="hu"/><a class="ae ka" href="https://www.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt" rel="noopener ugc nofollow" target="_blank"><em class="hu">的惊人之作，他关于RNN </em> </a> <em class="hu">的课程确实很有用，推荐你去看一下</em></p></blockquote></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><p id="dead" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt nd translated"><span class="l ne nf ng bm nh ni nj nk nl di"> T </span>今天，我们将对编码器/解码器模型的核心组件进行一些修改，这些修改有助于网络从不同的可能性池中选择最佳结果，这被称为<strong class="je hv">波束搜索</strong>，我们还将讨论<strong class="je hv">注意力模型</strong>，这是一个简单的网络，添加到我们的架构中，帮助它更多地关注特定的单词，以帮助它更好地输出摘要。</p><p id="aafb" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以让我们开始吧！！</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h1 id="450d" class="mg kx hu bd ky mh mi mj lc mk ml mm lg mn mo mp lj mq mr ms lm mt mu mv lp mw dt translated">1.波束搜索</h1><h2 id="5e93" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">1.一种直觉(为什么要光束搜索)</h2><p id="cfc9" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">我们的文本摘要任务可以看作是一个条件语言模型，这意味着我们在给定一个输入句子的情况下生成一个输出，因此输出是以输入句子为条件的，所以这就是为什么它被称为条件。</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nm"><img src="../Images/1173b5bb5a0c3db91d8fe6db7c120e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*ZzFYOZ_aCNPKQ65KRWleXA.jpeg"/></div></figure><p id="17a9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这可以被视为与简单的语言模型相反，因为正常的语言模型只输出某个句子的概率，这可以用于生成新颖的句子，(如果它的输入可以是零)，但在我们的情况下，它会选择给定输入句子的最可能的输出。</p><p id="c60a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以我们的架构实际上分为两个部分，一个编码器和一个解码器(就像这里讨论的<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank"/>)，因为编码器将输入句子表示为一个向量，并将其传递给架构的下一部分(解码器)</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nn"><img src="../Images/78f29334e53d60ccd660f57f52068e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M74IebYnMCju85hk1dvwgA.jpeg"/></div></div></figure><p id="0208" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但是在解码器中会出现选择哪个句子的问题，因为对于某个输入句子可能有大量的输出</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nn"><img src="../Images/f07b9b33d780c4db626468330cb7ebae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkGBXSjUrzwf3Sur7jUD1Q.jpeg"/></div></div></figure><p id="7674" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">那么如何从这一大堆可能的输出中选择最有可能的句子呢？</p><ol class=""><li id="1ccf" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">简将在九月访问非洲</li><li id="e8b0" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">简将在九月访问非洲</li><li id="5c4f" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">九月份简将访问非洲</li></ol><p id="f75d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">一种解决方案是一次生成一个单词，生成第一个最可能的单词，然后生成下一个最可能的单词，如此等等，这被称为<strong class="je hv">贪婪搜索</strong>，但这往往不是最佳的方法，原因有二</p><ol class=""><li id="40c3" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">实际上，贪婪搜索的表现并不好</li><li id="2ce4" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">在每一步中，我们必须考虑字典中所有单词的所有可能性，如果你有10k个单词，那么在每一步中，你必须考虑10k^ 10k (10k的10k次方)</li></ol><p id="89a9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，我们将采用更优化的近似搜索方法<strong class="je hv"> →波束搜索</strong></p><h2 id="22dd" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">1.如何进行波束搜索</h2><p id="1862" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">现在我们已经了解了波束搜索背后的基本直觉，那么如何实际实现呢？</p><p id="8754" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">简单地说，波束搜索不同于基本的贪婪搜索<strong class="je hv">，它在每一步都考虑多个选项，而不仅仅是一个选项</strong>，这些选项的数量由一个叫做<strong class="je hv">波束宽度</strong>的变量控制</p><p id="f68a" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在贪婪搜索中:</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff no"><img src="../Images/507ac818546b08b046a2b0a8a7eb7662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQl7WobOMwYcTpN4pLyvWw.png"/></div></div></figure><p id="5ab3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在每一步中，我们只考虑每一步最可能的输出，但这并不总是保证最佳的解决方案</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff np"><img src="../Images/d75a3e687c65cbf3a422fd612a2f7356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2L9oJPMROaR8y9XRkOTL9w.png"/></div></div><figcaption class="mc md fg fe ff me mf bd b be z ek">greedy search would output that the best solution is Jane is going , while Jane is visiting is better output</figcaption></figure><p id="5cde" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这是因为在“is”之后的单词“going”的概率大于在“is”之后的单词“visiting ”,所以这就是为什么我们需要考虑另一种方法而不是贪婪搜索，一种将多个单词考虑在内的方法</p><p id="8f48" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，在波束搜索中，它将是<strong class="je hv">(波束宽度=3) </strong></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nq"><img src="../Images/2950ce3779e19cc40d10c8fb3aab3558.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/1*D5iegzANiGL5jcc_j2fCLg.gif"/></div></figure><p id="e1d9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在步骤1中，我们将选择3个最可能的单词，然后对于3个选择的单词中的每一个，我们将得到最可能的3个单词，然后在这3个输出句子上发生相同的逻辑，直到我们建立我们的最佳输出。</p><p id="c91e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，如果我们的词汇是10k，对于每一步，我们试图从10k词汇中寻找最有可能的单词，因此我们只考虑30k单词</p><h2 id="735e" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">1.波束宽度的影响</h2><p id="19d8" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">当波束宽度= 3时→每步考虑3个字</p><p id="553c" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">当波束宽度= 1时→每步考虑1个字→贪婪搜索</p><p id="5e5e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">随着波束宽度的增加→更好的结果→但是需要更多的计算资源</p></div><div class="ab cl kp kq hc kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hn ho hp hq hr"><h1 id="f1df" class="mg kx hu bd ky mh mi mj lc mk ml mm lg mn mo mp lj mq mr ms lm mt mu mv lp mw dt translated">2.注意力模型</h1><h2 id="a3e9" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">2.注意力直觉</h2><p id="afb2" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">当我们作为人类总结文本时，我们实际上一次只看几个词，而不是在给定的情况下总结整个文本，这是我们试图教给我们的模型的。</p><p id="e752" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们试图教会我们的模型只注意相邻的词而不是整篇文章，但是这种注意的作用是什么，我们其实也不知道！！这就是为什么我们要为这个任务构建一个简单的神经网络。</p><h2 id="0969" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">2.注意力结构</h2><p id="4026" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">在这里，我们将致力于我们的seq2seq编码器解码器结构(<a class="ae ka" href="https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0?source=post_stats_page---------------------------" rel="noopener ugc nofollow" target="_blank">更多关于这个</a>)，我们将致力于一个双向编码器(<a class="ae ka" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f">更多关于这个</a>)，我们的工作将实际上发生在编码器和解码器之间的一个新接口上，这个新接口被称为上下文向量，它实际上代表了给予单词的关注量</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff np"><img src="../Images/6225ef08a6e5fb7e6878c78bd431fa83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*62wtKuf3iytsUcP7cJrybg.png"/></div></div></figure><p id="458e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">编码器应该是</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nr"><img src="../Images/41e7f29bd273c0cb8fd367a103aaa345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KxBDcy6NSbPQIuscSvT1jg.png"/></div></div></figure><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ns"><img src="../Images/a758b36a45557829f825ef6405349b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1C6NhrEpHPO_wUHzT5ZBQ.png"/></div></div></figure><p id="20fc" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因此，为了计算作为解码器中第一个单词的输入的上下文C1，我们将计算多个阿尔法参数，因此现在我们需要知道如何获得这些阿尔法参数</p><h2 id="b0f5" class="kw kx hu bd ky kz la lb lc ld le lf lg jn lh li lj jr lk ll lm jv ln lo lp lq dt translated">2.计算注意力参数(上下文和alpha)</h2><p id="09ad" class="pw-post-body-paragraph jc jd hu je b jf lr jh ji jj ls jl jm jn lt jp jq jr lu jt ju jv lv jx jy jz hn dt translated">既然我们已经通过上下文向量了解了编码器和解码器之间的接口，而上下文向量实际上是通过attention alpha参数计算的，那么我们需要知道更多关于如何计算的信息</p><p id="4f43" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先:让我们来看看如何计算上下文向量</p><p id="9812" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">别忘了我们正在开发双向编码器，所以激活实际上分为左右两部分(更多关于这个的<a class="ae ka" rel="noopener" href="/@theamrzaki/multilayer-bidirectional-lstm-gru-for-text-summarization-made-easy-tutorial-4-a63db108b44f"/></p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ns"><img src="../Images/c239bfcd539191588f4519a161a04ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sY_c1qZyQG0Jg0BSz2ITOA.png"/></div></div></figure><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nt"><img src="../Images/e91fe1b576d290fda4684ec1c7e6518d.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*JlU0J9JFK8cvWwZviQCROA.png"/></div><figcaption class="mc md fg fe ff me mf bd b be z ek">so the attention would be</figcaption></figure><p id="c842" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以注意力有左右两部分，形成每个细胞的激活参数</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/ef923ef537d9681f36a4b685a48e587f.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*Q_I-BozeyoN-hvUQtQI7JQ.png"/></div></figure><p id="cec0" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">那么上下文将是激活的加权和(t代表编码器中的单元数),其中权重是阿尔法参数，这将发生在每个上下文向量中</p><p id="7da4" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">第二:</strong>我们需要知道如何计算阿尔法本身</p><p id="4cde" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">阿尔法群岛</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nv"><img src="../Images/22b6ba69bba89c6c9faea48421497102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4rnRUNNzrHUJooVYHGpww.png"/></div></div></figure><blockquote class="mx my mz"><p id="626a" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated">t代表输出中的时间步长，而t '代表输入中的时间步长</p><p id="0db3" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated">所有阿尔法的和是1</p></blockquote><p id="cd67" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">有一个公式确保总和为1</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nu"><img src="../Images/8532a19eebaf63138a6d0152a8d8a67a.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*_8xHCNtVo7HhPy9cvRMzSA.png"/></div></figure><p id="cd7d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">那么我们只需要知道什么是e</p><p id="ab77" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">凭直觉，我们可以说注意力实际上取决于</p><ol class=""><li id="9f24" class="kb kc hu je b jf jg jj jk jn kd jr ke jv kf jz kg kh ki kj dt translated">当前激活的输入(alpha <t>)</t></li><li id="cf69" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz kg kh ki kj dt translated">解码器 (s &lt; t-1 &gt;)中最后一个时间步的<strong class="je hv">先前状态</strong></li></ol><p id="4e58" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">但是我们实际上不知道它们之间的真实函数，所以<strong class="je hv">我们简单地建立一个简单的神经网络来为我们学习这个关系</strong>。这个网络的输出将是“e”参数，该参数将用于计算α注意力参数</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div class="fe ff nw"><img src="../Images/bf2894fe1d1bef93c88e84d3427f07b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*DflsdCfGFwVdHx9kTo8npg.png"/></div></figure><p id="fd49" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">所以整个场景会是</p><figure class="lx ly lz ma fq iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff nx"><img src="../Images/d8e0905ab2f465101a0669362371b3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIfmJlK4utU5vi4Y7g-I0w.png"/></div></div></figure><p id="a25b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">下一次，如果上帝愿意，我们将介绍如何真正实现一个文本摘要模型，我们将介绍实现<a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods/blob/master/Implementation%20A%20(seq2seq%20with%20attention%20and%20feature%20rich%20representation)/Model%202/Model_2.ipynb" rel="noopener ugc nofollow" target="_blank">的步骤。这个模型</a>使用张量流中的波束搜索和注意力实现了一个多层双向LSTM，代码来自<a class="ae ka" href="https://github.com/dongjun-Lee/text-summarization-tensorflow" rel="noopener ugc nofollow" target="_blank">董军-李</a>，我已经将其构建到google colab笔记本中，并将数据托管到google drive， 这样你就不需要下载数据，也不需要在你的电脑上运行代码，你只需要在google colab上运行，并将google colab连接到你的google drive上。</p><blockquote class="mx my mz"><p id="a6b5" class="jc jd lw je b jf jg jh ji jj jk jl jm na jo jp jq nb js jt ju nc jw jx jy jz hn dt translated"><em class="hu">我真心希望你喜欢阅读本教程，我希望我已经把这些概念讲清楚了，这一系列教程的所有代码都可以在这里找到</em><a class="ae ka" href="https://github.com/theamrzaki/text_summurization_abstractive_methods" rel="noopener ugc nofollow" target="_blank"><em class="hu"/></a><em class="hu">，你可以简单地使用google colab来运行它，请查看教程并告诉我你对它的看法，希望再次见到你。</em></p></blockquote><h1 id="04d2" class="mg kx hu bd ky mh ny mj lc mk nz mm lg mn oa mp lj mq ob ms lm mt oc mv lp mw dt translated">后续教程</h1><ul class=""><li id="4b66" class="kb kc hu je b jf lr jj ls jn od jr oe jv of jz og kh ki kj dt translated"><a class="ae ka" href="http://bit.ly/2ZeEmvO" rel="noopener ugc nofollow" target="_blank">在Tensorflow的94行中构建一个抽象的文本摘要器！！(教程6) </a></li><li id="7e8a" class="kb kc hu je b jf kk jj kl jn km jr kn jv ko jz og kh ki kj dt translated"><a class="ae ka" href="http://bit.ly/2EhcRIZ" rel="noopener ugc nofollow" target="_blank">用于文本摘要的抽象的组合&amp;提取方法(教程7) </a></li></ul><figure class="lx ly lz ma fq iv"><div class="bz el l di"><div class="oh oi l"/></div></figure></div></div>    
</body>
</html>