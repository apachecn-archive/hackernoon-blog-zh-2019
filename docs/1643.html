<html>
<head>
<title>ML MODEL TO DETECT THE BIGGEST OBJECT IN AN IMAGE PART - 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">检测图像中最大对象的ML模型第2部分</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/single-object-detection-part-2-2deafc911ce7?source=collection_archive---------2-----------------------#2019-03-10">https://medium.com/hackernoon/single-object-detection-part-2-2deafc911ce7?source=collection_archive---------2-----------------------#2019-03-10</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><div class=""><h2 id="4fb9" class="pw-subtitle-paragraph ir ht hu bd b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ek translated">2-在包围盒的帮助下对图像中的对象进行分类和定位。</h2></div><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff jj"><img src="../Images/fd36685364cb5e1ac7e67003baa73e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0bfYf9_8LR2x_CP7KPMTw.jpeg"/></div></div></figure><p id="85a3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">(<a class="ae kr" href="https://medium.com/p/e65a537a1c31/edit" rel="noopener">此处阅读第1部分</a>)</p><p id="f461" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">欢迎来到fast.ai的第二部分。这是<a class="ae kr" href="http://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> Fastdotai </a>的第8课，我们将处理<strong class="jx hv">单个物体检测。</strong>在我们开始之前，我想感谢<a class="ae kr" href="https://twitter.com/jeremyphoward" rel="noopener ugc nofollow" target="_blank"> <strong class="jx hv">杰瑞米·霍华德</strong> </a>和<a class="ae kr" href="https://twitter.com/math_rachel" rel="noopener ugc nofollow" target="_blank"> <strong class="jx hv">雷切尔·托马斯</strong> </a>为AI民主化所做的努力。</p><p id="4a99" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">第二部分假设对第一部分有很好的理解。以下是链接，请按以下顺序随意探索本系列的第一部分。</p><ol class=""><li id="4bf2" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-2-1-e9cc80d81a9d" rel="noopener" target="_blank">狗Vs猫图像分类</a></li><li id="9406" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-2-2-dog-breed-classification-5555c0337d60" rel="noopener" target="_blank">犬种图像分类</a></li><li id="5a27" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-3-a-case-of-multi-label-classification-a4a90672a889" rel="noopener" target="_blank">多标签图像分类</a></li><li id="b325" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-4-1-time-series-analysis-a23217418bf1" rel="noopener" target="_blank">利用神经网络进行时间序列分析</a></li><li id="93cd" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://geneashis.medium.com/nlp-sentiment-analysis-on-imdb-movie-dataset-fb0c4d346d23" rel="noopener">对IMDB电影数据集的NLP情感分析</a></li><li id="49cc" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-5-1-movie-recommendation-using-fastai-a53ed8e41269" rel="noopener" target="_blank">电影推荐系统的基础</a></li><li id="11d8" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-5-2-collaborative-filtering-from-scratch-1877640f514a" rel="noopener" target="_blank">从无到有的协同过滤</a></li><li id="933a" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://towardsdatascience.com/fast-ai-season-1-episode-5-3-collaborative-filtering-using-neural-network-48e49d7f9b36" rel="noopener" target="_blank">使用神经网络的协同过滤</a></li><li id="61c9" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://geneashis.medium.com/fast-ai-season-1-episode-6-1-write-philosophy-like-nietzsche-using-rnn-8fe70cfb923c" rel="noopener">像尼采一样写哲学</a></li><li id="075d" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" href="https://geneashis.medium.com/fast-ai-season-1-episode-7-1-performance-of-different-neural-networks-on-cifar-10-dataset-c6559595b529" rel="noopener">不同神经网络在Cifar-10数据集上的性能</a></li><li id="ed16" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" rel="noopener" href="/hackernoon/single-object-detection-e65a537a1c31"> ML模型检测图像中最大的物体Part-1 </a></li><li id="b69a" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated"><a class="ae kr" rel="noopener" href="/hackernoon/single-object-detection-part-2-2deafc911ce7"> ML模型检测图像中最大的物体Part-2 </a></li></ol><p id="488f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这篇博文关注的是<strong class="jx hv">对图片</strong>中最大的物体进行分类。我们将使用的数据集是PASCAL VOC (2007版)。这是<a class="ae kr" rel="noopener" href="/@GeneAshis/single-object-detection-e65a537a1c31">上一部</a>的延续。请登录查看</p><ol class=""><li id="7506" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq kx ky kz la dt translated"><a class="ae kr" rel="noopener" href="/@GeneAshis/single-object-detection-e65a537a1c31">单个物体检测</a></li></ol><p id="875c" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">希望你们记得我们之前讨论的内容。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="lg lh l"/></div></figure><p id="8a3c" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv"> <em class="li"> 1.5。找到图像中最大的物体</em> </strong></p><ul class=""><li id="d0e6" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated">众所周知，每个图像都有多个对象，而多个对象都有多个与之相关联边界框。我们的目标是找到图像中最大的对象，这可以从图像中对象周围的包围盒区域中得到。为此，我们将利用以下最大的项目分类器函数。</li></ul><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="ae35" class="lp lq hu ll b fv lr ls l lt lu"><strong class="ll hv"># get largest</strong><br/>def get_largest(b):<br/>    if not b: raise Exception()<br/>    b = sorted(b, key=<strong class="ll hv">lambda x:</strong> <strong class="ll hv">np.product(x[0][-2:]-x[0][:2])</strong>, reverse=True)<br/>    return b[0]</span></pre><ul class=""><li id="4c43" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated">让我们分解上面的函数。假设我们对特定图像的注释是<code class="eh lv lw lx ll b">x = ([96, 155, 269, 350],16)</code>。在我们继续之前，让我提醒您，注释是一个元组，包含边界框和边界框所属的类。边界框代表左上角和右下角的坐标。如下图。</li></ul><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff ly"><img src="../Images/a2b3061afa4888283f027d0e4efd35db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_qSffr7NebzbfCS6gSTmnQ.png"/></div></div></figure><p id="1ebf" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">下面的快照详细解释了上图。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff lz"><img src="../Images/c4bf070cfd2a4a69db0378d47c5dfc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gsEIai6DpDnOJDWT0Ifx_Q.png"/></div></div></figure><p id="d95e" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在上面的函数中，我们使用了下面的代码</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="38d8" class="lp lq hu ll b fv lr ls l lt lu">b = sorted(b, key=lambda x: np.product(x[0][-2:]-x[0][:2]), reverse=True)</span></pre><p id="d87a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">获取图像中所有边界框的面积，然后按面积排序。<code class="eh lv lw lx ll b">b[0]</code>返回第一个边界框，即最大的一个。</p><p id="e665" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在，让我们创建一个字典，其中的键是Image ID，值是一个包含最大边界框及其类的元组。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="007c" class="lp lq hu ll b fv lr ls l lt lu">training_largest_annotations = {a: get_largest(b) for a,b in training_annotations.items()}<br/><strong class="ll hv">training_annotations[17]</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ma"><img src="../Images/65c01a6fca393c217cde4fc8488af77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*672rTUwiArF3WclEAyY8hA.png"/></div></figure><p id="ae11" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">图像ID 17 </strong>包含两个属于类别15和13的边界框。为了获得这两个中最大的边界框:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="2e3f" class="lp lq hu ll b fv lr ls l lt lu">training_largest_annotations[17]</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mb"><img src="../Images/c79f6ccf3acdc792afa137a13fe49426.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*OHQR60s1lGxGTRdk3YJrzg.png"/></div></figure><p id="1671" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv"> <em class="li"> 1.6。绘制图像中最大的对象</em> </strong></p><p id="bfbc" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">因为我们有一个图像及其相应的bbox和它的类的字典。让我们画出这个。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="772a" class="lp lq hu ll b fv lr ls l lt lu">b,c = training_largest_annotations[17]<br/>b = bb_hw(b)<br/>ax = show_img(open_image(IMG_PATH/training_filenames[17]), figsize=(5,10))<br/>draw_rect(ax, b)<br/>draw_text(ax, b[:2], categories[c], sz=16)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mc"><img src="../Images/90ea7e9474089af0673505273661a29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*i0vODqbOcYK_kXFtmOkCOw.png"/></div></figure><p id="c67f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这个字典包含了我们将从中获取图像文件名和图像中最大的类的数据。在这里，我们得到了图像中的<strong class="jx hv">马</strong>和<strong class="jx hv">人</strong>。马是最大的物体，正在被绘制。利用这些信息，我们可以进行建模。因此把它转换成一个<code class="eh lv lw lx ll b"><strong class="jx hv">.CSV</strong></code>文件数据。我们用熊猫来做这件事。</p><p id="6e12" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv"> <em class="li"> 1.7。为建模获取适当格式的数据。</em>T13】</strong></p><ul class=""><li id="108b" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated">设置CSV文件应该存在的路径。</li></ul><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="c49b" class="lp lq hu ll b fv lr ls l lt lu">(PATH/'tmp').mkdir(exist_ok=True)<br/>CSV = PATH/'tmp/lrg.csv'</span></pre><ul class=""><li id="b270" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated">CSV文件有两列，一列包含图像文件名(fn ),另一列包含图像中的对象所属的类别/类。</li></ul><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="d36c" class="lp lq hu ll b fv lr ls l lt lu">df = pd.DataFrame({'fn': [training_filenames[o] for o in training_ids],<br/>    'cat': [categories[training_largest_annotations[o][1]] for o in training_ids]}, columns=['fn','cat'])<br/>df.to_csv(CSV, index=False)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff md"><img src="../Images/616bc380dfc811e94c5793d2d7d5fea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*Huaw8YbInf7Qjvz0Nyw6Vg.png"/></div></figure><p id="3056" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">步骤2 :-为最大项目分类器的建模获得合适的架构。</strong></p><p id="5b8c" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在这一步中，我们训练我们的模型来寻找图像中最大的物体。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="4c1e" class="lp lq hu ll b fv lr ls l lt lu">f_model = resnet34<br/>sz=224<br/>bs=64</span></pre><p id="e688" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">从这里开始，它就像狗和猫的分类器。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="212a" class="lp lq hu ll b fv lr ls l lt lu">tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, crop_type=CropType.NO)<br/>md = ImageClassifierData.from_csv(PATH, JPEGS, CSV, tfms=tfms, bs=bs)</span></pre><ul class=""><li id="b958" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated">在<code class="eh lv lw lx ll b">tfms_from_model(...)</code>函数中，我们没有裁剪<code class="eh lv lw lx ll b">crop_type=CropType.NO</code>。我们不是裁剪，而是挤压图像。</li><li id="14ac" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq lj ky kz la dt translated">我们通常如何调整大小？？将最小的边设置为224，然后在训练期间随机进行方形裁剪。除非我们使用数据扩充，否则在验证过程中采取中间裁剪。对于边界框来说，这与Imagenet不同，在Imagenet中，我们关心的东西主要在中心，而且非常大，对象检测中的许多东西非常小，可能也更接近边缘。</li><li id="1ac5" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq lj ky kz la dt translated">使用<code class="eh lv lw lx ll b">ImageClassifierData.from_csv()</code>,我们正在创建模型数据，即格式化数据，以便它可以按照fastai格式使用。</li></ul><p id="8e27" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">使用下面的代码片段可以可视化图像的挤压</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="bd86" class="lp lq hu ll b fv lr ls l lt lu">show_img(md.val_ds.denorm(to_np(x))[0]);</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff me"><img src="../Images/f713dd88b51881195e51254f03299107.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*Qqq7EEyVYIs5MRPobEcs7w.png"/></div></figure><p id="7f18" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在让我们检查一下我们的数据加载器:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="71d6" class="lp lq hu ll b fv lr ls l lt lu">x,y=next(iter(md.val_dl))</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mf"><img src="../Images/0d88744c1f1ace9868bf13d240bba025.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*LAtDYX5D8bivCwpsdtlZPA.png"/></div></figure><p id="1d20" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">因此，数据加载器抓取一批64个图像，其中<code class="eh lv lw lx ll b">x</code>包含这64个图像，<code class="eh lv lw lx ll b">y</code>包含这64个图像所属的类。</p><p id="37b2" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如果我们查看一下<code class="eh lv lw lx ll b">x </code>的值，我们会发现这些值不在0和1之间。这是因为对输入做了大量的处理，以使其准备好传递给预训练的模型。它使用硬编码的图像统计做一些标准化。要了解更多，请使用<code class="eh lv lw lx ll b">?? tfms_from_model</code>查看转换函数。</p><p id="cdc2" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在，当我们想要可视化来自验证数据加载器的图像时，我们必须去规格化图像，因为它已经在<code class="eh lv lw lx ll b">tfms_from_model</code>中被规格化了。这就是为什么我们使用:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="cda3" class="lp lq hu ll b fv lr ls l lt lu">show_img(md.val_ds.<strong class="ll hv">denorm</strong>(to_np(x))[0]);</span></pre><p id="93ca" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">反规格化不仅对图像进行反规格化，还修正了维数顺序。Denorm是一种撤销已经应用到<code class="eh lv lw lx ll b"><strong class="jx hv">md.val_ds</strong></code> <strong class="jx hv">的每一个变换。通过小批量，但你必须先把它变成小批量。</strong></p><p id="cee9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">让我们建立神经网络。这里的<code class="eh lv lw lx ll b">f_model=resnet34</code>，前面描述过。使用的优化器是Adam。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="7e3c" class="lp lq hu ll b fv lr ls l lt lu">learn = ConvLearner.pretrained(f_model, md, metrics=[accuracy])<br/>learn.opt_fn = optim.Adam<br/>lrf=learn.lr_find(1e-5,100)<br/>learn.sched.plot()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mg"><img src="../Images/76ca5204e8b34edde3c8b16660bdf6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*32JfwJc0GxaHHzqnOHnzzQ.png"/></div></figure><p id="5097" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">我们没有看到上升的原因是因为我们已经默认删除了前几个和最后几个点。这是由于最后几个点实际上射向无穷大的原因，即它们的损失太大，所以我们基本上看不到任何东西。因此删除它。要撤消这一操作并使上升可视化，请使用下面的代码:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="4eb8" class="lp lq hu ll b fv lr ls l lt lu">learn.sched.plot(n_skip=5, n_skip_end=1)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mh"><img src="../Images/3fb6a4a59281b60ab98fe168831e8b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*VJrdA1vWhqt_xQVtWv6VJQ.png"/></div></figure><p id="7201" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">上面的代码所做的是，它从开始跳过5个点，从结尾跳过1个点，这样除了这些点之外的所有点都可以被可视化。</p><p id="fddc" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">让我们在这个模型的基础上用<code class="eh lv lw lx ll b">lr=2e-2</code>做一些训练，如图所示。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="410f" class="lp lq hu ll b fv lr ls l lt lu">lr = 2e-2<br/>learn.fit(lr, 1, cycle_len=1)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mi"><img src="../Images/6645552751cd10cb8004dccd16c9d08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Id0bwiCpNF1jfTGkbjYcpw.png"/></div></div></figure><p id="c188" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">只要训练最后一层，我们就能得到80%的准确率。</p><p id="4538" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">解冻几层并训练</strong></p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="c476" class="lp lq hu ll b fv lr ls l lt lu">lrs = np.array([lr/1000,lr/100,lr])<br/>learn.freeze_to(-2)<br/>lrf=learn.lr_find(lrs/1000)<br/>learn.sched.plot(1)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mj"><img src="../Images/5cf7996ea6da56d6067179945929e15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mpWfmhMYpP_4iynMIfhFZA.png"/></div></div></figure><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="9242" class="lp lq hu ll b fv lr ls l lt lu">learn.fit(lrs/5, 1, cycle_len=1)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mk"><img src="../Images/27a44b337ad071856237bea705990ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrifkKFfQZDv8FzHTsRKKQ.png"/></div></div></figure><p id="d468" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在我们已经达到了81%的更好的准确率。</p><p id="e4c1" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">解冻所有层并训练:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="a6f1" class="lp lq hu ll b fv lr ls l lt lu">learn.unfreeze()<br/>learn.fit(lrs/5, 1, cycle_len=2)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff ml"><img src="../Images/666f313f03207ee45deb522e170da947.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*z_FOw-DrkzBlqQLdGFtldQ.png"/></div></figure><p id="077f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">为什么准确率没有提高到83%以上？</p><p id="bb06" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">不像狗对猫或ImageNet，每个图像有一个主要的东西。这是我们被要求去寻找的一件重要的事情。但是在Pascal数据集中，我们有很多小东西。所以即使是最好的分类器也不一定能做得很好。让我们来看看结果。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="a952" class="lp lq hu ll b fv lr ls l lt lu">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br/>for i,ax in enumerate(axes.flat):<br/>    ima=md.val_ds.denorm(x)[i]<br/>    b = md.classes[preds[i]]<br/>    ax = show_img(ima, ax=ax)<br/>    draw_text(ax, (0,0), b)<br/>plt.tight_layout()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff mm"><img src="../Images/3ddace25157c21b35de7b73082656332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLg1XSZKZYH0HdJ-zXDcQw.png"/></div></div></figure><p id="82f1" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在上面的例子中，我们的模型可以找到图像中最大的物体。现在让我们在图像中最大的物体周围放置一个边界框。</p></div><div class="ab cl mn mo hc mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hn ho hp hq hr"><p id="f3cf" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">步骤3:-获得一个合适的架构，用于对图像中最大对象周围的边界框进行建模。</strong></p><ul class=""><li id="8dea" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq lj ky kz la dt translated"><strong class="jx hv">如何创建包围盒？</strong></li></ul><p id="d06d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">为此，只需要弄清楚两件事</p><ol class=""><li id="6e88" class="ks kt hu jx b jy jz kb kc ke ku ki kv km kw kq kx ky kz la dt translated">创建一个有四个激活的神经网络，预测四个数字，即最大对象的边界框边缘(左上坐标和右下坐标)。这是一个有四个输出的回归问题。</li><li id="5852" class="ks kt hu jx b jy lb kb lc ke ld ki le km lf kq kx ky kz la dt translated">以这样一种方式决定损失函数，当它最小化时，我们的四个预测数字相当好。让我们看看如何做到这一点。</li></ol><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="7959" class="lp lq hu ll b fv lr ls l lt lu">BB_CSV = PATH/'tmp/bb.csv'<br/>bb = np.array([training_largest_annotations[o][0] for o in training_ids])<br/><strong class="ll hv"># Pick largest item in the image.</strong><br/>bbs = [' '.join(str(p) for p in o) for o in bb]<br/><strong class="ll hv"># Create bounding boxes separated by space.</strong><br/>df = pd.DataFrame({'fn': [training_filenames[o] for o in training_ids], 'bbox': bbs}, columns=['fn','bbox'])<br/><strong class="ll hv"># Put the image filename in one column 'fn' and bounding box of the largest object in that image in the 'bbox' column<br/></strong>df.to_csv(BB_CSV, index=False)<br/><strong class="ll hv"># Write it in a .csv file</strong><br/>BB_CSV.open().readlines()[:5]<br/><strong class="ll hv"># Check out how the data is stored in the below snapshot.<br/># To do multiple label classification , the multiple labels should be space separated and file name should be comma separated.</strong></span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mu"><img src="../Images/ea2e6a110b23107bf5e2c933ed82fd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*cD8qnh6XVCltYpkOjUnaqg.png"/></div></figure><p id="a4b8" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><strong class="jx hv">注意:-当我们对图像进行缩放或数据扩充时，也需要对边界框坐标进行缩放或数据扩充。</strong></p><p id="3bee" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">但是为什么呢？</p><p id="7547" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在前面的分类案例中，我们使用来扩充数据集中的图像。但是在边界框的情况下有一个小的变化。这里我们得到了图像和图像中物体的边界框坐标。在这种情况下，我们必须增加因变量，即边界框坐标以及图像。所以让我们看看如果我们只放大图像会发生什么。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="3338" class="lp lq hu ll b fv lr ls l lt lu">augs = [RandomFlip(), <br/>        RandomRotate(30),<br/>        RandomLighting(0.1,0.1)]<br/>tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, aug_tfms=augs)<br/>md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, continuous=True, bs=4)<br/>idx=3<br/>fig,axes = plt.subplots(3,3, figsize=(9,9))<br/>for i,ax in enumerate(axes.flat):<br/>    x,y=next(iter(md.aug_dl))<br/>    ima=md.val_ds.denorm(to_np(x))[idx]<br/>    b = bb_hw(to_np(y[idx]))<br/>    print(b)<br/>    show_img(ima, ax=ax)<br/>    draw_rect(ax, b)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mv"><img src="../Images/52391421c06fabc3cf590de64d03142f.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*EuWG0fppMNjYDUyuXkVbSA.png"/></div></figure><p id="d7f2" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如我们所见，当我们增大图像而不是边界框坐标时，图像被增大，而表示图像中对象坐标的边界框保持不变，这是不正确的。这使得数据是错误的。换句话说，当增强图像改变时，表示图像中的对象的边界框保持不变。因此，我们需要增加因变量，即边界框坐标，因为这两者是相互关联的。边界框坐标应该像图像坐标一样经历所有的几何变换。正如在下面的粗体代码中可以看到的，我们使用了<code class="eh lv lw lx ll b"><strong class="jx hv">tfm_y=TfmType.COORD</strong></code> <strong class="jx hv"> </strong>参数，这明确意味着无论对图像进行什么样的放大，也应该对边界框坐标进行放大。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="66d9" class="lp lq hu ll b fv lr ls l lt lu">augs = [RandomFlip(<strong class="ll hv">tfm_y=TfmType.COORD</strong>),<br/>        RandomRotate(3,p=0.5, <strong class="ll hv">tfm_y=TfmType.COORD</strong>),<br/>        RandomLighting(0.1,0.1, <strong class="ll hv">tfm_y=TfmType.COORD</strong>)]<br/># <strong class="ll hv">RandomRotate</strong> <strong class="ll hv">parameters:- Maximum of 3 degree of rotations .p=0.5 means rotate the image half of the time.</strong></span><span id="cbeb" class="lp lq hu ll b fv mw ls l lt lu">tfms = tfms_from_model(f_model, sz, crop_type=CropType.NO, <strong class="ll hv">tfm_y=TfmType.COORD, </strong>aug_tfms=augs)</span><span id="b525" class="lp lq hu ll b fv mw ls l lt lu"># <strong class="ll hv">Adding (tfm_y=TfmType.COORD) helps in changing the bounding box coordinates in case the model is squeezing or zooming the image</strong></span><span id="3c1f" class="lp lq hu ll b fv mw ls l lt lu">md = ImageClassifierData.from_csv(PATH, JPEGS, BB_CSV, tfms=tfms, <strong class="ll hv">continuous=True</strong>, bs=4)</span><span id="bc68" class="lp lq hu ll b fv mw ls l lt lu"><strong class="ll hv"># Note that we have to tell the transforms constructor that our labels are coordinates, so that it can handle the transforms correctly.</strong><br/>idx=4<br/>fig,axes = plt.subplots(3,3, figsize=(9,9))<br/>for i,ax in enumerate(axes.flat):<br/>    x,y=next(iter(md.aug_dl))<br/>    ima=md.val_ds.denorm(to_np(x))[idx]<br/>    b = bb_hw(to_np(y[idx]))<br/>    print(b)<br/>    show_img(ima, ax=ax)<br/>    draw_rect(ax, b)</span></pre><p id="1a18" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><code class="eh lv lw lx ll b"><strong class="jx hv">TfmType.COORD</strong></code> <strong class="jx hv"> </strong>基本上代表了如果我们对图像应用翻转变换，我们需要相应地改变包围盒坐标。因此，我们将<code class="eh lv lw lx ll b"><strong class="jx hv">TfmType.COORD</strong></code> <strong class="jx hv"> </strong>添加到应用于图像的所有变换中。</p><p id="d161" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">如果我们看到下图，这是有道理的。边界框随着图像不断变化，并在正确的位置表示对象。</p><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mx"><img src="../Images/ec3d1b2181102959caf94ce2dd2f728d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*u-39p7inev3h4kmqz_oxZg.png"/></div></figure><p id="0249" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">现在，基于resnet34创建一个ConvNet，但是这里有一个变化。我们不希望在已经创建了分类器的最后有任何标准的FC层集，而是希望添加具有四个输出的单个层，因为这是一个回归问题。</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="6807" class="lp lq hu ll b fv lr ls l lt lu"><strong class="ll hv">head_reg4 </strong>= nn.Sequential(Flatten(), nn.Linear(25088,4))<br/><strong class="ll hv"># Append head_reg4 on top of resnet34 model which will result in creation of regressor that predicts four values as output as shown in the code below.<br/># Here it is creating a tiny model that flattens the previous layer of the dimensions 7*7*512 =25088 and brings it down to 4 activations</strong></span><span id="49f7" class="lp lq hu ll b fv mw ls l lt lu">learn = ConvLearner.pretrained(f_model, md, custom_head=head_reg4)<br/>learn.opt_fn = optim.Adam<br/><strong class="ll hv"># Use Adam optimizer to optimize the loss function.</strong></span><span id="d366" class="lp lq hu ll b fv mw ls l lt lu">learn.crit = nn.L1Loss()<br/><strong class="ll hv"># The loss function here is L1 loss.</strong></span></pre><p id="74eb" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated"><code class="eh lv lw lx ll b"><strong class="jx hv">ConvLearner.pretrained(...)</strong></code> <strong class="jx hv"> </strong>中的<code class="eh lv lw lx ll b"><strong class="jx hv">custom_head</strong></code> <strong class="jx hv"> </strong>参数添加在模型顶部。它阻止创建任何完全连接的层和自适应最大池层，这是默认情况下完成的。相反，它会用我们要求的任何型号来取代那些型号。这里我们想要四个代表边界框坐标的激活。我们将把这个<code class="eh lv lw lx ll b"><strong class="jx hv">custom_head </strong></code>放在预训练的模型上，然后训练一段时间。</p><p id="b2e5" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">检查最后一层:-</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="8f5e" class="lp lq hu ll b fv lr ls l lt lu">learn.summary()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff my"><img src="../Images/b28eb4ddd401c054ddd1453f067b6cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*3t4APjoV_7f7RwDlb1h6qg.png"/></div></figure><p id="10a9" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在这一步之后，同样要找到一个最佳的学习率，并使用这个学习率来训练你的神经网络模型。让我们看看这是怎么做到的</p><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="cd76" class="lp lq hu ll b fv lr ls l lt lu">learn.lr_find(1e-5,100)<br/>learn.sched.plot(5)</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div class="fe ff mz"><img src="../Images/47033a46065481464062d679bab9a3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*w_3fc7zN91G8jGJlAhrEcg.png"/></div></figure><pre class="jk jl jm jn fq lk ll lm ln aw lo dt"><span id="fa76" class="lp lq hu ll b fv lr ls l lt lu">lr = 2e-3<br/>learn.fit(lr, 2, cycle_len=1, cycle_mult=2)<br/>lrs = np.array([lr/100,lr/10,lr])<br/>learn.freeze_to(-2)<br/>lrf=learn.lr_find(lrs/1000)<br/>learn.sched.plot(1)<br/>learn.fit(lrs, 2, cycle_len=1, cycle_mult=2)<br/>learn.freeze_to(-3)<br/>learn.fit(lrs, 1, cycle_len=2)<br/>learn.save('reg4')<br/>learn.load('reg4')<br/>x,y = next(iter(md.val_dl))<br/>learn.model.eval()<br/>preds = to_np(learn.model(VV(x)))</span><span id="a8c8" class="lp lq hu ll b fv mw ls l lt lu">fig, axes = plt.subplots(3, 4, figsize=(12, 8))<br/>for i,ax in enumerate(axes.flat):<br/>    ima=md.val_ds.denorm(to_np(x))[i]<br/>    b = bb_hw(preds[i])<br/>    ax = show_img(ima, ax=ax)<br/>    draw_rect(ax, b)<br/>plt.tight_layout()</span></pre><figure class="jk jl jm jn fq jo fe ff paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="fe ff na"><img src="../Images/0e5a9b337e344d3bacd7ebfd4a002dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgPcLVy1xs7BMIB-uuRuDw.png"/></div></div></figure><p id="1829" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">从上面预测的输出快照可以看出，它做得非常好。虽然它在孔雀和牛的例子中失败了。</p><p id="a7a3" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在我们的下一篇博文中，我们将结合步骤2和步骤3。<strong class="jx hv">这将帮助我们预测图像中最大的对象，同时预测该最大对象的边界框。</strong></p><p id="4264" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">这篇博文涵盖了很多东西。你可能会有这样的感觉</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nb lh l"/></div></figure><p id="e69a" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">不过没关系！！！我强烈建议您回到上一部分，检查流程。我已经用<strong class="jx hv">粗体</strong>点标出了重要的东西，它将帮助你理解中间的重要步骤。</p><p id="bc7f" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">感谢你坚持这一部分。</p><p id="92f0" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在我的下一篇博文中，我们将会看到如何将第二步和第三步结合起来。从计算机视觉的角度来看，这并不是什么新鲜事，但这正是我们将要深入探讨的Pytorch编码之美。</p><p id="27dc" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">在那之前再见..</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nc lh l"/></div></figure><p id="299d" class="pw-post-body-paragraph jv jw hu jx b jy jz iv ka kb kc iy kd ke kf kg kh ki kj kk kl km kn ko kp kq hn dt translated">被杰瑞米·霍华德赏识的感觉真的很好。看看他对我的Fast.ai第一部分博客的看法。一定要看一看。</p><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="nd lh l"/></div></figure><figure class="jk jl jm jn fq jo"><div class="bz el l di"><div class="ne lh l"/></div></figure></div></div>    
</body>
</html>