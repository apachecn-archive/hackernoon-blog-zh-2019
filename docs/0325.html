<html>
<head>
<title>Spark Custom Stream Sources</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark自定义流源</h1>
<blockquote>原文：<a href="https://medium.com/hackernoon/spark-custom-stream-sources-ec360b8ae240?source=collection_archive---------4-----------------------#2019-01-14">https://medium.com/hackernoon/spark-custom-stream-sources-ec360b8ae240?source=collection_archive---------4-----------------------#2019-01-14</a></blockquote><div><div class="ef hi hj hk hl hm"/><div class="hn ho hp hq hr"><div class=""/><figure class="fi fk is it iu iv fe ff paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="fe ff ir"><img src="../Images/9a2fc5a276a181e27ca670349f8e9676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vKQr88j5-VoLzuvg5gN1GA.jpeg"/></div></div></figure><p id="2833" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Apache Spark是最通用的大数据框架之一。能够在内存处理和函数风格中混合不同种类的工作负载，这使得任何人都希望从事数据处理领域的编码工作。</p><p id="76b3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">Spark的一个重要方面是它是为可扩展性而构建的。为<strong class="je hv"> RDD </strong> API编写新的连接器，或者扩展<strong class="je hv">数据框架</strong> / <strong class="je hv">数据集</strong> API，可以让第三方轻松集成Spark。大多数人会使用内置的API，比如Kafka用于流处理，或者JSON / CVS用于文件处理。然而，有些时候我们需要更具体的实现，离我们更近。例如，我们可能有一个在我们公司使用的专有数据库，但是没有用于它的连接器。我们可以简单地编写一个，就像我们在上一篇文章中解释的那样(<a class="ae ka" href="https://hackernoon.com/extending-our-spark-sql-query-engine-5f4a088de986" rel="noopener ugc nofollow" target="_blank"><strong class="je hv"><em class="kb">)Spark数据源API。扩展我们的Spark SQL查询引擎</em> </strong> </a>)。</p><p id="ffa8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">从Spark 2.0开始，我们可以从流中创建源，这赋予了<em class="kb"> Spark结构化流API </em>生命。正如我们可以想象的那样，有一些内置的流媒体源，Kafka就是其中之一，还有FileStreamSource、TextSocketSource等等…</p><p id="e0a1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">使用新的<em class="kb">结构化流API </em>应该优于旧的<strong class="je hv">数据流</strong>。然而，同样的问题又出现了。我们如何扩展这个新的API，以便使用我们自己的流媒体源？这个问题的答案就在这个帖子上。</p><h1 id="6fbe" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">扩展点</h1><p id="41a0" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">让我们首先回顾一下为了创建我们自己的流源，我们需要接触的主要组件。</p><p id="a5ef" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">首先，<strong class="je hv"> StreamSourceProvider </strong>是指示什么源将被用作流读取器的。</p><p id="38a5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">其次，<strong class="je hv"> DataSourceRegister </strong>将允许我们在Spark中注册我们的源，这样它就可以用于流处理。</p><p id="01ad" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">第三，<strong class="je hv"> Source </strong>是我们需要实现的接口，因此我们提供类似流源代码的行为。</p><h1 id="cb8d" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">我们的流媒体源</h1><p id="4cab" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">为了这篇文章，我们将实现一个相当简单的流源，但是同样的概念也适用于任何你需要自己实现的流源。</p><p id="1333" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们的流源叫做<strong class="je hv">inmemorrandomstrings</strong>。它基本上生成一系列随机字符串及其长度，这些字符串被视为一个数据帧。</p><p id="c243" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">因为我们希望保持简单，所以我们将批处理存储在内存中，并在过程完成时丢弃它们。<strong class="je hv"> InMemoryRandomStrings </strong>不具备容错能力，因为数据是在处理时生成的，而内置的Kafka数据源中的数据实际上位于Kafka集群中。</p><p id="9498" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们可以从定义我们的<strong class="je hv"> StreamSourceProvider </strong>开始，它定义了我们的<strong class="je hv">源</strong>是如何创建的。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="fe8b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">类<strong class="je hv"> DefaultSource </strong>是我们的<strong class="je hv"> StreamSourceProvider </strong>，我们需要实现两个必需的函数，<strong class="je hv"> sourceSchema </strong>和<strong class="je hv"> createSource </strong>。</p><p id="496e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv">inmemoryrandomstrings . schema</strong>是我们将在示例中使用的固定模式，但是该模式可以动态传入。</p><p id="430e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> createSource </strong>函数<strong class="je hv"> </strong>然后返回一个<strong class="je hv">inmemorrandomstrings</strong>的实例，这是我们实际的<strong class="je hv">源。</strong></p><h1 id="4d47" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">InMemoryRandomStrings</h1><p id="c3cb" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">现在，让我们分部分地查看<strong class="je hv">inmemorrandomstrings</strong>代码，这样我们就可以专注于所有的细节。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="5840" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">返回我们的源使用的模式，在我们的例子中，我们知道模式是固定的。</p><p id="9ca9" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh ll lm ln lo b">getOffset</code>应该返回我们的源看到的最新偏移。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="1b96" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">请注意，我们添加了一个名为<code class="eh ll lm ln lo b">offset</code>的变量，它将跟踪所看到的数据。然后，如果我们的源没有看到任何数据，我们返回<code class="eh ll lm ln lo b">None</code>，否则返回<code class="eh ll lm ln lo b">Some(offset)</code>。</p><p id="564d" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，让我们看看我们的源是如何产生一些数据的，我们将为它使用一个运行线程。请注意<strong class="je hv">dataGeneratorStartingThread</strong>函数。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="b693" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这里，我们添加了一个线程，它生成随机值并递增偏移量，同时将值和偏移量存储在内部缓冲区中。我们的源代码一创建，线程就开始运行。<code class="eh ll lm ln lo b">stop</code>功能停止正在运行的线程。</p><p id="ef61" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">在这一点上，我们离我们的目标只有两个功能。</p><p id="16e6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><code class="eh ll lm ln lo b">getBatch</code>返回一个<strong class="je hv">数据帧</strong>给spark，数据在传递的偏移范围内。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="1687" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们可以看到，我们正在从内部缓冲区获取数据，这样数据就有了相应的索引。从那里，我们生成<strong class="je hv">数据帧</strong>，然后发送回Spark。</p><p id="6d45" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">最后，<code class="eh ll lm ln lo b">commit</code>是Spark如何向我们表明它不会请求小于或等于被传递的偏移量。换句话说，我们可以用小于或等于传递给<code class="eh ll lm ln lo b">commit</code>的偏移量从内部缓冲区中移除所有数据。这样，我们可以节省一些内存，避免耗尽内存。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="55b6" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在，我们已经完成了我们的源代码，整个代码如下。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><h1 id="15e3" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">使用我们的定制源</h1><p id="b401" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">现在，我们需要通过指明要使用的正确格式，将我们的源代码插入到<em class="kb"> Spark结构化流API </em>中。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="3dd5" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">这里我们使用常规的<code class="eh ll lm ln lo b">.readStream</code> API，并指定流格式是我们对<code class="eh ll lm ln lo b">StreamSourceProvide</code>的实现，即<em class="kb">com . github . anicolaspp . spark . SQL . streaming . default source</em>。</p><p id="afc3" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">现在我们可以像查询任何其他数据帧一样查询我们的流源。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="487b" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">输出如下所示。</p><figure class="lf lg lh li fq iv"><div class="bz el l di"><div class="lj lk l"/></div></figure><p id="f6a1" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated">我们从我们的数据源生成的数据的连续聚合中看到的。</p><h1 id="63ea" class="kc kd hu bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz dt translated">结论</h1><p id="5f4d" class="pw-post-body-paragraph jc jd hu je b jf la jh ji jj lb jl jm jn lc jp jq jr ld jt ju jv le jx jy jz hn dt translated">Apache Spark是处理大规模数据的不二之选。它的特性几乎胜过任何其他工具。此外，它可以以许多不同的方式进行扩展，正如我们所看到的，我们可以编写自己的数据源和流源，以便可以轻松地将它们插入到我们的spark代码中。</p><p id="afc8" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><strong class="je hv"> <em class="kb">整个项目和源代码可以在这里找到</em></strong><a class="ae ka" href="https://github.com/anicolaspp/SparkStreamSources" rel="noopener ugc nofollow" target="_blank"><strong class="je hv"><em class="kb">SparkStreamSources</em></strong></a><strong class="je hv"><em class="kb">。</em> </strong></p><p id="929e" class="pw-post-body-paragraph jc jd hu je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hn dt translated"><em class="kb">快乐编码。</em></p></div></div>    
</body>
</html>